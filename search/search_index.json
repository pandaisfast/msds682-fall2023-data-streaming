{"config":{"lang":["en","de","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MSDS 682 Data Streaming Processing Overview","text":""},{"location":"#course-description","title":"Course Description","text":"<p>This class are to equips students with the skills necessary to process continuous data streams at scale and in real-time.</p> <p>Students will gain hands-on experience with Apache Kafka and other modern data engineering tools. We focus on blending foundational knowledge with hands-on skills, often using real-world examples to anchor theoretical concepts. The primary programming language for the course is Python. </p> <ul> <li>Semester: Fall 2023</li> <li>Graduate Course in Data Engineering Track</li> <li>Course Notes: https://pandaisfast.github.io/msds682-fall2023-data-streaming/</li> <li>Canvas: https://usfca.instructure.com/courses/1617043</li> <li>Additional Materials: From time to time, I'll be publishing supplementary resources and materials on Canvas. Please check periodically to ensure you have all the required content.</li> <li>Homework Submission: Please submit all your homework assignments through Canvas. This ensures there's a central location for submission, and it's easier to keep track.</li> <li>Piazza: Q/A &amp; discussing lectures and assignments. https://piazza.com/usfca/fall2023/msds682</li> </ul>"},{"location":"#instructor-inforomation","title":"Instructor Inforomation","text":"<ul> <li>Jeremy W. Gu</li> <li>wgu9@usfca.edu</li> </ul>"},{"location":"#office-hours","title":"Office Hours","text":"<ul> <li>8:45-9:30am on Wed</li> <li>Office Location: Virtual Zoom</li> </ul>"},{"location":"#prerequisite-details","title":"Prerequisite Details","text":"<p>To ensure your success in this course, the following knowledge prerequisites are essential:</p> <ul> <li>Statistics: Grasp of concepts like mean, variance, data visualization, tabular data, and pandas. Relevant courses include MSDS 504 - Review Probability and Stats and MSDS 593 - EDA and Visualization.</li> <li>Python: Familiarity with Python Classes and Objects. While proficiency with the Mac command line is beneficial, there will be no focus on Java or Scala in the JVM ecosystem.</li> <li>SQL and Data Pipelines: Knowledge derived from courses such as MSDS 681 - Data Lakehouse is ideal.</li> <li>Machine Learning: Familiarity with scikit-learn, supervised and unsupervised learning, evaluation of model performance, and feature selection. Courses like MSDS 621 - Intro to Machine Learning, MSDS 630 - Advanced Machine Learning, and MSDS 680 - Machine Learning Operations are recommended, though not mandatory.</li> <li>Business Setting: This course requires project presentations and reports. Effective and professional communication in both written and spoken English is crucial. Business writing should be concise, straightforward, and to the point. </li> </ul>"},{"location":"#course-learning-outcomes","title":"Course Learning Outcomes","text":"<p>This applied, hands-on course provides students with the essential skills for processing and analyzing real-time data streams using modern data engineering tools and technologies. The goal is to equip students with foundational knowledge paired with practical skills in core data streaming tools and technologies like Kafka to prepare them for real-world data engineering and data science roles.</p> <p>The course focuses on core concepts like the Kafka ecosystem while providing practical skills development through hands-on projects. Students will gain experience with real-time data ingestion using Kafka, conducting streaming analytics, and extracting insights. Optional materials will expose students to additional technologies like Faust and Spark Structured Streaming for building and evaluating streaming applications.</p>"},{"location":"#fundamentals","title":"Fundamentals","text":"<ul> <li> <p>Kafka Ecosystem: Grasp the components of data streaming systems and delve into the Kafka ecosystem, recognizing the challenges each solution addresses. Kafka remains the primary focus.</p> </li> <li> <p>Confluent Kafka and Confluent Cloud: Use the Confluent Kafka Python library for tasks like topic management, production, and consumption. Demonstrations will be held using Confluent Cloud.</p> </li> <li> <p>Real-time Data Streaming with Apache Kafka: Students will undertake a course project for hands-on experience, ingesting real-time data using Apache Kafka, conducting live analytics, and extracting insights from streaming console reports.</p> </li> </ul>"},{"location":"#additional-skills","title":"Additional Skills","text":"<ul> <li> <p>Faust Stream Processing: Familiarize with the Faust Stream Processing Python library to craft real-time stream-based applications.</p> </li> <li> <p>Additional Tools: Time permitting, other tools will be introduced. For instance, students will explore the integration of Apache Spark Structured Streaming with Apache Kafka and understand the reports generated by the Structured Streaming console.</p> </li> </ul>"},{"location":"#textbooks-recommended-readings","title":"Textbooks &amp; Recommended Readings","text":"<p> Kafka in Action</p> <ul> <li>ISBN: 9781617295232</li> <li>Authors: Dylan Scott, Viktor Gamov, Dave Klein</li> <li>Publisher: Simon and Schuster</li> <li>Publication Date: 2022-02-15</li> <li>Required or recommended?: Recommended reading as a reference book.</li> </ul> <p></p> <p>Kafka: The Definitive Guide</p> <ul> <li>ISBN: 9781492043034</li> <li>Authors: Gwen Shapira, Todd Palino, Rajini Sivaram, Krit Petty</li> <li>Publisher: \"O'Reilly Media, Inc.\"</li> <li>Publication Date: 2021-11-05</li> <li>Edition: 2nd Edition</li> <li>Required or recommended?: Recommended. Not Required.</li> </ul>"},{"location":"#assignments","title":"Assignments","text":"<p>There will be three Assignments and one Final Project. It's crucial to adhere to deadlines. No submissions will be accepted past the due date.</p> <p>Policy on Collaboration and Academic Integrity</p> <ul> <li>Group Discussions &amp; Collaborations: We encourage effective group discussions and collaborations. Discussing concepts, techniques, and general approaches to problems with classmates can be beneficial. However, the work you submit must be your own.</li> <li>Originality of Work: All submitted assignments, code, and project reports must be your own original work. Copying and pasting code or text from classmates or any other source is strictly prohibited.</li> <li>Using AI Services (e.g., ChatGPT): Utilizing ChatGPT or other LLM models to aid in understanding course materials is permissible. However, always write in your own words and code independently. If you incorporate insights or specific code from any AI Services, clearly indicate which part(s) were influenced or sourced from them. </li> <li>Online Sources &amp; References: If you utilize or are inspired by online sources, always provide proper citation and reference. Respect the intellectual property of others.</li> <li>Cheating and plagiarism: We have a zero-tolerance stance on cheating and plagiarism. Engaging in any form of academic dishonesty will result in severe penalties, which may include failure in the course.</li> </ul>"},{"location":"#grading-breakdown-and-grading-policies","title":"Grading Breakdown and Grading Policies","text":""},{"location":"#grading-breakdown","title":"Grading Breakdown","text":"<ul> <li>Attendance and Professionalism: 5%</li> <li>Regular attendance and active participation are expected. </li> <li>Individual Assignment: 30%</li> <li>Assignments (10% each) will assess individual understanding and application of course material.</li> <li>Final Project: 45%</li> <li>Project Proposal 5%</li> <li>Written Report 20%</li> <li>Final Presentation 20%</li> <li>Midterm Exam: 20%</li> <li>No Final Exam</li> </ul> <p>This class is a standard, graded course with letter grades A - F. Each grade reflects the quality and understanding demonstrated by the student, as follows:</p> <p>A (90-100): Exceptional understanding and application. Demonstrates depth of knowledge and skill and indicates readiness to apply concepts in a professional setting.</p> <ul> <li> <p>A+: 96-100</p> </li> <li> <p>A: 93-95</p> </li> <li> <p>A-: 90-92</p> </li> </ul> <p>B (80-89): Competent understanding and application of course material, representing the expected level of competence in a business setting.</p> <ul> <li> <p>B+: 87-89</p> </li> <li> <p>B: 83-86</p> </li> <li> <p>B-: 80-82</p> </li> </ul> <p>C (70-79): Basic understanding, with room for improvement in application and depth, indicating achievements lower than the expected competence in the subject.</p> <ul> <li> <p>C+: 77-79</p> </li> <li> <p>C: 73-76</p> </li> <li> <p>C-: 70-72</p> </li> </ul> <p>F (Below 70): Limited understanding and application of course material, representing an unacceptably low level of knowledge and understanding of the subject matter.</p> <p>The expected class average is 85+ (Letter grade of B or above), with a normal distribution around this mean.</p>"},{"location":"#beyond-grades","title":"Beyond Grades","text":"<p>While the grading system in this course is a measure of academic performance, it can also shed light on the challenges one might face in the professional realm and the manner in which these are addressed. For example, when faced with tight deadlines, how does one ensure the quality of work? How does collaboration occur within a team, especially when compromises are necessary to meet delivery dates amidst differing opinions? How well do you explain data science concepts to people with no data background, and how effective is communication with peers and superiors? How are colleagues persuaded when introducing innovative ideas? </p> <p>In reality, certain missteps or attitude problems can have severe consequences, while standout performances can bring about many opportunities in your career. These are aspects indirectly reflected in your grades. If you earn an 'A+', there\u2019s a high probability that you will be a superstar in the workplace. Earning a 'B' is also commendable, as in a professional setting, it typically signifies \"Meeting Expectations\", meaning that you are performing your job. Conversely, if your grades are low, you might face significant risks of criticism from both managers and peers during performance evaluations. I hope every student can treat this course as a practical experience of the working world.</p>"},{"location":"#course-schedule","title":"Course Schedule","text":"Warning <p>This version of Course Schedule is tentative and may be delayed in updates. For the most accurate and up-to-date information, please use our course website on USF Canvas for the latest syllabus.</p> <p>Tentative Schedule (Each lecture: 5:30 - 7:20pm PST)</p> <ul> <li>Lecture #1: Oct 20, 2023 (F) - San Francisco-101 Howard 529</li> <li>Lecture #2: Oct 24, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #3: Oct 27, 2023 (F) - San Francisco-101 Howard 529</li> <li>Assignment #1: Due by 11:59pm on 10/28/2023</li> <li>Lecture #4: Oct 31, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #5: Nov 03, 2023 (F) - San Francisco-101 Howard 529</li> <li>Assignment #2: Due by 11:59pm on 11/4/2023</li> <li>Lecture #6: Nov 07, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #7: Nov 10, 2023 (F) - San Francisco-101 Howard 529 | 60-min Midterm Exam</li> <li>Lecture #8: Nov 14, 2023 (T) - San Francisco-101 Howard 529</li> <li>Final Project Proposal: Due by 11:59pm on 11/13/2023</li> <li>Lecture #9: Nov 17, 2023 (F) - San Francisco-101 Howard 527</li> <li>Lecture #10: Nov 21, 2023 (T) - San Francisco-101 Howard 529</li> <li>Assignment #3: Due by 11:59pm on 11/22/2023</li> <li>Lecture #11: Nov 24, 2023 (F) - San Francisco-101 Howard 529 | Thanksgiving: No Class</li> <li>Lecture #12: Nov 28, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #13: Dec 01, 2023 (F) - San Francisco-101 Howard 529</li> <li>Final Project Written Report and Code: Due by 11:59pm on 12/3/2023</li> <li>Final Project Presentation Deck: Must be submitted before the final lecture 5:30pm on 12/5/2023</li> <li>Lecture #14: Dec 05, 2023 (T) - San Francisco-101 Howard 157<ul> <li>Note: This lecture is allocated for the Final Project Presentation. Each student has a about 10-minute presentation slot.</li> </ul> </li> </ul>"},{"location":"#attendance-policy","title":"Attendance Policy","text":"<ul> <li>Mandatory attendance for every lecture. </li> <li>Use of Laptops: Please keep your laptops closed unless instructed otherwise, specifically during demo or exercise sessions. This is to ensure focus and participation during lectures.</li> <li>Absence Due to Illness: If you are unable to attend a lecture due to sickness or any other unavoidable circumstance, please notify me in advance. </li> <li>No Distractions: Mobile phones and other electronic devices should be kept silent and should not be used during class time. </li> </ul>"},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/","title":"Notebook - Setup &amp; Creating Topics","text":"In\u00a0[1]: Copied! <pre>from confluent_kafka.admin import AdminClient, NewTopic\n\n## Not recommended. Secrets are open to public.\n# def load_config():\n#     \"\"\"Load Kafka configuration.\"\"\"\n#     return {\n#         'bootstrap.servers': '{server}',\n#         'security.protocol': '{}',\n#         'sasl.mechanisms': '{}',\n#         'sasl.username': '{api key}',\n#         'sasl.password': '{api password}'\n#     }\n    \n## Recommended way of loading secrets from .env file\nimport os\nfrom dotenv import load_dotenv\n# Load environment variables\nload_dotenv()\ndef load_config():\n\"\"\"Load Kafka configuration.\"\"\"\n    return {\n        'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n        'security.protocol': os.getenv('SECURITY_PROTOCOL'),\n        'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),\n        'sasl.username': os.getenv('SASL_USERNAME'),\n        'sasl.password': os.getenv('SASL_PASSWORD')\n    }\n\n## \ndef topic_exists(admin_client, topic_name):\n\"\"\"Check topic existence.\"\"\"\n    return topic_name in set(admin_client.list_topics(timeout=5).topics.keys())\n\ndef create_topic(admin_client, topic_name, partitions=1, replication_factor=1, config={}):\n\"\"\"Create topic if not existing.\"\"\"\n    if not topic_exists(admin_client, topic_name):\n        new_topic = [\n            NewTopic(\n                topic_name, \n                num_partitions=partitions, \n                replication_factor=replication_factor, \n                config=config)]\n        created_topic = admin_client.create_topics(new_topic)\n        for topic, f in created_topic.items():\n            try:\n                f.result()\n                print(f\"Topic {topic} created\")\n            except Exception as e:\n                print(f\"Failed to create topic {topic}: {e}\")\n    else:\n        print(f\"Topic {topic_name} already exists\")\n\n# Main execution\nconf = load_config()\nadmin_client = AdminClient(conf)\ntopic_name = \"topic_example_v1\"\n\n# Example topic config\ntopic_config = {'cleanup.policy': 'compact'} \n# setting 'cleanup.policy': 'compact' ensures that the topic retains only the latest message for a particular key, discarding older messages for the same key.\n\ncreate_topic(admin_client, topic_name, partitions=3, replication_factor=3, config=topic_config)\n</pre> from confluent_kafka.admin import AdminClient, NewTopic  ## Not recommended. Secrets are open to public. # def load_config(): #     \"\"\"Load Kafka configuration.\"\"\" #     return { #         'bootstrap.servers': '{server}', #         'security.protocol': '{}', #         'sasl.mechanisms': '{}', #         'sasl.username': '{api key}', #         'sasl.password': '{api password}' #     }      ## Recommended way of loading secrets from .env file import os from dotenv import load_dotenv # Load environment variables load_dotenv() def load_config():     \"\"\"Load Kafka configuration.\"\"\"     return {         'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),         'security.protocol': os.getenv('SECURITY_PROTOCOL'),         'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),         'sasl.username': os.getenv('SASL_USERNAME'),         'sasl.password': os.getenv('SASL_PASSWORD')     }  ##  def topic_exists(admin_client, topic_name):     \"\"\"Check topic existence.\"\"\"     return topic_name in set(admin_client.list_topics(timeout=5).topics.keys())  def create_topic(admin_client, topic_name, partitions=1, replication_factor=1, config={}):     \"\"\"Create topic if not existing.\"\"\"     if not topic_exists(admin_client, topic_name):         new_topic = [             NewTopic(                 topic_name,                  num_partitions=partitions,                  replication_factor=replication_factor,                  config=config)]         created_topic = admin_client.create_topics(new_topic)         for topic, f in created_topic.items():             try:                 f.result()                 print(f\"Topic {topic} created\")             except Exception as e:                 print(f\"Failed to create topic {topic}: {e}\")     else:         print(f\"Topic {topic_name} already exists\")  # Main execution conf = load_config() admin_client = AdminClient(conf) topic_name = \"topic_example_v1\"  # Example topic config topic_config = {'cleanup.policy': 'compact'}  # setting 'cleanup.policy': 'compact' ensures that the topic retains only the latest message for a particular key, discarding older messages for the same key.  create_topic(admin_client, topic_name, partitions=3, replication_factor=3, config=topic_config)  <pre>Topic topic_example_v1 already exists\n</pre> In\u00a0[2]: Copied! <pre># List all topics \n# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n\ntopic_metadata = admin_client.list_topics(timeout=5)\nlist(topic_metadata.topics.keys())\n</pre> # List all topics  # https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html  topic_metadata = admin_client.list_topics(timeout=5) list(topic_metadata.topics.keys()) Out[2]: <pre>['demo_3_producer_trial3_applicants',\n 'test_1_topic',\n 'demo_3_producer_trial2_soccer',\n 'demo_3_producer_trial3_evaluation',\n 'demo_3_producer_trial1',\n 'demo1_free_text',\n 'topic_example_v1']</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/#notebook-setup-creating-topics","title":"Notebook - Setup &amp; Creating Topics\u00b6","text":""},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/#virtual-environment-setup","title":"Virtual Environment Setup\u00b6","text":"<p>A virtual environment is a tool that helps to keep dependencies required by different projects separate. It essentially allows you to create a virtual Python environment that is isolated from the global Python environment. This way, different projects can have different dependencies without any conflicts.</p>"},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/#setting-up-a-virtual-environment","title":"Setting up a Virtual Environment\u00b6","text":"<ol> <li><p>Create a Virtual Environment: Navigate to your project directory in the terminal and run the following command to create a new virtual environment named <code>venv</code>:</p> <pre>python -m venv venv\n</pre> </li> <li><p>Activate the Virtual Environment:</p> <ul> <li><p>Linux &amp; Mac:</p> <pre>source venv/bin/activate\n</pre> </li> <li><p>Windows (PowerShell):</p> <pre>.\\venv\\Scripts\\Activate.ps1\n</pre> </li> </ul> </li> <li><p>Install Required Packages:</p> <p>After activating your virtual environment, you can install required packages using pip. If the project provides a <code>requirements.txt</code> file (we have <code>confluent_kafka</code> and <code>python-dotenv</code>), you can install all the required packages with:</p> <pre>pip install -r requirements.txt\n</pre> <p>This command will install the versions of packages as mentioned in <code>requirements.txt</code>.</p> </li> <li><p>Secrets</p> <p>Create a <code>.env</code> file in your project directory with the following content:</p> <pre><code>BOOTSTRAP_SERVERS=YOUR_BOOTSTRAP_SERVERS_URL\nSECURITY_PROTOCOL=YOUR_SECURITY_PROTOCOL\nSASL_MECHANISMS=YOUR_SASL_MECHANISM\nSASL_USERNAME=YOUR_SASL_USERNAME\nSASL_PASSWORD=YOUR_SASL_PASSWORD\n</code></pre> </li> </ol>"},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/#specifying-python-kernel-within-jupyter-notebook-ipynb","title":"Specifying Python Kernel within Jupyter Notebook (ipynb)\u00b6","text":"<p>If you're using Jupyter Notebook or Jupyter Lab, it's important to ensure that the notebook is using the correct Python kernel, especially if you have multiple Python environments or versions.</p> <ol> <li><p>Install <code>ipykernel</code>:</p> <p>After activating your virtual environment, ensure you have <code>ipykernel</code> installed:</p> <pre>pip install ipykernel\n</pre> </li> <li><p>Choose the Correct Kernel in Jupyter:</p> <ul> <li>Start Jupyter Notebook or Jupyter Lab.</li> <li>Open your <code>.ipynb</code> notebook.</li> <li>From the menu, go to <code>Kernel</code> -&gt; <code>Change kernel</code> and select <code>venv</code> (or the name you provided in the previous step).</li> </ul> </li> </ol> <p>Now, your Jupyter notebook will use the Python version from your virtual environment and any packages you've installed in it.</p>"},{"location":"assets/msds-lec2-topic-producer/demo_a_creating_topic/#creating-topic","title":"Creating Topic\u00b6","text":""},{"location":"assets/msds-lec2-topic-producer/demo_b_producer_async/","title":"Notebook - Asynchronous Producer","text":"In\u00a0[1]: Copied! <pre># Topic topic_example_v1 has been created\n\nfrom confluent_kafka.admin import AdminClient, NewTopic\n\n# def load_config():\n#     \"\"\"Load Kafka configuration.\"\"\"\n#     return {\n#         'bootstrap.servers': '{server}',\n#         'security.protocol': '{}',\n#         'sasl.mechanisms': '{}',\n#         'sasl.username': '{api key}',\n#         'sasl.password': '{api password}'\n#     }\n    \n## Recommended way of loading secrets from .env file\nimport os\nfrom dotenv import load_dotenv\n# Load environment variables\nload_dotenv()\ndef load_config():\n\"\"\"Load Kafka configuration.\"\"\"\n    return {\n        'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n        'security.protocol': os.getenv('SECURITY_PROTOCOL'),\n        'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),\n        'sasl.username': os.getenv('SASL_USERNAME'),\n        'sasl.password': os.getenv('SASL_PASSWORD')\n    }\n\n# in python \nconfig = load_config()\n</pre> # Topic topic_example_v1 has been created  from confluent_kafka.admin import AdminClient, NewTopic  # def load_config(): #     \"\"\"Load Kafka configuration.\"\"\" #     return { #         'bootstrap.servers': '{server}', #         'security.protocol': '{}', #         'sasl.mechanisms': '{}', #         'sasl.username': '{api key}', #         'sasl.password': '{api password}' #     }      ## Recommended way of loading secrets from .env file import os from dotenv import load_dotenv # Load environment variables load_dotenv() def load_config():     \"\"\"Load Kafka configuration.\"\"\"     return {         'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),         'security.protocol': os.getenv('SECURITY_PROTOCOL'),         'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),         'sasl.username': os.getenv('SASL_USERNAME'),         'sasl.password': os.getenv('SASL_PASSWORD')     }  # in python  config = load_config() In\u00a0[2]: Copied! <pre>from confluent_kafka.admin import AdminClient, NewTopic\n\nadmin_client = AdminClient(config)\n\n# List all topics \n# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n\ntopic_metadata = admin_client.list_topics(timeout=5)\nlist(topic_metadata.topics.keys())\n</pre> from confluent_kafka.admin import AdminClient, NewTopic  admin_client = AdminClient(config)  # List all topics  # https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html  topic_metadata = admin_client.list_topics(timeout=5) list(topic_metadata.topics.keys()) Out[2]: <pre>['demo_3_producer_trial3_applicants',\n 'test_1_topic',\n 'demo_3_producer_trial2_soccer',\n 'demo_3_producer_trial3_evaluation',\n 'demo_3_producer_trial1',\n 'demo1_free_text',\n 'topic_example_v1']</pre> In\u00a0[3]: Copied! <pre># Create callback function\n\n# Producer callback\ndef delivery_report(err, msg):\n\"\"\"Callback to report the result of the produce operation.\"\"\"\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n</pre> # Create callback function  # Producer callback def delivery_report(err, msg):     \"\"\"Callback to report the result of the produce operation.\"\"\"     if err is not None:         print(f\"Message delivery failed: {err}\")     else:         print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")  In\u00a0[4]: Copied! <pre># Define message to generate\n\nimport random\ndef produce_messages(producer, topic_name, num_messages=10):\n\"\"\"Produce messages with the given format.\"\"\"\n    names = [\"Alice\", \"Bob\", \"Charlie\"]\n\n    for i in range(num_messages):\n        name = random.choice(names)\n        lat = random.uniform(-90, 90)\n        long = random.uniform(-180, 180)\n        message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.\n        message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"\n        \n        producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)\n        producer.poll(0.1)  # To trigger the delivery report callback for feedback\n\n    producer.flush()  # Ensure all messages are sent\n\n## Using producer.flush() before shutting down your producer ensures that all messages are delivered and that any callbacks associated with these messages are executed.\n</pre> # Define message to generate  import random def produce_messages(producer, topic_name, num_messages=10):     \"\"\"Produce messages with the given format.\"\"\"     names = [\"Alice\", \"Bob\", \"Charlie\"]      for i in range(num_messages):         name = random.choice(names)         lat = random.uniform(-90, 90)         long = random.uniform(-180, 180)         message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.         message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"                  producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)         producer.poll(0.1)  # To trigger the delivery report callback for feedback      producer.flush()  # Ensure all messages are sent  ## Using producer.flush() before shutting down your producer ensures that all messages are delivered and that any callbacks associated with these messages are executed.    In\u00a0[5]: Copied! <pre>from confluent_kafka import Producer\n\n## main: Producer\nproducer = Producer(config)\ntopic_name = \"topic_example_v1\"\nproduce_messages(producer, topic_name)\n</pre> from confluent_kafka import Producer  ## main: Producer producer = Producer(config) topic_name = \"topic_example_v1\" produce_messages(producer, topic_name)   <pre>Message delivered to topic_example_v1 [1] at offset 538\nMessage delivered to topic_example_v1 [1] at offset 539\nMessage delivered to topic_example_v1 [1] at offset 540\nMessage delivered to topic_example_v1 [1] at offset 541\nMessage delivered to topic_example_v1 [0] at offset 501\nMessage delivered to topic_example_v1 [0] at offset 502\nMessage delivered to topic_example_v1 [0] at offset 503\nMessage delivered to topic_example_v1 [0] at offset 504\nMessage delivered to topic_example_v1 [2] at offset 521\nMessage delivered to topic_example_v1 [2] at offset 522\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"assets/msds-lec2-topic-producer/demo_b_producer_async/#notebook-asynchronous-producer","title":"Notebook - Asynchronous Producer\u00b6","text":""},{"location":"assets/msds-lec2-topic-producer/demo_b_producer_serialization/","title":"Notebook - Asynchronous Producer w/ Serialization","text":"In\u00a0[13]: Copied! <pre># Topic topic_example_v1 has been created\n\nfrom confluent_kafka.admin import AdminClient, NewTopic\n\n# def load_config():\n#     \"\"\"Load Kafka configuration.\"\"\"\n#     return {\n#         'bootstrap.servers': '{server}',\n#         'security.protocol': '{}',\n#         'sasl.mechanisms': '{}',\n#         'sasl.username': '{api key}',\n#         'sasl.password': '{api password}'\n#     }\n    \n## Recommended way of loading secrets from .env file\nimport os\nfrom dotenv import load_dotenv\n# Load environment variables\nload_dotenv()\ndef load_config():\n\"\"\"Load Kafka configuration.\"\"\"\n    return {\n        'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n        'security.protocol': os.getenv('SECURITY_PROTOCOL'),\n        'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),\n        'sasl.username': os.getenv('SASL_USERNAME'),\n        'sasl.password': os.getenv('SASL_PASSWORD')\n    }\n\n# in python \nconfig = load_config()\n</pre> # Topic topic_example_v1 has been created  from confluent_kafka.admin import AdminClient, NewTopic  # def load_config(): #     \"\"\"Load Kafka configuration.\"\"\" #     return { #         'bootstrap.servers': '{server}', #         'security.protocol': '{}', #         'sasl.mechanisms': '{}', #         'sasl.username': '{api key}', #         'sasl.password': '{api password}' #     }      ## Recommended way of loading secrets from .env file import os from dotenv import load_dotenv # Load environment variables load_dotenv() def load_config():     \"\"\"Load Kafka configuration.\"\"\"     return {         'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),         'security.protocol': os.getenv('SECURITY_PROTOCOL'),         'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),         'sasl.username': os.getenv('SASL_USERNAME'),         'sasl.password': os.getenv('SASL_PASSWORD')     }  # in python  config = load_config() In\u00a0[14]: Copied! <pre>from confluent_kafka.admin import AdminClient, NewTopic\n\nadmin_client = AdminClient(config)\n\n# List all topics \n# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n\ntopic_metadata = admin_client.list_topics(timeout=5)\nlist(topic_metadata.topics.keys())\n</pre> from confluent_kafka.admin import AdminClient, NewTopic  admin_client = AdminClient(config)  # List all topics  # https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html  topic_metadata = admin_client.list_topics(timeout=5) list(topic_metadata.topics.keys()) Out[14]: <pre>['demo_3_producer_trial3_applicants',\n 'test_1_topic',\n 'demo_3_producer_trial2_soccer',\n 'demo_3_producer_trial3_evaluation',\n 'demo_3_producer_trial1',\n 'demo1_free_text',\n 'topic_example_v1']</pre> In\u00a0[22]: Copied! <pre># Create callback function\n\n# Producer callback\ndef delivery_report(err, msg):\n\"\"\"Callback to report the result of the produce operation.\"\"\"\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n</pre> # Create callback function  # Producer callback def delivery_report(err, msg):     \"\"\"Callback to report the result of the produce operation.\"\"\"     if err is not None:         print(f\"Message delivery failed: {err}\")     else:         print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")  In\u00a0[31]: Copied! <pre>from dataclasses import dataclass\nimport json\n\n@dataclass\nclass RiderRequest:\n    name: str\n    lat: float\n    long: float\n\n    def serialize(self) -&gt; str:\n\"\"\"Serializes the object in the desired string format\"\"\"\n        return f\"rider {self.name} requests a car at ({self.lat:.2f}, {self.long:.2f})\"\n\ndef produce_messages(producer, topic_name, num_messages=10):\n    names = [\"Alice\", \"Bob\", \"Charlie\"]\n\n    for i in range(num_messages):\n        name = random.choice(names)\n        lat = random.uniform(-90, 90)\n        long = random.uniform(-180, 180)\n        message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.\n\n        ## serialization of the message_value\n        request = RiderRequest(name=name, lat=lat, long=long)\n        serialized_message = request.serialize()\n\n        print(serialized_message)\n        producer.produce(topic_name, key=message_key, value=serialized_message, callback=delivery_report)\n        producer.poll(0.1)\n    producer.flush()\n</pre> from dataclasses import dataclass import json  @dataclass class RiderRequest:     name: str     lat: float     long: float      def serialize(self) -&gt; str:         \"\"\"Serializes the object in the desired string format\"\"\"         return f\"rider {self.name} requests a car at ({self.lat:.2f}, {self.long:.2f})\"  def produce_messages(producer, topic_name, num_messages=10):     names = [\"Alice\", \"Bob\", \"Charlie\"]      for i in range(num_messages):         name = random.choice(names)         lat = random.uniform(-90, 90)         long = random.uniform(-180, 180)         message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.          ## serialization of the message_value         request = RiderRequest(name=name, lat=lat, long=long)         serialized_message = request.serialize()          print(serialized_message)         producer.produce(topic_name, key=message_key, value=serialized_message, callback=delivery_report)         producer.poll(0.1)     producer.flush()  In\u00a0[\u00a0]: Copied! <pre># # Define message to generate\n\n# import random\n# def produce_messages(producer, topic_name, num_messages=10):\n#     \"\"\"Produce messages with the given format.\"\"\"\n#     names = [\"Alice\", \"Bob\", \"Charlie\"]\n\n#     for i in range(num_messages):\n#         name = random.choice(names)\n#         lat = random.uniform(-90, 90)\n#         long = random.uniform(-180, 180)\n#         message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.\n#         message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"\n        \n#         producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)\n#         producer.poll(0.1)  # To trigger the delivery report callback for feedback\n\n#     producer.flush()  # Ensure all messages are sent\n</pre> # # Define message to generate  # import random # def produce_messages(producer, topic_name, num_messages=10): #     \"\"\"Produce messages with the given format.\"\"\" #     names = [\"Alice\", \"Bob\", \"Charlie\"]  #     for i in range(num_messages): #         name = random.choice(names) #         lat = random.uniform(-90, 90) #         long = random.uniform(-180, 180) #         message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed. #         message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"          #         producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report) #         producer.poll(0.1)  # To trigger the delivery report callback for feedback  #     producer.flush()  # Ensure all messages are sent In\u00a0[32]: Copied! <pre>from confluent_kafka import Producer\n\n## main: Producer\nproducer = Producer(config)\ntopic_name = \"topic_example_v1\"\nproduce_messages(producer, topic_name)\n</pre> from confluent_kafka import Producer  ## main: Producer producer = Producer(config) topic_name = \"topic_example_v1\" produce_messages(producer, topic_name)   <pre>rider Bob requests a car at (-27.66, 130.09)\nrider Charlie requests a car at (65.48, -90.80)\nrider Alice requests a car at (72.15, -110.09)\nrider Alice requests a car at (32.27, -95.44)\nrider Alice requests a car at (-39.97, -168.07)\nrider Alice requests a car at (-33.27, -85.01)\nrider Charlie requests a car at (-19.57, 100.28)\nrider Bob requests a car at (-11.50, 141.31)\nMessage delivered to topic_example_v1 [2] at offset 541\nMessage delivered to topic_example_v1 [2] at offset 542\nrider Alice requests a car at (38.43, 40.31)\nMessage delivered to topic_example_v1 [0] at offset 520\nMessage delivered to topic_example_v1 [0] at offset 521\nMessage delivered to topic_example_v1 [0] at offset 522\nrider Bob requests a car at (0.37, 30.56)\nMessage delivered to topic_example_v1 [2] at offset 543\nMessage delivered to topic_example_v1 [1] at offset 569\nMessage delivered to topic_example_v1 [1] at offset 570\nMessage delivered to topic_example_v1 [1] at offset 571\nMessage delivered to topic_example_v1 [1] at offset 572\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"assets/msds-lec2-topic-producer/demo_b_producer_serialization/#notebook-asynchronous-producer-w-serialization","title":"Notebook - Asynchronous Producer w/ Serialization\u00b6","text":"<p>Serialization with JSON:</p> <ul> <li>The serialization in <code>RiderRequest</code> now produces a formatted string as per your new requirements.</li> <li>In the <code>produce_messages()</code> function, I've created an instance of <code>RiderRequest</code> and then called the <code>serialize()</code> method on it to generate the desired message value.</li> <li>You had commas at the end of your lat and long assignments, which would make them tuples. I've corrected that mistake.</li> </ul> <p>The <code>dataclass</code> decorator plays a couple roles in this producer code that uses serialization. It provides a structured data model that serializes cleanly. The serialization happens outside of the class, so dataclasses are not required. But they make the code and data model clearer.</p> <ol> <li>It defines the RiderRequest data model in a structured way:</li> </ol> <ul> <li><p>The fields (name, lat, long) are explicitly declared with types.</p> </li> <li><p>Using a class lets you encapsulate serialization logic in the serialize() method.</p> </li> <li><p>The dataclass decorator autogenerates init, repr, etc.</p> </li> </ul> <ol> <li>It allows creating RiderRequest objects easily:</li> </ol> <ul> <li><p>Can instantiate with RiderRequest(name, lat, long)</p> </li> <li><p>Cleaner than using a dictionary or tuple</p> </li> </ul> <ol> <li>It integrates well with serialization:</li> </ol> <ul> <li><p>Libraries like Avro/Protobuf work well with classes/structs</p> </li> <li><p>The data is organized logically for serialization</p> </li> <li><p>serialize() method has access to fields directly</p> </li> </ul>"},{"location":"assets/msds-lec2-topic-producer/demo_c_producer_compare/","title":"Notebook - Compare Async and Sync Producers","text":"In\u00a0[6]: Copied! <pre># Topic topic_example_v1 has been created\n\nfrom confluent_kafka.admin import AdminClient, NewTopic\n\n# def load_config():\n#     \"\"\"Load Kafka configuration.\"\"\"\n#     return {\n#         'bootstrap.servers': '{server}',\n#         'security.protocol': '{}',\n#         'sasl.mechanisms': '{}',\n#         'sasl.username': '{api key}',\n#         'sasl.password': '{api password}'\n#     }\n    \n## Recommended way of loading secrets from .env file\nimport os\nfrom dotenv import load_dotenv\n# Load environment variables\nload_dotenv()\ndef load_config():\n\"\"\"Load Kafka configuration.\"\"\"\n    return {\n        'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n        'security.protocol': os.getenv('SECURITY_PROTOCOL'),\n        'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),\n        'sasl.username': os.getenv('SASL_USERNAME'),\n        'sasl.password': os.getenv('SASL_PASSWORD')\n    }\n\n# in python \nconfig = load_config()\n</pre> # Topic topic_example_v1 has been created  from confluent_kafka.admin import AdminClient, NewTopic  # def load_config(): #     \"\"\"Load Kafka configuration.\"\"\" #     return { #         'bootstrap.servers': '{server}', #         'security.protocol': '{}', #         'sasl.mechanisms': '{}', #         'sasl.username': '{api key}', #         'sasl.password': '{api password}' #     }      ## Recommended way of loading secrets from .env file import os from dotenv import load_dotenv # Load environment variables load_dotenv() def load_config():     \"\"\"Load Kafka configuration.\"\"\"     return {         'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),         'security.protocol': os.getenv('SECURITY_PROTOCOL'),         'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),         'sasl.username': os.getenv('SASL_USERNAME'),         'sasl.password': os.getenv('SASL_PASSWORD')     }  # in python  config = load_config() In\u00a0[7]: Copied! <pre>from confluent_kafka.admin import AdminClient, NewTopic\n\nadmin_client = AdminClient(config)\n\n# List all topics \n# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n\ntopic_metadata = admin_client.list_topics(timeout=5)\nlist(topic_metadata.topics.keys())\n</pre> from confluent_kafka.admin import AdminClient, NewTopic  admin_client = AdminClient(config)  # List all topics  # https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html  topic_metadata = admin_client.list_topics(timeout=5) list(topic_metadata.topics.keys()) Out[7]: <pre>['demo_3_producer_trial3_applicants',\n 'test_1_topic',\n 'demo_3_producer_trial2_soccer',\n 'demo_3_producer_trial3_evaluation',\n 'demo_3_producer_trial1',\n 'demo1_free_text',\n 'topic_example_v1']</pre> In\u00a0[8]: Copied! <pre># Create callback function\n\n# Producer callback\ndef delivery_report(err, msg):\n\"\"\"Callback to report the result of the produce operation.\"\"\"\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n</pre> # Create callback function  # Producer callback def delivery_report(err, msg):     \"\"\"Callback to report the result of the produce operation.\"\"\"     if err is not None:         print(f\"Message delivery failed: {err}\")     else:         print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")  In\u00a0[9]: Copied! <pre># Define message to generate\n\nimport random\ndef produce_messages(producer, topic_name, num_messages=10):\n\"\"\"Produce messages with the given format.\"\"\"\n    names = [\"Alice\", \"Bob\", \"Charlie\"]\n\n    for i in range(num_messages):\n        name = random.choice(names)\n        lat = random.uniform(-90, 90)\n        long = random.uniform(-180, 180)\n        message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.\n        message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"\n        \n        producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)\n        producer.poll(0.1)  # To trigger the delivery report callback for feedback\n\n    producer.flush()  # Ensure all messages are sent\n    ## Using producer.flush() before shutting down your producer ensures that all messages are delivered and that any callbacks associated with these messages are executed.\n\nfrom confluent_kafka import Producer\n\n## main: Producer\nproducer = Producer(config)\ntopic_name = \"topic_example_v1\"\nproduce_messages(producer, topic_name,num_messages=10)\n</pre> # Define message to generate  import random def produce_messages(producer, topic_name, num_messages=10):     \"\"\"Produce messages with the given format.\"\"\"     names = [\"Alice\", \"Bob\", \"Charlie\"]      for i in range(num_messages):         name = random.choice(names)         lat = random.uniform(-90, 90)         long = random.uniform(-180, 180)         message_key = f\"rider-{name}-{i}\"  # Just as an example. Choose a meaningful key if needed.         message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"                  producer.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)         producer.poll(0.1)  # To trigger the delivery report callback for feedback      producer.flush()  # Ensure all messages are sent     ## Using producer.flush() before shutting down your producer ensures that all messages are delivered and that any callbacks associated with these messages are executed.  from confluent_kafka import Producer  ## main: Producer producer = Producer(config) topic_name = \"topic_example_v1\" produce_messages(producer, topic_name,num_messages=10) <pre>Message delivered to topic_example_v1 [2] at offset 526\nMessage delivered to topic_example_v1 [2] at offset 527\nMessage delivered to topic_example_v1 [0] at offset 507\nMessage delivered to topic_example_v1 [0] at offset 508\nMessage delivered to topic_example_v1 [0] at offset 509\nMessage delivered to topic_example_v1 [1] at offset 547\nMessage delivered to topic_example_v1 [1] at offset 548\nMessage delivered to topic_example_v1 [1] at offset 549\nMessage delivered to topic_example_v1 [1] at offset 550\nMessage delivered to topic_example_v1 [1] at offset 551\n</pre> In\u00a0[10]: Copied! <pre>## synchronous producer\n\nimport random\ndef produce_messages_synchronously(producer, topic_name, num_messages=10):\n\"\"\"Produce messages synchronously.\"\"\"\n    names = [\"Alice\", \"Bob\", \"Charlie\"]\n\n    for i in range(num_messages):\n        name = random.choice(names)\n        lat = random.uniform(-90, 90)\n        long = random.uniform(-180, 180)\n        message_key = f\"rider-{name}-{i}\"  # Example key\n        message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"\n        \n        producer.produce(topic_name, key=message_key, value=message_value)\n        \n        # Wait for message delivery and handle the result\n        result = producer.flush(timeout=10)  # Adjust the timeout as needed\n        \n        if result &gt; 0:\n            print(f\"Failed to deliver {result} messages\")\n        else:\n            print(\"All messages delivered successfully\")\n</pre> ## synchronous producer  import random def produce_messages_synchronously(producer, topic_name, num_messages=10):     \"\"\"Produce messages synchronously.\"\"\"     names = [\"Alice\", \"Bob\", \"Charlie\"]      for i in range(num_messages):         name = random.choice(names)         lat = random.uniform(-90, 90)         long = random.uniform(-180, 180)         message_key = f\"rider-{name}-{i}\"  # Example key         message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\"                  producer.produce(topic_name, key=message_key, value=message_value)                  # Wait for message delivery and handle the result         result = producer.flush(timeout=10)  # Adjust the timeout as needed                  if result &gt; 0:             print(f\"Failed to deliver {result} messages\")         else:             print(\"All messages delivered successfully\")               In\u00a0[11]: Copied! <pre>from confluent_kafka import Producer\n\n# Main Execution\nproducer = Producer(config)\ntopic_name = \"topic_example_v1\"\nproduce_messages_synchronously(producer, topic_name, num_messages=10)\n</pre> from confluent_kafka import Producer  # Main Execution producer = Producer(config) topic_name = \"topic_example_v1\" produce_messages_synchronously(producer, topic_name, num_messages=10) <pre>All messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\nAll messages delivered successfully\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"assets/msds-lec2-topic-producer/demo_c_producer_compare/#notebook-compare-async-and-sync-producers","title":"Notebook - Compare Async and Sync Producers\u00b6","text":""},{"location":"assets/msds-lec2-topic-producer/demo_c_producer_compare/#summary","title":"Summary\u00b6","text":"<p>The <code>producer.flush(timeout=10)</code> method in the Confluent Kafka Python client is used to ensure that all outstanding/queued messages are delivered and all callbacks are invoked. The <code>flush()</code> method works by waiting until all the messages in the producer's local queue are delivered to the Kafka broker, or the provided timeout (in seconds) expires.</p> <p>Here's a breakdown of what happens:</p> <ol> <li><p>The <code>flush()</code> method initiates the delivery of all the messages that are currently in the producer's local queue.</p> </li> <li><p>It then waits for the acknowledgments from the broker for these messages.</p> </li> <li><p>If the acknowledgments for all the messages are received before the provided timeout, <code>flush()</code> will return 0, indicating that all messages have been successfully delivered.</p> </li> <li><p>If the provided timeout expires before acknowledgments for all messages are received, <code>flush()</code> will return a non-zero value indicating the number of messages that remain in the queue (i.e., the number of messages that haven't been successfully delivered).</p> </li> </ol> <p>In the synchronous producer code you provided, after each message is produced, the <code>flush()</code> method is called with a timeout of 10 seconds. This means that after producing each message, the code will wait up to 10 seconds for the acknowledgment of that message from the Kafka broker. If the acknowledgment is received within that timeframe, the message is considered delivered; otherwise, the code will move to the next iteration of the loop (or exit the loop) and the <code>if-else</code> block will print out how many messages failed to be delivered.</p> <p>This use of <code>flush()</code> after producing each message gives the producer synchronous behavior: it won't proceed until it has confirmation that the current message has been delivered or the timeout is reached.</p>"},{"location":"lec1/","title":"Lecture 1. Streaming Processing","text":"<p>Author: Jeremy Gu</p>"},{"location":"lec1/#basics","title":"Basics","text":"<p>Stream processing involves continuous calculations on constantly evolving data streams in real-time. Key aspects:</p> <ul> <li> <p>Data streams are unbounded sequences (potentially endless) of data that can change in shape and size over time. </p> </li> <li> <p>Immutable data - once in a stream, data is fixed and append-only. Existing data cannot be modified. Updates create new events.</p> </li> <li> <p>Varying data rates - streams can have bursts and lulls at uneven frequencies.</p> </li> <li> <p>Low latency - results are produced with minimal delays to enable real-time actions.</p> </li> <li> <p>Fault tolerance - streaming systems must handle failures gracefully.</p> </li> </ul>"},{"location":"lec1/#streaming-data-characteristics","title":"Streaming Data Characteristics","text":"<ul> <li> <p>Small data sizes - events are typically less than 1 MB.</p> </li> <li> <p>High throughput - streams sustain high input data velocities. </p> </li> <li> <p>Irregular arrival patterns - events arrive at inconsistent, uneven frequencies.</p> </li> </ul>"},{"location":"lec1/#events-in-streaming-data","title":"Events in Streaming Data","text":"<p>Events capture immutable facts about something that happened in the system. </p> <ul> <li>Example: GPS pings, Ads Clicks, purchases, sensor readings, etc.</li> </ul> <p>Event data is dynamic, short-lived and not intended to be stored for a long duration, compared to traditional databases that overwrite older data.</p> <p>Event producers emit facts without targeting specific consumers, unlike messaging queues which often have designated receivers.</p> <p>Note: On updating existing record. To 'update' an event in an immutable system, one doesn't modify the existing event. Instead, a new event (or record) is appended to represent the change or the new state. Subsequent processing or reading systems can then consider the latest event as the 'current' state, effectively overriding the previous event.</p> <p>Info</p> <p>We will cover more on message queues in Additional Topics. </p>"},{"location":"lec1/#stream-processing-and-batch-processing","title":"Stream Processing and Batch Processing","text":"<p>In the realm of data engineering, batch and stream processing serve as two prominent paradigms. </p> <pre><code>          +-----------------------------+\n          |                             |\n    +-----&gt;     Batch Processing        |\n    |     |                             |\n    |     +-----------------------------+\n    |     |   Dataset 1   |   Dataset 2   | ... |   Dataset N   |\n    |     +-----------------------------+---------------------+\n    |      \n    |      Time -------------------------&gt;\n\n    |      +-----------------------------+\n    |      |                             |\n    +------&gt;   Streaming Processing      |\n           |                             |\n           +-----------------------------+\n           | Event 1 | Event 2 | Event 3 | ... | Event N |\n           +-----------------------------+---------------------+\n\n           Time -------------------------&gt;\n</code></pre>"},{"location":"lec1/#comparison-of-batch-and-stream-processing","title":"Comparison of Batch and Stream Processing","text":"Batch Processing Stream Processing Definition and Usage Periodic analysis of unrelated groups of data. Historically common for data engineers. Continuous analysis as new events are generated. Operation Frequency Runs on a scheduled basis. Runs at whatever frequency events (that may be uncertain) are generated. Duration &amp; Storage May run for a longer period of time and write results to a SQL-like store. Typically runs quickly, updating in-memory aggregates. Stream Processing applications may simply emit events themselves, rather than write to an event store. Data Analyzed May analyze all historical data at once. Typically analyzes trends over a limited period of time due to data volume. Some batch jobs might only process a subset of the available data. Data Mutability Typically works with mutable data and data stores. Typically analyzes immutable data and data stores. Examples (1) Aggregates the last hour of data every hour. This can help in capturing hourly user engagement metrics. (2) Results usually written to a SQL-like store, like aggregating monthly sales data for financial reporting. (1) Real-time fraud detection based on latest transactions. (2) Social media trends based on real-time posts and interactions. Pros (1) Data consumers receive updates periodically. (2) Can leverage all available data in the system. (1) Outputs are up-to-date with the latest event. (2) Typically deals with immutable data, ensuring data consistency. Cons (1) Best resolution of data is based on the batch interval. (2) Data might be mutable, meaning records can change between batches. This might lead to inconsistencies. (1) Might lack full historical context. For instance, while it can capture sudden spikes in website traffic, it might not have the context of the overall daily or monthly trend. (2) Best suited for short-term trends."},{"location":"lec1/#key-distinctions-between-batch-and-stream-processing","title":"Key Distinctions Between Batch and Stream Processing","text":"Key Characteristics Batch Processing Stream Processing Nature of Data Processed Operates on finite stored datasets. Handles near-infinite streams. Job Frequency Runs jobs at discrete intervals. Continuous processing. Result Availability Results are available later. Results are available with low latency. Focus &amp; Accuracy Emphasizes on completeness. Emphasizes on low latency but can offer accurate results with \"exactly-once\" semantics. <p>Note: \"Exactly-once semantics\" means the system has mechanisms in place to ensure every piece of data is processed one time only.</p> <pre><code>Batch Processing:\n\n+---------+    +----------+    +----------+    +---------+   +---------+\n| Raw     | -&gt; | Extract  | -&gt; | Transform| -&gt; | Load    | -&gt;| Database|\n| Data    |    | &amp; Clean  |    | &amp; Model  |    | Process |   |         |\n+---------+    +----------+    +----------+    +---------+   +---------+\n              [Periodic intervals: e.g., daily, weekly]\n\nStreaming Processing:\n\n+---------+    +----------+     +--------+\n| Event   | -&gt; | Real-time| -&gt;  |Output  |\n| Stream  |    | Analysis |     |or DB   |\n+---------+    +----------+     +--------+\n              [Continuous: events processed as they arrive]\n</code></pre>"},{"location":"lec1/#general-notes","title":"General Notes","text":"<ul> <li>Distinctions &amp; Exceptions: While we often categorize \"batch processing\" as periodic and \"stream processing\" as real-time, these are broad generalizations. In practice:<ul> <li>Some batch jobs may operate close to real-time.</li> <li>Some stream processes might not be entirely real-time.</li> </ul> </li> <li>Hybrid Systems: Systems that integrate both batch and stream processing can harness the advantages of each approach, yielding more versatile and robust solutions.</li> <li>Interplay Between Batch and Stream: Batch systems often produce events that are subsequently processed in real-time by stream systems. This synergy bridges the divide between historical data processing and real-time analytics.<ul> <li>It's essential to recognize the unique strengths of each approach. Neither makes the other redundant. In the realm of data engineering, batch and stream processing often complement each other, rather than serving as alternatives.</li> </ul> </li> </ul>"},{"location":"lec1/#streaming-processing-application","title":"Streaming Processing Application","text":"<p>Two key components: Streaming data store and Streaming calculations.</p>"},{"location":"lec1/#e-commerce-example","title":"E-commerce Example","text":"<pre><code>+-------------------+          +------------------+          +---------------------+          +---------------------+\n| Customer places   |   ---&gt;   | System generates |   ---&gt;   | Processes the       |   ---&gt;   | Inform Warehouse &amp;  |\n| an order          |          | an order ID      |          | payment             |          | Send Delivery Info  |\n+-------------------+          +------------------+          +---------------------+          +---------------------+\n                                                                                        |\n                                                                                        v\n                                                                                +---------------+\n                                                                                | Notify User   |\n                                                                                +---------------+\n\nOutcome: Seamless integration of Kafka and stream processing applications for real-time e-commerce analytics.\n</code></pre> <p>When a customer places an order, the system of the e-commerce website will begin the tasks below:</p> <ul> <li> <p>The system creates an order ID</p> </li> <li> <p>Process payment </p> </li> <li> <p>Inform the warehouse for packaging</p> </li> <li> <p>Send delivery information to USPS or FedEx for shipping</p> </li> </ul> <p>The system creates an order, and the user receives a response that the order is being processed. </p> <p>1. Streaming data store: The e-commerce platform can use Kafka as a streaming data store to store event data such as users browsing, adding items to cart, and placing orders. Kafka stores data in time order and ensures data immutability.</p> <p>2. Stream processing calculation: Perform real-time calculation of users' browsing volume in the past 1 hour and generate reports. Consume real-time event data from Kafka, perform counting, and generate browsing volume report events output to the database. This allows observing some issues, such as some goods being too popular leading to low inventory, or finding issues in certain steps (like payment) that requires engineer maintenance. Other use cases of stream processing calculation:</p> <ul> <li> <p>Real-time tracking of most popular products (most clicked items)</p> </li> <li> <p>Real-time analysis of cart conversion rate (how many add to cart but do not purchase) </p> </li> <li> <p>Real-time generation of purchase suggestions e.g. \"Frequently bought together\"</p> </li> </ul> <p>Kafka stores the input data, stream processing applications perform real-time processing and analysis on data consumed from Kafka, and output results. The combination enables real-time data processing for the e-commerce platform.</p>"},{"location":"lec1/#summary-and-typical-data-processing-workflow","title":"Summary and Typical Data Processing Workflow","text":"<p>In summary, Streaming Data prioritizes immediacy. Stored Data prioritizes query-ability and depth. In many contexts, especially when distinguishing between \"Stored\" Data and Streaming data, it's generally implied that the data is stored in traditional, structured, and query-friendly systems, such as SQL-like databases.</p> Parameter/Feature Streaming Data (Real-time Data) Stored Data (SQL-like databases) Purpose Primarily used for real-time analytics and responses. Used for long-term storage and in-depth analysis. Typical Use Cases - Analyzing user behaviors such as browsing, clicking, and purchasing in real-time. - Making instantaneous decisions, e.g., adjusting product recommendations. - Product Details: Long-term storage of static info like product names, descriptions, categories, images, etc. - Historical Pricing: Time series data storing price fluctuations for trend analysis. - User Reviews: Long-term storage of feedback from customers. - Order Information: Permanent record of each transaction's details. Characteristics - Emphasizes immediacy and real-time actions. - Data might be transient or archived after being processed. - Emphasizes query-ability and historical context. - Data is persistently stored for future retrieval and analysis. <p>Typical Data Processing Workflow:</p> <ol> <li>Real-time Stream Processing: Streaming data undergoes real-time computations, providing insights like trending products.</li> <li>Data Storage: Results from real-time computations are written to storage systems.</li> <li>In-depth Analysis: Business intelligence systems query the stored data for comprehensive reports.</li> <li>Feedback Loop: Analytical results are integrated back into the product or business strategies, completing a full cycle.</li> </ol>"},{"location":"lec1/#course-coverage","title":"Course Coverage","text":"<pre><code>    +------------------+\n    |    Kafka (MQ)    |\n    +------------------+\n               |\n               v\n    +--------------------------+\n    | Stream Processing Apps   |\n    +--------------------------+\n               |\n               v\n    +------------------------+\n    | Confluent KSQL &amp; Faust  |\n    +------------------------+\n</code></pre> <p>Streaming Data Store : We will focus on using Kafka in the course. Kafka is a message queue system mainly used for processing and transporting real-time data streams. It is designed as a publish-subscribe system to ensure data can be consumed in real-time by multiple consumers.</p> <p>SQL stores like Cassandra will not be covered in our course. Cassandra is a database mainly used for long term data storage and querying. Although it supports stream data, its primary function is as a data storage system.</p> <p>Stream Processing Framework - A comprehensive set of tools and utilities, often bundled together as a library or a platform, which facilitates the creation and management of Stream Processing Applications. This framework offers components that handle various aspects of stream processing, such as data ingestion, real-time analytics, state management, and data output, allowing developers to focus on the specific business logic of their application. Here is a list of Common Stream Processing Application Frameworks, as follows:</p> <ul> <li> <p>Confluent KSQL - will be focused on in the course</p> </li> <li> <p>Faust Python Library - will be focused on in the course</p> </li> <li> <p>Apache Flink - will be mentioned in the course</p> </li> <li> <p>Apache Spark Structure Streaming - self-study needed</p> </li> <li> <p>Kafka Streams - self-study needed, especially for students proficient in Java</p> </li> <li> <p>Apache Samza - will not be covered in the course</p> </li> </ul>"},{"location":"lec1/#examples-of-using-data-streaming-services","title":"Examples of Using Data Streaming Services","text":""},{"location":"lec1/#shareride-requests","title":"Shareride Requests","text":"<ul> <li> <p>Input data: Continuous stream of location data from riders' smartphones using GPS, details about driver locations, and availability.</p> </li> <li> <p>Output: Match rider requests with the most suitable driver in real time.</p> </li> <li> <p>Problem statement: To enable efficient and swift ride dispatching, process incoming location streams, and match the closest available driver to a requesting rider.</p> </li> <li> <p>Models/Algorithms: Could use the Haversine formula for calculating distances between geo-coordinates; a greedy assignment algorithm to match riders with drivers based on proximity and availability. (Optional reading: Approximate Nearest Neighbor (ANN) algorithms could be more efficient when dealing with a vast number of drivers and riders in close proximity.)</p> </li> <li> <p>Streaming services: Kafka for ingesting streams of rider requests and driver location updates; Spark Streaming for processing and assignment.</p> </li> </ul> <p>Diagram and Example Data</p> <pre><code>+---------------------+\n| Rider's Smartphone  |\n|                     |\n|  - GPS Location     |\n+----------+----------+\n           |\n           v\n+----------+----------+\n| Kafka (Data Ingest) |\n+----------+----------+\n           |\n           v\n+----------+----------+\n| Spark Streaming App |\n|                     |\n|  - Haversine Calc.  |\n|  - Greedy Matching  |\n+----------+----------+\n           |\n           v\n+---------------------+\n|    Ride Dispatch    |\n|                     |\n|  - Matched Driver   |\n+---------------------+\n</code></pre> <p>In this diagram:</p> <ul> <li>The \"Rider's Smartphone\" is continuously sending GPS location data.</li> <li>The data is ingested into the system using <code>Kafka</code>.</li> <li><code>Spark Streaming</code> processes this data in real-time. It uses the Haversine formula to calculate distances and a greedy algorithm to match riders to drivers.</li> <li>The final result, a matched driver, is dispatched to the rider.</li> </ul> <p>Here's an example of how the JSON data might look for various components in the ride-sharing example:</p> <ol> <li>Rider's Smartphone Data:</li> <li>Represents the continuous stream of location data from a rider's smartphone.</li> </ol> <pre><code>{\n\"rider_id\": \"12345\",\n\"timestamp\": \"2023-10-18T14:00:00Z\",\n\"location\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n},\n\"ride_request\": true\n}\n</code></pre> <ol> <li>Driver Location and Availability Data:</li> <li>Represents the details about driver locations and availability.</li> </ol> <pre><code>{\n\"driver_id\": \"98765\",\n\"timestamp\": \"2023-10-18T14:00:05Z\",\n\"location\": {\n\"latitude\": 40.731000,\n\"longitude\": -73.934500\n},\n\"availability\": true\n}\n</code></pre> <ol> <li>Matched Ride Dispatch Data:</li> <li>Represents the output after processing the above inputs.</li> </ol> <pre><code>{\n\"ride_id\": \"67890\",\n\"rider\": {\n\"rider_id\": \"12345\",\n\"location\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n}\n},\n\"driver\": {\n\"driver_id\": \"98765\",\n\"location\": {\n\"latitude\": 40.731000,\n\"longitude\": -73.934500\n}\n},\n\"estimated_arrival_time\": \"2023-10-18T14:05:00Z\"\n}\n</code></pre>"},{"location":"lec1/#fraud-detection-in-payments","title":"Fraud Detection in Payments","text":"<ul> <li> <p>Input data: Continuous stream of financial transaction data, which includes purchases, withdrawals, and deposits.</p> </li> <li> <p>Output: Real-time alerts on suspicious or anomalous transactions.</p> </li> <li> <p>Problem statement: From a continuous stream of financial transactions, quickly identify and flag those that display patterns indicative of fraud.</p> </li> <li> <p>Models/Algorithms: Utilize unsupervised outlier detection models; clustering algorithms to identify and spotlight unusual patterns. Supervised learning techniques, such as Random Forests or Gradient Boosted Machines, could be employed when we have labeled data for fraudulent and non-fraudulent transactions. They can offer higher precision in anomaly detection compared to unsupervised methods. Also, mostly important process is to incorporate human feedback loop. </p> </li> <li> <p>Streaming services: Kafka for ingesting transactional data streams; Flink for real-time anomaly and pattern detection.</p> </li> </ul> <p>Diagram and Example Data</p> <p><pre><code>  +----------------------+\n  | Financial Transaction|\n  |      Data Source     |\n  +----------------------+\n           |\n           v\n  +-------------------+\n  |  Kafka Stream     |\n  |    (Ingestion)    |\n  +-------------------+\n           |\n           v\n  +------------------+\n  | Stream Processing|\n  |   (Flink)        |\n  +------------------+\n           |\n           |\n  +--------v-------+\n  |Machine Learning|\n  |   Models (e.g. |\n  |Random Forest)  |\n  +----------------+\n           |\n           v\n  +----------------------+\n  | Real-time Fraud Alert|\n  |  &amp; Feedback System   |\n  +----------------------+\n</code></pre> The data flows from top to bottom:</p> <ol> <li>Financial Transaction Data Source is where all transactions originate.</li> <li>This data streams into a Kafka Stream for ingestion.</li> <li>Stream Processing (with Apache Flink here) consumes this data, processes it in real-time, and applies various algorithms/models to detect potential fraud.</li> <li>Machine Learning Models, like Random Forest, analyze patterns in the data to make predictions about potential fraud.</li> <li>Detected anomalies are sent to a Real-time Fraud Alert &amp; Feedback System, which could involve notifying account holders, bank agents, or flagging transactions for review.</li> </ol> <p>Now, let's represent potential JSON data for this use case:</p> <p>Financial Transaction Data:    - Represents a single financial transaction event, such as a purchase.</p> <pre><code>{\n\"transaction_id\": \"abcd1234\",\n\"timestamp\": \"2023-10-18T14:10:00Z\",\n\"account_id\": \"A78901\",\n\"transaction_type\": \"purchase\",\n\"amount\": 500.00,\n\"currency\": \"USD\",\n\"merchant\": {\n\"name\": \"ElectronicsStore\",\n\"category\": \"Electronics\",\n\"location\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n}\n}\n}\n</code></pre> <p>Fraud Alert Data:    - Represents the output data after processing the transaction data.</p> <pre><code>{\n\"alert_id\": \"alert5678\",\n\"timestamp\": \"2023-10-18T14:10:05Z\",\n\"transaction_id\": \"abcd1234\",\n\"account_id\": \"A78901\",\n\"reason\": \"Unusual high amount\",\n\"action\": \"Flagged for review\"\n}\n</code></pre>"},{"location":"lec1/#package-delivery-tracking","title":"Package Delivery Tracking","text":"<ul> <li> <p>Input data: Continuous GPS pings from delivery drivers' smartphones; shipment status updates.</p> </li> <li> <p>Output: Real-time updates on package delivery status and estimated time of arrival.</p> </li> <li> <p>Problem statement: Using a continuous stream of location data, frequently update the delivery status, and apply real-time optimizations to delivery routes.</p> </li> <li> <p>Models/Algorithms: Use rules-based algorithms to infer delivery status based on GPS data; real-time algorithms for optimizing delivery routes based on traffic and other constraints. Optimizing routes based on certain metrics (e.g. minimizing costs, maximizing speed, or other metrics) when there are multiple drops in bundles in a single trip.</p> </li> <li> <p>Streaming services: Kafka to ingest streams of GPS data and delivery updates; Spark Streaming for route analysis and optimization.</p> </li> </ul> <p>Diagram and Example Data</p> <pre><code>  +----------------------+\n  | Package &amp; GPS Data   |\n  |     Data Source      |\n  +----------------------+\n           |\n           v\n  +-------------------+\n  |  Kafka Stream     |\n  |    (Ingestion)    |\n  +-------------------+\n           |\n           v\n  +--------------------+\n  | Stream Processing  |\n  |   (Spark Streaming)|\n  +--------------------+\n           |\n           v\n  +------------------+\n  |Rules-based Status|\n  |   Inference      |\n  +------------------+\n           |\n           v\n  +---------------------+\n  |Route Optimization   |\n  | Algorithms          |\n  +---------------------+\n           |\n           v\n  +----------------------+\n  |Real-time Package     |\n  |Status &amp; ETA Updates  |\n  +----------------------+\n</code></pre> <p>Here's the data flow:</p> <ol> <li>Package &amp; GPS Data Source: This is where the raw data (like GPS pings and shipment statuses) originates from the delivery drivers' devices.</li> <li>This data then flows into a Kafka Stream for ingestion.</li> <li>Stream Processing (using Spark Streaming in this case) takes this data and applies basic processing.</li> <li>Rules-based Status Inference analyses the processed data to infer the delivery status based on the GPS data.</li> <li>Route Optimization Algorithms take this inferred data, consider other variables (like traffic), and optimize the delivery routes.</li> <li>The results from the above process are then sent to a system that provides Real-time Package Status &amp; ETA Updates to the recipients or the main server.</li> </ol> <p>Here are some potential JSON data structures:</p> <ol> <li> <p>Package &amp; GPS Data Source: <pre><code>{\n\"driver_id\": \"D12345\",\n\"timestamp\": \"2023-10-18T14:30:45Z\",\n\"gps_coordinates\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n},\n\"shipment_status\": \"in_transit\",\n\"package_id\": \"PKG78910\"\n}\n</code></pre></p> </li> <li> <p>Kafka Stream (Ingestion): (This would look the same as the input data as Kafka is just ingesting the data.) <pre><code>{\n\"driver_id\": \"D12345\",\n\"timestamp\": \"2023-10-18T14:30:45Z\",\n\"gps_coordinates\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n},\n\"shipment_status\": \"in_transit\",\n\"package_id\": \"PKG78910\"\n}\n</code></pre></p> </li> <li> <p>Stream Processing (Spark Streaming): (The processed data might include more details, such as current speed or next destination.) <pre><code>{\n\"driver_id\": \"D12345\",\n\"timestamp\": \"2023-10-18T14:30:45Z\",\n\"gps_coordinates\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n},\n\"current_speed\": \"35 mph\",\n\"next_destination\": {\n\"latitude\": 40.741895,\n\"longitude\": -73.989308\n},\n\"shipment_status\": \"in_transit\",\n\"package_id\": \"PKG78910\"\n}\n</code></pre></p> </li> <li> <p>Rules-based Status Inference: (This step infers the status based on the processed data, e.g., whether a driver has reached the destination.) <pre><code>{\n\"driver_id\": \"D12345\",\n\"timestamp\": \"2023-10-18T14:35:00Z\",\n\"package_id\": \"PKG78910\",\n\"inferred_status\": \"delivered\"\n}\n</code></pre></p> </li> <li> <p>Route Optimization Algorithms: (This data structure might focus on optimal route details.) <pre><code>{\n\"driver_id\": \"D12345\",\n\"current_location\": {\n\"latitude\": 40.730610,\n\"longitude\": -73.935242\n},\n\"optimal_route\": [\n{\n\"latitude\": 40.741895,\n\"longitude\": -73.989308\n},\n{\n\"latitude\": 40.752880,\n\"longitude\": -73.977326\n}\n],\n\"estimated_arrival\": \"2023-10-18T15:10:00Z\"\n}\n</code></pre></p> </li> <li> <p>Real-time Package Status &amp; ETA Updates: (This is the final data that would be sent to end-users.) <pre><code>{\n\"package_id\": \"PKG78910\",\n\"current_status\": \"delivered\",\n\"driver_id\": \"D12345\",\n\"estimated_arrival\": \"2023-10-18T15:10:00Z\"\n}\n</code></pre></p> </li> </ol> <p>Note: The timestamps and coordinates provided in these examples are arbitrary and are for illustrative purposes only. Actual data will vary based on real-world inputs.</p>"},{"location":"lec1/#other-considerations","title":"Other Considerations","text":"<ul> <li> <p>Trade-offs: Every streaming solution has to deal with the balance between latency (how fast data is processed) and throughput (how much data can be processed in a time frame). There's also accuracy, especially in ML where a faster prediction may be less accurate.</p> </li> <li> <p>Stream Imperfections: Data in the real world is messy. Streams can have missing data, or data can arrive out of sequence. Handling these imperfections requires strategies like watermarking or windowing.</p> </li> <li> <p>Joining Streaming with Historical Data: Often, the value from streaming data comes when it's combined with larger, historical datasets. Doing this efficiently is a challenge in stream processing.</p> </li> <li> <p>Testing and Monitoring: Streaming systems are complex and require sophisticated monitoring solutions. There should be mechanisms to ensure data integrity, monitor system health, and handle failures gracefully.</p> </li> </ul>"},{"location":"lec1/#quiz","title":"Quiz","text":"<p>Question 1. What is a key characteristic of data streams?</p> <ul> <li>A) Bounded sequences </li> <li>B) Immutable data</li> <li>C) Regular data arrival patterns</li> <li>D) Potentially unbounded sequences</li> </ul> <p>Question 2. What is a difference between streaming and batch data processing?</p> <ul> <li>A) Streaming focuses on completeness while batch emphasizes speed </li> <li>B) Streaming uses finite stored data while batch uses infinite streams</li> <li>C) Batch has higher latency than streaming</li> <li>D) Batch runs at scheduled intervals while streaming is continuous </li> </ul> <p>Question 3. Which statement describes event data accurately?</p> <ul> <li>A) Event data overwrites older data </li> <li>B) Events directly target specific downstream consumers</li> <li>C) Events capture immutable facts about a system</li> <li>D) Events are typically larger than 1 MB in size</li> </ul> <p>Question 4. What does stream processing involve?</p> <ul> <li>A) Discrete jobs running at regular intervals</li> <li>B) One-time calculations on finite data</li> <li>C) Continuous calculations on evolving data streams</li> <li>D) Loading data batches from databases</li> </ul> <p>Question 5. Which is NOT a streaming data characteristic?</p> <ul> <li>A) Potentially high throughput </li> <li>B) Strictly ordered arrival patterns</li> <li>C) Low latency results</li> <li>D) Small data sizes for individual events</li> </ul> <p>Question 6. What are benefits of using Streaming Processing Applications?</p> <p>(Open-ended question. Think about the themes like \"use cases\", \"speed\", \"scalability\", \"storage\", \"architecture\", etc.)</p>"},{"location":"lec1/#answers","title":"Answers","text":"<p>Question 1.</p> <p>D) Potentially unbounded sequences</p> <p>Explanation: Data streams are potentially infinite and unbounded in size, rather than having a fixed length like bounded sequences. This endless nature is a core characteristic.</p> <p>Question 2.</p> <p>D) Batch runs at scheduled intervals while streaming is continuous  </p> <p>Explanation: Batch processing runs at discrete scheduled intervals to process fixed datasets, while stream processing is continuous and operates on constantly evolving data streams.</p> <p>Question 3.</p> <p>C) Events capture immutable facts about a system</p> <p>Explanation: Events are immutable facts about occurrences in a system, rather than overwriting older data like in databases. Events also emit facts indirectly rather than targeting specific downstream systems.</p> <p>Question 4.</p> <p>C) Continuous calculations on evolving data streams</p> <p>Explanation: Stream processing continually performs calculations on live, updating data streams rather than finite stored data or intermittent batch jobs.</p> <p>Question 5.</p> <p>B) Strictly ordered arrival patterns</p> <p>Explanation: Irregular and uneven arrival patterns are a characteristic of streaming data. Strictly ordered patterns are not typical as streams can have bursts and lulls.</p>"},{"location":"lec1/1.1/","title":"Lec 1: Additional Topics","text":"<p>Author: Jeremy Gu</p> <p>Note</p> <p>In each lecture, we will have a section called \"Additional Topics\" covering concepts that, due to our fast-paced curriculum, we may not have time to cover in class. However, I believe understanding these is important for fully mastering data streaming. I may also include recommended readings and coding exercises to reinforce your learning. Note that additional topics will not be on the midterm exam. In your future work, I encourage revisiting these to strengthen your data streaming knowledge. </p>"},{"location":"lec1/1.1/#message-queues","title":"Message Queues","text":"<p>We introduce an example to demonstrate the concepts of Message Queues and Invented Systems in the context of an e-commerce website.</p> <p>Let's revisit the e-commerce website example to illustrate these concepts:</p>"},{"location":"lec1/1.1/#e-commerce-website-example","title":"E-commerce Website Example","text":"<p>Scenario: A customer places an order on an e-commerce website. Upon this action, the system needs to:</p> <ul> <li>Create an order ID.</li> <li>Process payment.</li> <li>Inform the warehouse for packaging.</li> <li>Send delivery information to USPS or FedEx for shipping.</li> </ul> <p>Case 1: Synchronous Processing</p> <ul> <li>User clicks \u201cPlace Order.\u201d</li> <li>The system begins all tasks one by one.<ul> <li>First, it creates an order ID.</li> <li>Next, it processes the payment.</li> <li>After that, it informs the warehouse.</li> <li>Finally, it sends the delivery information to USPS or FedEx.</li> </ul> </li> <li>Synchronous: Only after all these tasks are completed does the user receive a response.</li> <li>This Synchronous Processing doesn't use message queue. The user might have to wait a significant amount of time before getting feedback. </li> </ul> <p>Case 2: Asynchronous Processing with Message Queue</p> <ul> <li>User clicks \u201cPlace Order.\u201d</li> <li>The system quickly creates an order ID and sends the user a response that their order is being processed.</li> <li>The subsequent tasks (payment processing, notifying the warehouse, sending delivery info) are queued up in a message queue and are handled separately without making the user wait.</li> <li>The message queue acts as an intermediary, helping different services work together asynchronously, enhancing the system's responsiveness and decoupling.</li> </ul> <p>Decoupling - In the context of our example, decoupling means that different parts of the system (e.g., payment processing, warehouse notifications, and shipping notifications) operate independently. If one part fails or is slow, it doesn't directly impact the other parts. The use of a message queue helps in achieving this by allowing these parts to communicate without being directly connected.</p> <p>Asynchronous - Tasks or operations do not wait for the previous one to complete. They can start, run, and complete in overlapping time periods. In our example, after placing the order, the user doesn't have to wait for all tasks to complete. They immediately get feedback, while the system processes other tasks in the background.</p> <p>Synchronous - Tasks or operations happen in a specific order. Each task waits for the previous one to finish before starting. In our e-commerce example, it would mean the user waits for every single task to complete before getting any feedback.</p>"},{"location":"lec1/1.1/#append-only","title":"Append-only","text":"<p>Append-only logs serve as a fundamental mechanism both in stream processing and traditional SQL databases, but with different primary focuses. </p> <p>In streaming, they're vital for ensuring the real-time recording of events with guaranteed ordering, even at high rates. On the other hand, in conventional SQL databases, these logs primarily record and synchronize data changes between database nodes, ensuring data consistency.</p> <p>It's essential to recognize the diverse applications of this mechanism across various technological domains, highlighting its versatility and fundamental role in data management. This dual application elucidates the need for precision when adopting append-only logs, depending on the specific requirements and constraints of the given context.</p> Feature/Characteristic Stream Processing Traditional SQL Databases Primary Use High-throughput stream processing Data synchronization and backup Core Concept Append-only logs ensure data ordering and immutability Append-only or \"write-ahead logs\" record and synchronize all changes Application Real-time event recording, saved in the order they are received Data synchronization between primary and secondary database nodes Benefits Ensures events are correctly ordered even at high throughputs Ensures data consistency and synchronization between nodes Limitations \u2014\u2014 Append-only logs typically not suited for stream processing but are mainly for replication and backup"},{"location":"lec1/1.1/#optional-log-structured-storage-systems","title":"(Optional) Log-structured Storage Systems","text":"<p>1. Background</p> <ul> <li>Early Storage Challenges: Traditional storage inefficiencies with random write operations due to disk seek times.</li> <li>Solution: Emergence of log-structured storage to prioritize sequential writes, diminishing disk seek overhead.</li> <li>Transition: Continued relevance of log-structured storage with SSDs due to inherent benefits beyond disk optimization.</li> </ul> <p>2. Benefits of Log-structured Systems</p> <ul> <li>Efficiency: Continuous writes to the end of the log, optimizing disk throughput with minimized disk head movement.</li> <li>Reliability: Non-overwriting nature provides a natural version history and lowered data corruption risk.</li> <li>Concurrency: Enables multiple operations to write data simultaneously, ideal for multi-core processors and distributed architectures.</li> </ul> <p>3. Common Characteristics</p> <ul> <li>Immutable Data &amp; Append-Only Nature: Fundamental to the design, once data is written, it remains unaltered. All new data is added to the end, ensuring the append-only approach.</li> <li>Merge and Compaction: With log growth, older or less relevant data can be pruned, and logs can be merged for better efficiency.</li> <li>Log Files Handling: Logs can be processed based on:</li> <li>Time: Old logs might be archived or deleted after a pre-defined duration.</li> <li>Size: A new log is initiated once the current one hits a certain size.</li> </ul> <p>4. Challenges and Solutions</p> <ul> <li>Data Retrieval: Efficiently finding older data in sequential storage.</li> <li>Solution: Leveraging indexing and SSTables for speedy data segment access.</li> <li>Garbage Collection: Identifying and eliminating obsolete data over time.</li> <li>Obsolete Data Example: In a user activity log, a record showing a user liked a post followed by an entry indicating they unliked it can render the former redundant.</li> </ul> <p>5. Real-World Applications</p> <ul> <li>Apache Kafka: Adopted by corporations like LinkedIn for real-time activity tracking and analysis. Application scenario: Streamlining real-time analytics for website interactions.</li> <li>Apache Cassandra: Chosen by businesses like Instagram for large data volumes with high availability demands. Application scenario: Managing user profiles and activity feeds with low latency.</li> <li>Apache HBase: Used by Facebook for storing vast quantities of messages and posts, given its capacity to handle large, sparse datasets. Application scenario: Real-time analytics on user interactions and content delivery.</li> </ul>"},{"location":"lec1/1.1/#lambda-architecture-vs-kappa-architecture","title":"Lambda Architecture vs Kappa architecture","text":"<p>Lambda Architecture is a data processing architecture that uses both batch and stream-processing methods. Stream-processing is fast and caters to real-time requirements, while batch processing is thorough and handles large datasets. By combining the two, we can handle real-time data analytics without losing the depth that comes with batch processing. The beauty of Lambda Architecture is that it offers the best of both worlds: you get a snapshot of the present moment and also a detailed picture of the past. Here is Lambda Architecture from Databricks that covers more details.</p> <p>Example: Think of a social media platform where users post content and interact with one another. The platform wants to understand what's trending right now, but also wishes to make in-depth analyses periodically.</p> <ul> <li> <p>Speed Layer (Real-time): As soon as users post or interact, this data gets processed instantly, giving a real-time view of trends or popular posts. This is like looking at what's hot in the last hour or day.</p> </li> <li> <p>Batch Layer (Deep Analysis): Every day or week, the platform gathers all its data and performs a deep analysis, offering a comprehensive view. This could show things like the most influential users over a month or which topics consistently trended.</p> </li> <li> <p>Serving Layer (Combination): This is where the magic happens. It combines the real-time and the in-depth data, so that apps or analysts can get insights from both instant and historical data.</p> </li> </ul> <p>However, a Twitter architecture article in 2021 describes how Twitter migrated from a Lambda architecture to a Kappa-like architecture for large-scale real-time data processing. For more information on kappa architecture, please refer to What is the Kappa Architecture.</p> <p>Original Lambda Architecture:</p> <ul> <li>Batch layer - Process data in Hadoop using Scalding </li> <li>Speed layer - Real-time processing using Heron</li> <li>Serving layer - TSAR to query batch and real-time results</li> </ul> <p>Lambda Architecture had issues like data loss and high latency. Batch processing was also costly.</p> <p>(New) Kappa-like Architecture: </p> <ul> <li>Pre-processing - Data preprocessing on Kafka</li> <li>Real-time processing - Aggregation on Google Cloud Dataflow</li> <li>Serving layer - Query results from BigQuery and BigTable</li> </ul> <p>Twitter migrated from a Lambda architecture to a Kappa-like architecture by removing the batch path and relying solely on real-time processing. This improved performance and costs while maintaining accuracy. Key highlights below:</p> <ul> <li>Removed the batch processing path, only real-time path remains</li> <li>Reduced latency, improved throughput and accuracy</li> <li>Simplified architecture and reduced costs</li> <li>Validated accuracy by comparing to batch results</li> </ul>"},{"location":"lec1/1.1/#optional-reading-real-world-examples","title":"(Optional Reading) Real World Examples","text":"<p>The paper Shopper intent prediction from clickstream e-commerce data with minimal browsing information<sup>1</sup> addresses the problem of predicting whether a user will make a purchase during their session on an e-commerce website, based on their clickstream data.</p> <p>Here is a summary of the clickstream prediction problem formulation for Alice's example session, used in the paper. In this case, the clickstream data from users browsing the ecommerce site is a data stream - a continuous sequence of user actions happening in real time. Each user action like a page view, add to cart, purchase etc is considered an event. The rich clickstream sessions are distilled into symbolic trajectories retaining only event types. The prediction task is to take Alice's symbolic trajectory as input and predict whether she will make a purchase in that session.</p> <ul> <li> <p>A-&gt;B: Getting Formatted Data From Streams Alice's actual browsing session contains rich metadata like time, product info, etc (Layer A in Figure 1). </p> </li> <li> <p>B-&gt;C: Labeling Certain Events This is simplified into a minimal symbolic trajectory by retaining only the event types (page view, add to cart, etc.) as integers (Layer C in Figure 1).</p> </li> </ul> <p>The window of each session: It is segmented into sessions using a 30 min threshold. Events in a session are symbolized into a sequence retaining only the event type. For Alice's session, this results in a symbolic trajectory of event types. Her actual session is simplified to a symbolic sequence of events. This symbolic sequence is used to make the purchase prediction. The authors target only these minimal symbolic trajectories (Layer C) in experiments, providing a benchmark for methods that leverage the richer session metadata (Layer A). The prediction task is to take Alice's symbolic trajectory as input and predict if she will purchase in that session. </p> <p></p> <p>Data Processing:</p> <ul> <li>The raw clickstream data is sessionized and symbolized in pre-processing.</li> <li>Experiments are done on both balanced and imbalanced datasets.</li> <li>Analysis provides insights into how different amounts of clickstream data affects prediction performance.</li> </ul> <p>Output:</p> <p>The response is a binary prediction of whether the user will purchase something or not in that session.</p> <ul> <li>Page view event is mapped to symbol '1'</li> <li>Detail event (user views product page) is mapped to '2'</li> <li>Add event (add to cart) is mapped to '3'</li> <li>Remove event (remove from cart) is mapped to '4'</li> <li>Purchase event (buy product) is mapped to '5'</li> <li>Click event (click result after search) is mapped to '6'</li> </ul> <p>Machine Learning Approach:</p> <ul> <li>The paper compares two approaches:</li> <li>Hand-crafted feature engineering with classical ML classifiers</li> <li>Deep Learning based classification using LSTM architectures</li> <li>For feature engineering, they use k-gram statistics and visibility graph motifs as features.</li> <li>For deep learning, they benchmark and improve on previous LSTM models for sequential data.</li> </ul> <p></p> <ol> <li> <p>Requena, B., Cassani, G., Tagliabue, J. et al. Shopper intent prediction from clickstream e-commerce data with minimal browsing information. Sci Rep 10, 16983 (2020). https://doi.org/10.1038/s41598-020-73622-y\u00a0\u21a9</p> </li> </ol>"},{"location":"lec1/assignment/","title":"Assignment 1","text":"<p>Assignment #1: Kafka Producers Performance Analysis</p> <p>Due Date: 10/28/2023 by 11:59pm</p> <p>Scoring: Maximum of 10 points. Even with extra credits, the total score will not exceed 10 points.</p> <p>HW Submission: Canvas</p> <p>Pre-requisites:</p> <pre><code>- Completion of Demo #1\n- Understanding Coding Examples in Lec 2. \n- Creation of a Confluent Cloud account using your USF Gmail account (preferred).\n</code></pre> <p>Problem 1: Confluent Cloud Producers Performance Analysis</p> <p>Objective: You will set up your cluster in the Confluent Cloud, use both asynchronous and synchronous Kafka producers, and compare their performance.</p> <p>Tasks:</p> <ol> <li> <p>Confluent Cloud Setup:     a. If you haven't already, sign up for Confluent Cloud.     b. Configure your Kafka producers using the provided sample:     <pre><code>config = {\n'bootstrap.servers': 'YOUR_BOOTSTRAP_SERVER',\n'security.protocol': 'SASL_SSL',\n'sasl.mechanisms': 'PLAIN',\n'sasl.username': 'YOUR_USERNAME',\n'sasl.password': 'YOUR_PASSWORD'\n}\n</code></pre></p> </li> <li> <p>Producer Implementation:</p> <p>a. Create an asynchronous producer. You may design your own or reuse the example provided in the class.</p> <p>b. Similarly, develop a synchronous producer.</p> </li> <li> <p>Performance Benchmarking:</p> <p>a. For the testing phase, set <code>num_messages = 20,000</code> or larger number.</p> <p>b. For both types of producers, track the time taken to send batches of <code>500</code> messages. The <code>time</code> module in Python will be useful for this task.</p> <p>c. Store the elapsed time for each batch in a suitable data structure of your choice.</p> </li> <li> <p>Analysis:</p> <p>a. Visualize the elapsed time data using a graph (possible tools: <code>matplotlib</code> or <code>seaborn</code>). The graph should provide insights into the performance variation of the two producers over each batch.</p> <p>b. Write an analysis (minimum 150 words) elucidating:</p> <pre><code>- The faster producer among the two.\n- Possible reasons for the observed performance differences.\n- Advantages and disadvantages of each producer type.\n</code></pre> </li> <li> <p>Deliverables:</p> <p>a. An organized Python Notebook (<code>.ipynb</code>) encapsulating all your code, visualizations, and concise written analysis.</p> <p>b. Ensure your code is well-commented, adhering to best practices, and is easy for coworkers to follow.</p> </li> </ol> <p>Grading Breakdown:</p> <ul> <li>Code quality and readability: 3pts</li> <li>Data visualization and performance analysis: 2pts</li> <li>Extra credit: when you're using your own data schema with dataclass (+1pt) and seriealization (+1pt).</li> <li>Extra creidt: when you're using another performance metric (other than the elapsed time per 500 messages) to measure Producer Performance. (See Appendix) </li> </ul> <p>Notes: </p> <ul> <li>Always back up your code and results.</li> <li>Stop the Confluent Cluster if you're not using it. (Save $$)</li> <li>Discussion is encouraged, but direct copying is not. Always cite your sources.</li> </ul> <p>Problem 2: Reading and Understanding the Kafka Producer</p> <p>Objective: Review and understand the provided Kafka producer code from the Confluent Developer website. Once you've gone through the code, answer the following questions. Please provide concise and straight forward answers (1-2 sentences.) Understanding the code from other people is important at work. So this exercise will help you hone your skills in reviewing code.</p> <ol> <li> <p>Configuration: 1 pt</p> <ul> <li>What is the purpose of the <code>config_file</code> argument in the script?</li> <li>How does the script handle the configuration file to setup the Kafka producer? Mention the Python modules used.</li> <li>Explain the significance of the line <code>config = dict(config_parser['default'])</code>.</li> </ul> </li> <li> <p>Producer Instance: 1 pt</p> <ul> <li>How is the Kafka producer instance created in the code?</li> <li>What configuration does it use to set up the producer?</li> </ul> </li> <li> <p>Delivery Callback: 1 pt</p> <ul> <li>What is the purpose of the <code>delivery_callback</code> function?</li> <li>In what scenarios is the error message printed in the delivery callback?</li> <li>How is the successful delivery of a message indicated in the callback?</li> </ul> </li> <li> <p>Message Production: 1pt</p> <ul> <li>To which topic are the messages being produced?</li> <li>Describe the logic behind the selection of <code>user_id</code> and <code>product</code> for each message.</li> <li>What does the <code>callback</code> argument do in the <code>producer.produce()</code> method?</li> </ul> </li> <li> <p>Final Actions: 1pt</p> <ul> <li>What is the purpose of the <code>producer.poll(10000)</code> line? What does the argument <code>10000</code> represent?</li> <li>Why is the <code>producer.flush()</code> method used at the end of the script?</li> </ul> </li> <li> <p>General Understanding: extra credit +1pt</p> <ul> <li>If you were to enhance this script to improve error handling or extend its functionality, what would you recommend?</li> </ul> </li> </ol> <p>Deliverables:</p> <p>Please submit your solutions in a file named <code>assignment1_{your name}</code>. This can be in raw markdown (.md) or text (.txt) format. Ensure your document does not have excessive formatting, as this may distract from the content of your answers.</p> <p>Appendix </p> <p>The metrics below are used to assess the performance of a Kafka producer. They measure slightly different aspects. </p> <ol> <li>Producer Response Rate:</li> <li>This metric tracks how many messages are being successfully delivered (acknowledged by the broker) over a period of time.</li> <li>It provides insight into how well the producer is succeeding in its primary role of sending messages to the broker.</li> <li> <p>Typically, the \"response\" in this context refers to the broker's acknowledgment for a message or a batch of messages.</p> </li> <li> <p>Request Rate:</p> </li> <li>This metric counts the number of requests the producer sends per second.</li> <li>Unlike the producer response rate, this considers both successful and failed attempts. Thus, a producer with a high request rate could still have a lot of failed deliveries if the system is overwhelmed or there are other issues.</li> <li> <p>This is more about the workload or activity level of the producer. A high rate might mean the producer is attempting to send messages rapidly, but it doesn't indicate how successful those attempts are.</p> </li> <li> <p>Elapsed Time per 500 Messages (Used in the Problem #1):</p> </li> <li>This is a specific measurement of efficiency. It calculates how long it takes for the producer to send 500 messages (or any other specified batch size).</li> <li>This can be helpful for benchmarking performance, especially when tuning or comparing different producer configurations.</li> <li>It's more about the speed or efficiency of the producer for a specific task.</li> </ol> <p>Tracking all three can give a good holistic view of producer performance. They complement each other by measuring different aspects. So in summary:</p> <ul> <li>Request rate - load on the producer</li> <li>Response rate - actual delivery throughput</li> <li>Elapsed time - latency for a batch</li> </ul>"},{"location":"lec1/demo_or_exercise/","title":"Demo #1 Setting Up Environment","text":""},{"location":"lec1/demo_or_exercise/#principles-in-our-demos","title":"Principles in our demos","text":"<ul> <li>You should focus more on learning the core concepts and applications than setting up environment. Environment setup and cluster management are usually not the responsibility of new hires, especially in large companies with dedicated ops teams. </li> <li>Don't spend too much time on complex cluster configuration and environment setup as a beginner. This can detract you from learning the fundamentals. Focus more on the higher level abstractions and use cases.</li> <li>Leverage existing documentation and sample code. Don\u2019t reinvent the wheel.</li> <li>Take an iterative approach to learning. Get the basics working first, and later dive into refinement, optimizations and customizations. </li> </ul> <p>We have three options of setting up Kafka clusters (Official Confluent Platform reference). The first one with Confluent Cloud + Confluent CLI (python) will be used in our classroom. The Confluent Cloud path provides a fully managed service so you don't have to provision your own Kafka infrastructure. The local install and Docker approaches (Method #2 and #3) allow you to run Confluent Platform locally for development and testing. </p> <ul> <li> <p>Method #1: Managed Confluent Cloud + CLI</p> <p>We have two options of python client: <code>confluent-kafka-python</code> and <code>kafka-python</code>. In Demo #2, we will cover both of them.</p> </li> <li> <p>Method #2: Local Docker install (not covered in the course)</p> </li> <li>Method #3: Local native install (not covered in the course)</li> </ul>"},{"location":"lec1/demo_or_exercise/#method-1-confluent-cloud-confluent-cli","title":"Method #1: Confluent Cloud + Confluent CLI","text":""},{"location":"lec1/demo_or_exercise/#overview","title":"Overview","text":"<p>Confluent Cloud is a managed Kafka service provided by Confluent, the company behind some of the popular Kafka toolsets. The significant advantage of using Confluent Cloud is that you don't have to worry about the infrastructure or configuration of a Kafka cluster. Everything is managed, and you can focus entirely on your application and client development. This method is used in the classroom.</p>"},{"location":"lec1/demo_or_exercise/#benefits","title":"Benefits","text":"<ol> <li>Simplicity: No need to set up or maintain the Kafka cluster.</li> <li>Scalability: Managed services often offer easy ways to scale your usage as needed.</li> <li>Reliability: Managed by experts, ensuring high availability, backups, and other best practices.</li> </ol>"},{"location":"lec1/demo_or_exercise/#method-2-local-cluster-self-managed","title":"Method #2: Local Cluster (self-managed)","text":""},{"location":"lec1/demo_or_exercise/#overview_1","title":"Overview","text":"<p>This is about setting up a Kafka cluster on your local machine, often for development or testing purposes. There are resources available in the official documentation to guide through this setup. Although Method #2 is not our default setup for demos, I highly recommend self-study so you may find it useful in your course project with other services to be developed on local machine. </p>"},{"location":"lec1/demo_or_exercise/#docker-with-confluent-platform","title":"Docker with Confluent Platform","text":"<p>Docker provides a way to run applications securely isolated in a container, packaged with all its dependencies and libraries. The links you provided are guides on setting up the Confluent Platform (which includes Kafka and other tools) using Docker containers.</p> <p>Note: The native Kafka installation is one option, though Docker provides isolation benefits.</p>"},{"location":"lec1/demo_or_exercise/#benefits-of-docker-based-local-setup","title":"Benefits of Docker-based Local Setup","text":"<ol> <li>Isolation: Ensures Kafka doesn't interfere with other software on your machine.</li> <li>Reproducibility: A consistent setup across different machines.</li> <li>Ease of Setup &amp; Cleanup: With a few commands, you can start and stop a Kafka cluster.</li> </ol>"},{"location":"lec1/demo_or_exercise/#instructions-method-1","title":"Instructions (Method #1)","text":"<p>Step 1.Confluent Cloud Account: Sign up and be aware of the $400 free credit. Link.</p> <p></p> <p>Step 2. Creating &amp; Running Confluent Cluster: Set up a Kafka cluster on Confluent Cloud.</p> <p>Create Cluster</p> <p></p> <p></p> <p></p> <p>Create Topic Let's create topic called \"demo1_free_text\". Produce 8 messages with same <code>key = 1</code> and different <code>sport</code> values. </p> <pre><code>value: {\n      \"sport\": \"400 metres\"\n}\n\nkey: 1\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Do NOT shutdown the cluster. Let's go to the next section to set up CLI. We will come back to this Confluent UI later in the end of the instructions.</p> <p>Step 3. Python 3: Make sure it's version <code>3.10.12</code> for consistency. If you already have python 3.10.x set up, then you can skip Step 3.</p> <p>3.1 Environment</p> <ul> <li>Use a Macbook with Ventura 13.5.x. Avoid upgrading to MacOS 14.0 Sonoma.</li> <li>Ensure <code>pip</code> (Python's package manager) is up-to-date.</li> <li> <p>Install <code>brew</code> if you haven't. It's a handy tool for Mac users. Please run <code>brew install brew</code> to update it to latest version.</p> </li> <li> <p>Consider using a Python virtual environment. It helps keep things tidy!</p> </li> <li>Before the demo, check your setups. For instance, verify the Python version with <code>python --version</code>.</li> <li>If you face setup issues, don't worry! We'll ensure everyone can follow along.</li> </ul> <p>3.2 Installing ASDF</p> <p>To manage python packages, ASDF can be installed on macOS using Homebrew:</p> <pre><code>brew install asdf\n</code></pre> <p>After installation, ensure ASDF is added to your shell.</p> <p>For zsh:</p> <pre><code>echo -e \"\\n. $(brew --prefix asdf)/libexec/asdf.sh\" &gt;&gt; ${ZDOTDIR:-~}/.zshrc\n</code></pre> <p>For bash:</p> <pre><code>echo -e \"\\n. $(brew --prefix asdf)/libexec/asdf.sh\" &gt;&gt; ~/.bash_profile\n</code></pre> <p>3.3 Using ASDF</p> <p>Here are some commonly used ASDF commands:</p> <p>List all available plugins:</p> <p>bash</p> <pre><code>asdf plugin list all | grep -i &lt;plugin&gt;\n</code></pre> <p>List installed plugins:</p> <p>bash</p> <pre><code>asdf plugin list\n</code></pre> <p>Add plugins:</p> <p>bash</p> <pre><code>asdf plugin add python asdf plugin add poetry\n</code></pre> <p>Install specific versions:</p> <p>bash</p> <pre><code>asdf install python 3.10.12\n# asdf uninstall python 3.10.8\nasdf list-all python\n</code></pre> <p>3.4 Error <pre><code>ModuleNotFoundError: No module named '_lzma'\nWARNING: The Python lzma extension was not compiled. Missing the lzma lib?\n</code></pre> -&gt; simply install it via <code>brew install xz</code></p> <p>Great, you have successfully installed <code>Python 3.10.12</code> with asdf. Here's what you might want to do next:</p> <p>3.5 Set the Python version</p> <p>Now that you have Python 3.10.12 installed, you might want to set it as your default version. You can do this with asdf's global command:</p> <p><pre><code>asdf global python 3.10.12\n</code></pre> This will make Python 3.10.12 your default Python version in all directories.</p> <p>Alternatively, if you want to use this Python version only in your current directory (for a specific project), you can use asdf's local command:</p> <pre><code>asdf local python 3.10.12\n</code></pre> <p>Step 4. Installing Confluent CLI</p> <p>Install the Confluent CLI and follow this official guide. </p> <p>You can update it to latest CLI version by running <code>brew upgrade confluentinc/tap/cli</code> (assuming you installed CLI via <code>brew</code>).</p> <p>My version of Confluent CLI: <code>3.38.0</code>.</p> <p>Step 5. Setting up Confluent CLI</p> <p>Again, I recommend using official guide for most up-to-date information. In our demo today, here are the key steps below.</p> <p>Environment: In Confluent Cloud, an environment is a logical workspace where you can organize your resources. You can think of it like a project folder in many other systems. One user can have multiple environments.</p> <p>Cluster: Within an environment, you can have one or more Kafka clusters. A cluster is essentially a running instance of Kafka, where you can produce and consume messages. </p> <ol> <li> <p>Logging in to Confluent Cloud: </p> <p>This logs you into Confluent Cloud via the CLI and saves your credentials locally, so you don't need to enter them repeatedly. <pre><code>confluent login --save\n</code></pre></p> <p>If you're using SSO (google login for example), then run this code below.</p> <pre><code>confluent login --no-browser\n</code></pre> </li> <li> <p>Listing Environments:</p> <p><pre><code>confluent environment list\n</code></pre> This lists all the environments you have access to. For a new account, this will typically be just one environment. Make note of the ID of the environment you want to use.</p> </li> <li> <p>Setting the Active Environment:     <pre><code>confluent environment use {ID}\n</code></pre>     With this command, you're telling the CLI: \"Hey, I want to work within this specific environment.\"</p> </li> <li> <p>Listing Clusters:     <pre><code>confluent kafka cluster list\n</code></pre>     This lists all the Kafka clusters within the currently active environment. Make note of the cluster ID you want to interact with.</p> </li> <li> <p>Setting the Active Cluster:     <pre><code>confluent kafka cluster use {ID}\n</code></pre>     This command sets the Kafka cluster you want to work with. All subsequent commands will interact with this cluster unless you change it.</p> </li> <li> <p>Creating an API Key:     <pre><code>confluent api-key create --resource {ID}\n</code></pre>     An API key and secret are needed to authenticate and interact with your Kafka cluster programmatically. The <code>--resource {ID}</code> is the cluster ID you've previously noted. This command will provide you an API key and secret. Keep them safe; you'll need them to authenticate your requests.</p> </li> <li> <p>Using the API Key:     <pre><code>confluent api-key use {API Key} --resource {ID}\n</code></pre>     This tells the CLI to use the provided API key for authentication when interacting with the specified resource (Kafka cluster).</p> </li> </ol> <p>Step 6. Managing topics with Confluent CLI</p> <p>In this step, we'll learn how to manage Kafka topics using the Confluent CLI. We'll produce some messages to a topic and consume them to see how Kafka handles message streams.</p> <p>6.1 Listing Existing Topics: Before diving into producing and consuming messages, let's see which topics are already available in your Kafka cluster:</p> <pre><code>confluent kafka topic list\n</code></pre> <p>6.2 Producing and Consuming Messages: To truly understand Kafka, it's beneficial to visualize the interaction between a producer (which sends messages) and a consumer (which reads messages). For this, we'll open two terminal windows: one for the producer and one for the consumer.</p> <ul> <li> <p>Terminal 1: Producer's Perspective:</p> <p>To send messages to a topic, use the following command:</p> <pre><code>confluent kafka topic produce {topic_name} --parse-key\n</code></pre> <p>Now, input the following messages:</p> <pre><code>1:\"200 metres\"\n1:\"100 metres\"\n1:\"archery\"\n</code></pre> </li> <li> <p>Terminal 2: Consumer's Perspective:</p> <p>To read messages from a topic, use the following command:</p> <pre><code>confluent kafka topic consume --from-beginning {topic_name}\n</code></pre> <p>Observe the messages appearing in real-time as they\u2019re produced in Terminal 1.</p> </li> </ul> <p>6.3 Verifying in Confluent Cloud UI:</p> <p>After producing and consuming messages using the CLI, it's a good practice to check the Confluent Cloud UI to see the data visually:</p> <ul> <li>Go to Confluent Cloud.</li> <li>Navigate to the topic overview page for the topic you chose.</li> <li>Click on the Messages tab.</li> <li>In the Jump to offset field, enter \"0\".</li> <li>Select different partitions to observe where the new messages have been stored.</li> </ul>"},{"location":"lec1/demo_or_exercise/#summary","title":"Summary","text":"<p>There are many concepts so far. I hope to give a clear picture of the relationship between the different components of Confluent Cloud and how they are accessed via the CLI. Let's break down the hierarchy and relationship of the Confluent Cloud UI and how it interacts with the CLI.</p>"},{"location":"lec1/demo_or_exercise/#confluent-cloud-hierarchy","title":"Confluent Cloud Hierarchy:","text":"<ol> <li> <p>Environment:</p> <ul> <li>An environment is the highest level of organization in Confluent Cloud. It's a logical grouping mechanism.</li> <li>You might have different environments for different purposes, such as Development, Testing, and Production.</li> <li>Each environment has a unique Environment ID.</li> </ul> </li> <li> <p>Cluster:</p> <ul> <li>Within an environment, you can have one or more Kafka clusters.</li> <li>Each cluster has its resources, such as brokers, topics, etc.</li> <li>Each cluster has a unique Cluster ID within the environment.</li> </ul> </li> </ol>"},{"location":"lec1/demo_or_exercise/#accessing-via-cli","title":"Accessing via CLI:","text":"<p>To access and manage resources within Confluent Cloud using the CLI, you often need to specify both the environment and cluster you want to interact with. This is especially true if you have multiple environments or clusters, as many organizations do.</p> <p>Typically, the process involves:</p> <ol> <li> <p>Authentication:</p> <ul> <li>You'll first authenticate your CLI with Confluent Cloud using your API key and secret.</li> </ul> </li> <li> <p>Setting the Environment:</p> <ul> <li>Use the Environment ID to specify which environment you want to work within.</li> </ul> </li> <li> <p>Accessing/Managing a Cluster:</p> <ul> <li>Once inside an environment, use the Cluster ID to specify which cluster you want to interact with.</li> </ul> </li> </ol>"},{"location":"lec1/demo_or_exercise/#example","title":"Example:","text":"<p>Imagine you have two environments: <code>Development</code> and <code>Production</code>. In the <code>Development</code> environment, you have a Kafka cluster named <code>DevCluster</code>.</p> <p>To manage a resource in <code>DevCluster</code> using the CLI:</p> <ol> <li>Authenticate your CLI with Confluent Cloud.</li> <li>Set your working environment to <code>Development</code> using its Environment ID.</li> <li>Access or manage <code>DevCluster</code> using its Cluster ID.</li> </ol>"},{"location":"lec1/demo_or_exercise/#conceptual-visualization","title":"Conceptual Visualization:","text":"<pre><code>Confluent Cloud\n|\n|-- Environment 1 (Development)\n|   |\n|   |-- Cluster A (DevCluster)\n|   |   |\n|   |   |-- Topic 1\n|   |   |-- Topic 2\n|   |\n|   |-- Cluster B\n|\n|-- Environment 2 (Production)\n    |\n    |-- Cluster C\n    |\n    |-- Cluster D\n</code></pre> <p>When working with the Confluent CLI, you'd authenticate, select Environment 1 (using its ID), then select Cluster A (using its ID) to interact with topics or other resources within that cluster.</p>"},{"location":"lec2/","title":"Lecture 2. Apache Kafka Part 1. Basics and Producers","text":"<p>(Working In Progress)</p> <p>Author: Jeremy Gu</p>"},{"location":"lec2/#basics-of-apache-kafka","title":"Basics of Apache Kafka","text":"<p>Before delving into the intricate details of Apache Kafka, it's essential to lay down a broad overview. Starting with a holistic understanding of key concepts will keep us anchored and ensure that we don't get lost. Given that the version of Kafka we're using is continually evolving, we'll heavily reference official definitions and explanations to avoid redundancy. </p> <p>For this course, we'll be working with the latest Kafka version <code>3.6</code>. We encourage you to familiarize yourself with our recommended readings ahead of the class.</p> <p>Recommended Reading:</p> <ul> <li>Kafka Official Documentation (Version: Kafka 3.6) at https://kafka.apache.org/documentation/. Credits: Any text in italics throughout this document is sourced directly from the Kafka Official Documentation.</li> </ul>"},{"location":"lec2/#introduction-to-apache-kafka","title":"Introduction to Apache Kafka","text":"<ul> <li>Definition: Apache Kafka is an event streaming platform engineered for real-time collection, processing, storage, and data integration.</li> <li> <p>Officiial Explanation: From https://kafka.apache.org/, Kafka combines three key capabilities so you can implement your use cases for event streaming end-to-end with a single battle-tested solution:</p> <ul> <li>To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.</li> <li>To store streams of events durably and reliably for as long as you want.</li> <li>To process streams of events as they occur or retrospectively.</li> </ul> </li> </ul>"},{"location":"lec2/#kafka-as-a-stream-processing-powerhouse","title":"Kafka as a Stream Processing Powerhouse","text":"<ul> <li> <p>Popularity: Kafka reigns supreme as a leading streaming data platform in the industry, trusted and adopted widely.</p> </li> <li> <p>Beyond Messaging: While it provides an intuitive message queue interface, Kafka's essence lies in its append-only log-structured storage, transcending traditional messaging paradigms.</p> </li> <li> <p>Events as Kafka\u2019s Core: </p> <ul> <li>Kafka is fundamentally an event log that chronicles occurrences.</li> <li>These events portray things that have happened, not directives for actions to be undertaken.</li> <li>An event has a key, value, timestamp, and optional metadata headers. Here's an example event from https://kafka.apache.org/:</li> </ul> </li> </ul> <pre><code>Event key: \"Alice\"\nEvent value: \"Made a payment of $200 to Bob\"\nEvent timestamp: \"Jun. 25, 2020 at 2:06 p.m.\"\n</code></pre> <ul> <li> <p>Distributed DNA: Kafka is innately distributed, fault-tolerant, and scales seamlessly from a single node to a massive thousand-node ensemble. From https://kafka.apache.org/, we have official definitions below.</p> <ul> <li>Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol.</li> <li>Servers: Kafka is run as a cluster of one or more servers that can span multiple data centers or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters.</li> <li>Clients allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures.</li> </ul> </li> <li> <p>Data Guarantees: A hallmark of Kafka is its assurance of data order preservation, crucial for deterministic real-time processing tasks.</p> </li> <li> <p>Ecosystem Synergy: Kafka seamlessly integrates with renowned streaming tools like Apache Spark, Flink, and Samza, broadening its capabilities.</p> </li> </ul>"},{"location":"lec2/#main-concepts-and-terminology","title":"Main Concepts and Terminology","text":"<ul> <li> <p>What is an Event? An event records the fact that \"something happened\".</p> </li> <li> <p>Kafka's Treatment of Events: </p> <ul> <li>Core Abstraction: At its core, Kafka offers a distributed commit log, persistently storing events and ensuring their replication.</li> <li>Events are organized and durably stored in topics.</li> </ul> </li> <li> <p>Topics: A topic is similar to a folder in a filesystem, and the events are the files in that folder.</p> <ul> <li>Topics are partitioned, meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers (storage layer).</li> </ul> </li> <li> <p>Producers: Client applications that publish (write) events to Kafka.</p> </li> <li> <p>Consumers: Client applications that subscribe to (read and process) these events.</p> <ul> <li>In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for.</li> </ul> </li> <li> <p>Source: Refers to the origin or starting point where data is produced or ingested. In Kafka's ecosystem, a source would typically be a producer or an external system/service that sends data into Kafka. For instance, a database, an application, or any system that produces events could act as a source.</p> </li> <li> <p>Sink: Refers to the destination or endpoint where data is sent to or consumed. In Kafka, a sink would be the consumer or another data store where Kafka data is written. In the Kafka ecosystem, particularly with Kafka Connect, sinks are often connectors that stream data from Kafka to another system, such as a database, search index, or another type of data store.</p> </li> </ul>"},{"location":"lec2/#a-peek-into-kafkas-legacy","title":"A Peek into Kafka's Legacy","text":"<ul> <li> <p>Birth at LinkedIn: Kafka was conceived at LinkedIn to cater to their internal streaming requisites. Now, as an Apache Foundation jewel, its adoption and growth are unparalleled.</p> </li> <li> <p>Industry Usage Titans like Uber, Apple, and Airbnb leverage Kafka for their mission-critical operations, a testament to its robustness.</p> </li> <li> <p>Confluent's Contribution:  After leaving LinkedIn, Kafka's creators founded Confluent. While Kafka remains under the Apache Software Foundation, Confluent significantly influences its development and provides extended tools and support.</p> </li> <li> <p>The Literary Connection: Kafka owes its name to Czech author Franz Kafka, reflecting its efficient nature of recording events.</p> </li> </ul> <p>Exercise</p> <p>Which of the following messages is more indicative of an event-driven paradigm suitable for a system like Kafka?</p> <p>A. <code>{\"directive\": \"increase_volume\", \"level\": 7}</code></p> <p>B. <code>{\"event\": \"volume_changed\", \"previous_level\": 6, \"new_level\": 7}</code></p> <p>C. <code>{\"message\": \"Hello World\", \"recipient\": \"user123\"}</code></p> Note <p>Answer: B. <code>{\"event\": \"volume_changed\", \"previous_level\": 6, \"new_level\": 7}</code> </p> <p>Explanation: Option B represents a change in state, capturing an \"event\" of the volume level changing, which aligns well with the event-driven paradigm. Option A seems more like a command or directive, while Option C is a generic message with no clear event or state change described.</p>"},{"location":"lec2/#examples-of-managing-topics","title":"Examples of Managing Topics","text":"<p>(Please read Lecture 2 PPT)</p>"},{"location":"lec2/#producers","title":"Producers","text":""},{"location":"lec2/#examples-of-writing-producers","title":"Examples of Writing Producers","text":"<p>(Please read Lecture 2 PPT)</p>"},{"location":"lec2/#asynchronous-producer-and-synchronous-producer","title":"Asynchronous Producer and Synchronous Producer","text":"<p>Below is a table that summarizes the use of <code>flush()</code>, <code>poll()</code>, and other methods for both asynchronous and synchronous Kafka producers:</p> Method/Action Asynchronous Producer Synchronous Producer flush() Used (After all messages) - Called once after the loop has produced all messages to ensure that all of them have been delivered. It waits up to the given timeout for the delivery report callbacks to be triggered. Used (After each message) - Called within the loop, after each <code>produce()</code>, to ensure that the current message is delivered and acknowledged before sending the next one. poll() Used - Can be used within the loop or after it, primarily to trigger the delivery report callback and get feedback about the delivery status of messages. If used after the loop (as in the provided example), it will process delivery reports for all previously sent messages. Optional - Not typically required in a purely synchronous producer since <code>flush()</code> waits for message acknowledgment. However, if you wanted feedback about delivery status after each message, you could use it. callback Used - Callback functions (like <code>delivery_report</code>) are used to handle delivery reports asynchronously. They provide feedback about the delivery status of each message. Optional - While callbacks can be used in synchronous producers, they're not as essential since you're relying on <code>flush()</code> to wait for acknowledgment. However, callbacks can provide more detailed information about the delivery. <p>Remember that in real-world scenarios, you might find hybrid approaches based on the exact requirements of your application. The above distinctions are generalized to help you understand the conceptual differences between the two types of producers.</p>"},{"location":"lec2/#producer-congiuration","title":"Producer Congiuration","text":"Configuration Default Value Recommended Value Description <code>client.id</code> (none) (unique value) Identifier for the client in the Kafka cluster. Useful for tracking and debugging. <code>retries</code> 2147483647 (depends on use case) The number of retry attempts. A high default ensures durability but can be adjusted based on use case. <code>enable.idempotence</code> false true Ensures that messages are delivered exactly once by allowing retries without duplication. <code>acks</code> all or -1 all or -1 The number of acknowledgments the producer requires the broker to receive before considering a message sent. <code>compression.type</code> none (depends on use case) Type of compression to use (<code>none</code>, <code>gzip</code>, <code>snappy</code>, <code>lz4</code>, <code>zstd</code>). Chosen based on data and performance needs. <p>Source of Truth is from the confluent github repo: https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md.</p> <p>Based on Apache Kafka Message Compression published on 9/18/2023, here are the relevant passages that pertain to the topic-level compression and its precedence:</p> <ol> <li> <p>Topic-Level Compression Precedence:</p> <p>\"Typically, it works in the following manner when the producer is responsible for compression, and the topic\u2019s compression.type is producer.\"</p> </li> <li> <p>Mismatch in Compression Settings:</p> <p>\"Brokers always perform some amount of batch decompression in order to validate data.\" \"The following scenarios always require full payload decompression:      The producer compresses batches (i.e., compression.type isn\u2019t none), and the topic-level compression.type specifies a different codec or is uncompressed.\"</p> </li> </ol>"},{"location":"lec2/#choice-of-compression-types","title":"Choice of Compression Types","text":"<p>When decompression speed is critical, the following compression algorithms are commonly recommended in Kafka and other systems:</p> <ol> <li> <p>Snappy: Developed by Google, Snappy prioritizes speed over compression ratio. It doesn't compress as tightly as some other algorithms, but it decompresses very quickly.</p> </li> <li> <p>LZ4: This is another compression algorithm that's designed for fast decompression speeds. In many benchmarks, LZ4 has been shown to be faster than Snappy, both in terms of compression and decompression, but with a slightly better compression ratio than Snappy.</p> </li> </ol> <p>Between the two, LZ4 is often recommended in Kafka for performance reasons, but the best choice can vary depending on the specific nature of the data and the requirements of the use case. Always consider benchmarking with your actual data to determine which one meets your needs best.</p> <p>When network overhead is critical, meaning you want to minimize the amount of data sent over the network, you should opt for compression algorithms that provide the highest compression ratios, even if they are slower in terms of compression and decompression speed. Here are the common choices:</p> <ol> <li> <p>gzip: This is a widely-used compression algorithm that typically provides a good balance between compression ratio and speed. While it's not the fastest in terms of compression or decompression, it usually achieves smaller compressed sizes compared to faster algorithms like Snappy or LZ4.</p> </li> <li> <p>zstd (Zstandard): Developed by Facebook, zstd offers compression ratios comparable to gzip but at much faster speeds, especially in its higher compression levels. This makes zstd a great choice for scenarios where both network bandwidth and speed are concerns.</p> </li> </ol> <p>For maximum reduction in network overhead, you might lean toward gzip or zstd. zstd is often recommended over gzip due to its better speed while maintaining a similar, if not better, compression ratio. As always, it's beneficial to benchmark different algorithms with your actual data to determine the best fit for your specific needs.</p>"},{"location":"lec2/#clientid","title":"client.id","text":"<p>The <code>client.id</code> is a configuration setting for Kafka clients (both producers and consumers). It's an identifier for the client in the Kafka cluster. This ID is used in logs, metrics, and for debugging purposes to identify the source of requests to the broker.</p> <p>Having a unique <code>client.id</code> for each client allows administrators and developers to track requests, observe behavior, and troubleshoot issues more effectively because they can see which client is making which requests.</p> <p>Simple Example: Let's say you have a system with multiple Kafka producers, one that sends user activity data and another that sends system metrics. You can assign unique client IDs to each of them:</p> <ul> <li>For the producer that sends user activity data: <code>client.id=user-activity-producer</code></li> <li>For the producer that sends system metrics: <code>client.id=system-metrics-producer</code></li> </ul> <p>Now, when you look at the logs or metrics from the Kafka broker, you can quickly identify which client (producer) is the source of a particular request or potential issue just by checking the <code>client.id</code>. This is especially helpful in environments with many clients interacting with a Kafka cluster.</p>"},{"location":"lec2/#idempotence","title":"Idempotence","text":"<p>When retries are enabled, enabling idempotence ensures messages remain in their intended order. For example, if messages A, B, and C are sent, but B fails initially while C succeeds, retries without idempotence might lead to an A, C, B order. With idempotence, the order A, B, C is preserved.</p>"},{"location":"lec2/#producer-batch","title":"Producer - Batch","text":"<p>A \"batch\" itself is a group of messages. </p> <ol> <li> <p>Batching Messages: Kafka client libraries (producers) don't necessarily send every message individually. Instead, they batch multiple messages together into a single batch for efficiency reasons. This helps in reducing the overhead of network and I/O operations for each message, thus increasing throughput.</p> </li> <li> <p>Batches: The term \"batch\" in Kafka's context refers to a collection of messages that are sent together in one go to a Kafka broker. Batching is a technique to improve application and network performance.</p> </li> <li> <p>Customizability: The way messages are batched and sent \u2014 such as the size of the batch (<code>batch.size</code>), the time the producer waits before sending the batch (<code>linger.ms</code>), and the maximum size of a request (<code>max.request.size</code>) \u2014 are all customizable using producer configurations.</p> </li> </ol> <p>The belief that the Kafka producer sends messages one by one is incorrect. Here's why:</p> <ul> <li> <p>Batching: The Kafka producer, including the Python client, doesn't send messages individually. Instead, it groups messages and sends them in batches for efficiency.</p> </li> <li> <p>Configurable Settings: The conditions for batching, like message count or elapsed time, are adjustable, allowing for performance tuning.</p> </li> <li> <p>Compression: Kafka can compress these batches, reducing bandwidth usage more effectively than compressing individual messages.</p> </li> <li> <p>Optimized I/O: Batching reduces the number of I/O operations, improving network and disk efficiency.</p> </li> </ul> <p>In the diagram below, each \"Message\" represents an individual message produced and ready to be sent. \"[ Batch 1 ]\" and \"[ Batch 2 ]\" represent groups of messages that the producer batches together based on certain conditions (like reaching a specific message count, size, or time limit). \"Send Batch\" represents the action of sending a batch of messages from the producer to the broker.</p> <pre><code>Producer                                        Broker\n   |                                              |\n   |---- Message 1 ----                           |\n   |---- Message 2 ----                           |\n   |---- Message 3 ----                           |\n   |---- Message 4 ----                           |\n   |                                              |\n   |                                              |\n   | Factors for batching:                        |\n   |   - Count (e.g., 4 messages)                 |\n   |   - Time (e.g., 500ms elapsed)               |\n   |   - Size (e.g., nearing 1 GB)                |\n   |                                              |\n   |----[ Batch 1 ]----&gt;|------ Send Batch ------&gt;|\n   |                    |                         |\n   |---- Message 5 ----                           |\n   |---- Message 6 ----                           |\n   |                                              |\n   |----[ Batch 2 ]----&gt;|------ Send Batch ------&gt;|\n   |                                              |\n</code></pre>"},{"location":"lec2/#data-types-and-serialization-in-kafka","title":"Data Types and Serialization in Kafka","text":"<p>Definition: Serialization in Kafka is the transformation of data into a format suitable for efficient storage and transmission, typically binary.</p> <p>Serialization is a common process in many systems. Its emphasis in Kafka is due to the need for efficient data transfer and storage. Formats like <code>Avro</code> not only offer compact serialization but also support evolving data structures and ensure type safety.</p> <p>Serialization Formats:</p> <ol> <li> <p>JSON: </p> <ul> <li>Text-based and human-readable.</li> <li>Maps to native data structures in many languages.</li> <li>Less efficient than binary formats like Avro.</li> </ul> </li> <li> <p>Avro: </p> <ul> <li>Binary serialization format.</li> <li>Compact and efficient.</li> <li>Uses schemas to ensure type safety and support data structure evolution.</li> </ul> </li> </ol> <p>The Process:</p> <ul> <li>Serialization: Transforming application data into binary format for efficient storage and transmission. Done by producers before dispatching to Kafka.</li> <li>Deserialization: Converting binary data back to its original format for consumers. Done by consumers after retrieving from Kafka.</li> </ul> <p>Consistency:</p> <p>It's vital to maintain a consistent serialization technique within a single Kafka topic for data integrity.</p> <p>Kafka's Role:</p> <p>Kafka doesn\u2019t directly manage serialization or deserialization. Instead, the Kafka client library handles this, streamlining the process for both producing and consuming messages.</p> <p>Avro Schemas:</p> <ol> <li> <p>Schema Evolution: </p> <ul> <li>Allows changes to the data schema without breaking compatibility. </li> <li>Enables old and new versions of data to coexist without errors, promoting backward and forward compatibility.</li> </ul> </li> <li> <p>Type Safety: </p> <ul> <li>Ensures consistency in data types with the specified schema.</li> <li>Avro verifies data against the schema during serialization and deserialization, catching type mismatches and reducing the risk of data corruption.</li> </ul> </li> </ol>"},{"location":"lec2/#performance-considerations-1-throughput-and-2-overhead","title":"Performance Considerations - 1) Throughput and 2) Overhead","text":"<p>Batching is largely for performance reasons. In high-throughput applications, sending each message individually would result in significant network overhead.</p> <p>Here is the trade-off: By waiting to accumulate messages, you can send fewer, larger batches, which can improve throughput. However, this could introduce a slight delay in individual message delivery times.</p> <p>Official Confluent Documentation: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html</p> <p>Batch Processing for Efficiency: https://docs.confluent.io/kafka/design/efficient-design.html</p> <p>Tutorial: How to optimize your Kafka producer for throughput: https://developer.confluent.io/tutorials/optimize-producer-throughput/confluent.html</p>"},{"location":"lec2/2.1/","title":"Lec 2: Additional Topics","text":"<p>Author: Jeremy Gu</p> <p>Note</p> <p>In each lecture, we will have a section called \"Additional Topics\" covering concepts that, due to our fast-paced curriculum, we may not have time to cover in class. However, I believe understanding these is important for fully mastering data streaming. I may also include recommended readings and coding exercises to reinforce your learning. Note that additional topics will not be on the midterm exam. In your future work, I encourage revisiting these to strengthen your data streaming knowledge. </p>"},{"location":"lec2/2.1/#terminology-review","title":"Terminology Review","text":"Term Definition Example Synchronous Operations prevent subsequent operations from starting until they are completed. When on a phone call, you can't start a second call. Sequential Operations occur in a specific order, one after the other. For breakfast, first brew the coffee, then fry the eggs. Parallel Multiple operations or tasks execute independently at the exact same time. A multi-core CPU can execute multiple tasks simultaneously. Concurrent Multiple operations or tasks start within the same timeframe, but might not start at the exact same moment; they may interleave or execute simultaneously. While cooking, you can concurrently do laundry. Asynchronous Once an operation starts, there's no need to wait for its completion; you can continue other operations. Upon completion of the original operation, a notification is typically given, often through mechanisms like callbacks. Email is sent in the background; you're notified once it's sent."},{"location":"lec2/2.1/#fire-and-forget","title":"Fire and Forget","text":"<p>\"Fire and Forget\" is a term commonly used in distributed systems and messaging architectures to describe a pattern where the producer sends a message and doesn't wait or care about the acknowledgment of its receipt or its processing status. In the context of Kafka, this would mean sending a message to the broker without waiting for any acknowledgment of its delivery.</p> <p>In the Kafka producer scenarios we discussed:</p> <ol> <li> <p>Asynchronous Producer: This can be considered a form of \"Fire and Forget\" if you remove the callbacks and the <code>flush()</code> method. You just \"fire\" messages to the broker in a loop and don't bother checking if they were received or if there were any errors.</p> </li> <li> <p>Synchronous Producer: This is the opposite of \"Fire and Forget.\" Every message is sent, and the producer waits for its acknowledgment before proceeding.</p> </li> </ol> <p>For a true \"Fire and Forget\" scenario with a Kafka producer in Python, the code might look something like:</p> <pre><code>from confluent_kafka import Producer\nconfig = {\n# ... your Kafka configuration ...\n}\nproducer = Producer(config)\ndef produce_messages_fire_and_forget(producer, topic_name, num_messages=10):\n\"\"\"Fire and forget message production.\"\"\"\nfor _ in range(num_messages):\nmessage_value = \"Some message value\"\nproducer.produce(topic_name, value=message_value)\n# Use the function\nproduce_messages_fire_and_forget(producer, \"your_topic_name\")\n</code></pre> <p>In the above, messages are sent without any checks or waits for acknowledgment. This method can achieve higher throughput because it doesn't involve the overhead of checking message delivery status. However, it comes at the potential cost of not knowing about failed deliveries, so it's best used in scenarios where occasional message loss is acceptable.</p>"},{"location":"lec2/2.1/#example-ubers-big-data-stack","title":"Example: Uber's Big Data Stack","text":"<p>We have walked through basics of Kafka ecosystem. Next, I'd love to introduce an example of how to read industry engineering blogs that cover big data streaming systems. In the diagram above (Uber Eng Blog source<sup>1</sup>), Uber leverages a diverse set of tools and technologies to manage its vast amount of data. This stack allows Uber to efficiently process, analyze, and act upon data generated from various sources like Rider App, Driver App, and other services. The diagram  illustrates the \"Big Data Stack\" used by Uber. The arrows between these components signify the flow of data and the relationships between various systems in the stack. Overall, the diagram provides a comprehensive view of how Uber might process, store, and analyze its big data. Let's break it down:</p> <ul> <li> <p>Sources: At the leftmost part of the diagram, there are sources of data such as the \"Rider App\", \"Driver App\", \"API/Services\", and others. These represent the various applications and services from where data originates.</p> <ul> <li>Rider App: Primarily for passengers, it collects data such as user locations, destinations, payment methods, and order histories.</li> <li>Driver App: Tailored for drivers, it captures data points like driver locations, driving routes, order information, and income details.</li> </ul> </li> <li> <p>Databases: Beneath the source, you'll find different databases like \"Cassandra\", \"Schemaless\", and \"MySQL\". These are databases where Uber's raw data might be stored.</p> </li> <li> <p>Ingestion: The center of the diagram features an \"Ingestion\" process. This process is responsible for taking in data from the sources and feeding it into the big data systems. The systems involved in the ingestion process are:</p> <ul> <li>Kafka: A real-time data streaming platform.</li> <li>Hadoop: A distributed storage and processing framework.</li> <li>Amazon S3: A cloud storage solution by Amazon.</li> </ul> </li> <li> <p>Data Processing and Analytics:</p> <ul> <li>Flink: A stream-processing framework.</li> <li>Presto: A distributed SQL query engine.</li> <li>Pinot: A real-time distributed OLAP datastore.</li> <li>ELK: Refers to the Elastic Stack, consisting of Elasticsearch, Logstash, and Kibana, used for searching, analyzing, and visualizing data in real-time.</li> <li>Spark: A distributed data processing framework. Jobs, written using Spark's API, are divided into tasks and executed across cluster nodes. Its in-memory computing capability ensures rapid data processing.</li> <li>Hive: Used for analytics reporting, it helps extract statistical data about orders, revenues, and user behaviors.</li> </ul> </li> <li> <p>End Applications: On the rightmost side, the diagram lists the potential applications or services that utilize the processed data. These include \"Micro-Services\", \"Mobile App\", \"Streaming Analytics, Processing\", \"Machine Learning\", \"ETL, Applications Data Science\", \"Ad-hoc Exploration\", \"Analytics Reporting\", and \"Debugging\".</p> </li> </ul> <p></p> <p>The subsequent diagram emphasizes Kafka's central role within Uber's tech stack. It facilitates numerous workflows, such as channeling event data from the Rider and Driver apps as a pub-sub message system, streaming analytics via Apache Flink, streaming database logs to downstream recipients, and ingesting assorted data streams into Uber's Apache Hadoop data lake. Considerable attention has been given to enhancing Kafka's efficiency, reliability, and user experience.</p> <p>Producers:</p> <ul> <li> <p>Rider APP: The application used by riders to book rides. It generates events like booking a ride, ride cancellations, rating a driver, etc. These events are produced and sent to Kafka clusters.</p> </li> <li> <p>Driver APP: The application used by drivers. It produces events like accepting a booking, starting a ride, ending a ride, and other driver-related activities. These events are subsequently channeled to Kafka clusters.</p> </li> <li> <p>API/SERVICES: Backend services or APIs produce events or logs. This could be any interaction with the system, error logs, or any backend process that needs to be logged or analyzed.</p> </li> </ul> <p>Kafka Clusters: The above apps and services generate event data that is produced/sent to the Kafka clusters.  The Kafka clusters sit at the core, tasked with receiving data from diverse producers and channeling this data to numerous consumers.</p> <p>Consumers: </p> <ul> <li> <p>Flink: Apache Flink consumes events from Kafka for real-time stream processing. For instance, it might analyze real-time ride data for surge pricing decisions or traffic patterns.</p> </li> <li> <p>Hadoop: A big data storage and processing framework, it consumes data for long-term storage and batch processing. The data consumed here might be used for trend analysis or financial reporting. Note: Flink for real-time stream processing; Hadoop for batch processing and storage.</p> </li> <li> <p>Apache Hive: Operating over Hadoop, Hive consumes data to run SQL-like queries for data analysis.</p> </li> <li> <p>Real-time Analytics, Alerts, Dashboards: Systems that provide real-time insights, alerts, and visualization dashboards will consume the relevant data streams from Kafka.</p> </li> <li> <p>Debugging: Any system or tool used to monitor and debug issues will consume error logs or event data from Kafka.</p> </li> <li> <p>Applications Data Science: Data science applications that might be building models or running experiments will consume data from Kafka for their analytics and training needs.</p> </li> <li> <p>Ad-hoc Exploration: Any tool or system that needs to run exploratory analysis will consume the required datasets from Kafka.</p> </li> <li> <p>Analytics Reporting: Systems designed to report and visualize data will be consuming the processed or raw data from Kafka.</p> </li> <li> <p>ELK (Elasticsearch, Logstash, Kibana): The ELK stack consumes log data for analytics and visualization. It helps in monitoring and troubleshooting.</p> </li> </ul> <ol> <li> <p>Real-Time Exactly-Once Ad Event Processing with Apache Flink, Kafka, and Pinot (9/2021). https://www.uber.com/blog/presto-on-apache-kafka-at-uber-scale/. Note that Marmaray is an open source data ingestion and dispersal framework developed by Uber's Hadoop Platform team to move data into and out of their Hadoop data lake.\u00a0\u21a9</p> </li> </ol>"},{"location":"lec2/demo_2a/","title":"Demo #2A: Using Python to Consume Events","text":"<p>Author: Jeremy Gu</p> <p>Demo Objective:</p> <ul> <li>Learn to use python client  <code>confluent-kafka-python</code></li> <li>We have introduced Producer and walked through Producer's python implementation. Now let's get a sense of how a Consumer works. </li> </ul>"},{"location":"lec2/demo_2a/#overview","title":"Overview","text":"<p>In Demo #1, we have explored Confluent Cloud and Confluent CLI. You might be thinking, \"Where does Python fit into all of this?\" By the end of this demo, the student should be able to consume messages in a topic with Python client. For this demo, you'll be utilizing <code>confluent-kafka-python</code>, the Apache Kafka Python Client developed by Confluent. This client allows us to interface with Kafka using Python, enabling a more programmatic way to produce and consume events.</p> <p>Recall that we have two options of python client: <code>confluent-kafka-python</code> and <code>kafka-python</code>.  </p> <ul> <li>confluent-kafka-python: presented in Demo #2A.</li> <li>kafka-python: presented in Demo #2B. </li> </ul> <p>Here's the comparison:</p> Criteria kafka-python (Demo #2B) confluent-kafka-python (Demo #2A) Origin Pure Python implementation. Based on <code>librdkafka</code>, a high-performance C library for Kafka. Performance Generally lower performance. Typically offers better performance due to <code>librdkafka</code>. Feature Support Supports core features but may lack some newer ones. Usually quicker to support newer Kafka features. Maintenance Maintained by Apache Kafka open-source community. Maintained by Confluent, a major contributor to Kafka. Dependencies No external dependencies. Requires <code>librdkafka</code>. <p>In summary, for high performance and newer Kafka features, go for <code>confluent-kafka-python</code>. For a pure Python solution, choose <code>kafka-python</code>.</p>"},{"location":"lec2/demo_2a/#instructions","title":"Instructions","text":""},{"location":"lec2/demo_2a/#step-1-setting-up-the-environment","title":"Step 1. Setting up the environment","text":"<p>Begin by following the getting-started installation guide provided by Kafka.</p> <p>1.1. Create a Project Folder <pre><code>mkdir msds-demo2a\ncd mkds-demo2a\n</code></pre></p> <p>1.2. Set Up a Virtual Environment <pre><code>python -m venv .venv\n</code></pre></p> <p>1.3. Activate the Virtual Environment <pre><code>source .venv/bin/activate\n</code></pre></p> <p>1.4. Install Kafka Python Client Packages <pre><code>pip install pip --upgrade\n\npip install confluent-kafka\n</code></pre></p> <p>The version we are using is latest <code>2.2.0</code>. You might see something like <code>Successfully installed confluent-kafka-2.2.0</code></p>"},{"location":"lec2/demo_2a/#step-2-cli-setup","title":"Step 2. CLI Setup","text":"<p>Run the following command to view the details of your Kafka cluster:</p> <pre><code>confluent kafka cluster describe\n</code></pre> <p>Details show up like below.</p> <p><pre><code>+----------------------+--------------------------------------------------------+\n| Current              | true                                                   |\n| ID                   | lkc-v1x365                                             |\n| Name                 | cluster_0                                              |\n| Type                 | BASIC                                                  |\n| Ingress Limit (MB/s) | 250                                                    |\n| Egress Limit (MB/s)  | 750                                                    |\n| Storage              | 5 TB                                                   |\n| Provider             | gcp                                                    |\n| Region               | us-west4                                               |\n| Availability         | single-zone                                            |\n| Status               | UP                                                     |\n| Endpoint             | SASL_SSL://pkc-lzvrd.us-west4.gcp.confluent.cloud:9092 |\n| REST Endpoint        | https://pkc-lzvrd.us-west4.gcp.confluent.cloud:443     |\n| Topic Count          | 1                                                      |\n+----------------------+--------------------------------------------------------+\n</code></pre> Note the value in the <code>Endpoint</code> field, e.g., <code>pkc-lzvrd.us-west4.gcp.confluent.cloud:9092</code>.</p> <p>Creating an API Key:</p> <p>To create an API key, execute: <pre><code>confluent api-key create --resource {ID}\n</code></pre></p> <p><pre><code>| API Key    | {.....} |\n| API Secret | {.....} |\n</code></pre> An API key and secret are needed to authenticate and interact with your Kafka cluster programmatically. The --resource {ID} is the cluster ID you've previously noted. This command will provide you an <code>API key</code> and <code>secret</code>. Keep them safe; you'll need them to authenticate your requests.</p> <p>Using the API Key:</p> <pre><code>confluent api-key use {API Key} --resource {ID}\n</code></pre> <p>This tells the CLI to use the provided API key for authentication when interacting with the specified resource (Kafka cluster).</p>"},{"location":"lec2/demo_2a/#step-3-configuration","title":"Step 3. Configuration","text":"<p>3.1 Create a file named <code>config.ini</code></p> <p>The config.ini file serves as a configuration file for your Kafka consumer. Ensure the config.ini file safe and not sharing it, as it contains sensitive credentials.</p> <p>To create the file in your current project directory, execute:</p> <pre><code>touch config.ini\n</code></pre> <p>Populate config.ini with the following content:</p> <pre><code>[default]\n# Specify the Kafka broker addresses. This is typically the address of your Confluent Cloud cluster. &lt; Endpoint &gt;\nbootstrap.servers=SASL_SSL://pkc-lzvrd.us-west4.gcp.confluent.cloud:9092\n\n# Determine the security protocol. SASL_SSL indicates authentication using SASL and SSL for encryption.\nsecurity.protocol=SASL_SSL\n\n# Specify the SASL mechanism used for authentication. PLAIN indicates plaintext authentication.\nsasl.mechanisms=PLAIN\n\n# The username for SASL/PLAIN authentication. Generally, this is a Confluent Cloud API key. &lt; API Key &gt;\nsasl.username={.....}\n\n# The password for SASL/PLAIN authentication. Generally, this is a Confluent Cloud API secret. &lt; API Secret &gt;\nsasl.password={.....}\n\n[consumer]\n# It's essential to set a unique group ID for your application. Kafka uses this to manage the offsets of messages the consumer group has processed.\ngroup.id=my_consumer_group\n\n# This setting determines how to behave when the consumer starts and no committed offset is found.\n# 'earliest' will make the consumer start from the beginning of the topic.\n# Other possible values: 'latest' (start at the end), 'none' (throw an exception if no offset found).\nauto.offset.reset=earliest\n</code></pre>"},{"location":"lec2/demo_2a/#step-4-write-python-consumer","title":"Step 4. Write Python Consumer","text":"<p>Create a new Python script named consumer.py:</p> <pre><code>touch consumer.py\n</code></pre> <p>You can copy and paste from the python script below.</p> <ul> <li>consumer.py</li> <li>config.ini</li> </ul> <p>To Execute the Consumer Script:</p> <p>First, grant execute permissions to the script:</p> <pre><code>chmod u+x consumer.py\n</code></pre> <p>Then, run the script specifying the configuration file and topic:</p> <pre><code>./consumer.py config.ini demo1_free_text\n</code></pre> <p>Observations:</p> <p>For best results, use two terminals:</p> <ul> <li>Terminal 1: Produce new events/messages.</li> <li>Terminal 2: Monitor messages being consumed by the Python script.</li> </ul> <p>Once you see the messages being consumed, you can stop the consumer script in Terminal 2 using ctrl+C.</p> <p></p>"},{"location":"lec2/demo_2a/#note-offset-in-consumer-setting","title":"Note: Offset in Consumer Setting","text":"<p>We will cover more in Lecture 3. Don't worry about this concept for now. </p> <p>When you run the consumer (<code>./consumer.py config.ini demo1_free_text</code>) for the first time and consume messages, Kafka keeps track of the last message you've read using a mechanism called \"offsets\". This allows Kafka to remember where each consumer left off and ensures that each message is processed once per consumer group.</p> <p>When you run the consumer (<code>./consumer.py config.ini demo1_free_text</code>) again without producing new messages to the topic, the consumer starts from where it left off. Since there are no new messages after the last consumed message, you only see \"Waiting...\".</p> <p>To better understand:</p> <ol> <li> <p>The first time you ran the consumer, it started from the earliest message (because you specified <code>auto.offset.reset=earliest</code>) and consumed all available messages in the topic.</p> </li> <li> <p>The next time you ran the consumer, it started from where it last left off. If no new messages were produced to the topic in the meantime, the consumer has nothing new to read, and it will just wait for new messages.</p> </li> </ol> <p>Summary</p> <ul> <li>earliest: Useful for scenarios like reprocessing, data migration, or cluster migration where replaying data is essential.</li> <li>latest: Ideal for stateless applications or situations where only the latest data is of interest.</li> <li>none: Best suited for strict processing needs where starting without a valid offset is not acceptable.</li> </ul>"},{"location":"lec2/demo_2b/","title":"Demo #2A: Kafka Topic Management with FastAPI and Python: A Tutorial","text":"<p>Author: Jeremy Gu</p> <p>Demo Objective:</p> <ul> <li>Learn to use python client  <code>kafka-python</code></li> <li>(Optional) Learn basics of <code>FastAPI</code></li> </ul> <p>Managing Kafka topics can become challenging as your data needs grow. This tutorial offers a solution by creating a microservice using <code>kafka-python</code> and <code>FastAPI</code>, allowing for streamlined topic management. By the end, you'll be able to create, view, and delete Kafka topics with ease.</p> <p>Reference: <code>kafka-python</code> Documentation</p>"},{"location":"lec2/demo_2b/#why-fastapi","title":"Why FastAPI?","text":"<p>FastAPI provides a rapid way to build web APIs. With automatic interactive API documentation and built-in OAuth and JWT, it's both developer-friendly and robust enough for production use. When combined with Kafka's powerful streaming capabilities, it becomes a potent tool for managing topics.</p>"},{"location":"lec2/demo_2b/#instructions","title":"Instructions","text":"<p>Prerequisites:</p> <ol> <li>Familiarity with Python programming.</li> <li>Basic understanding of Kafka.</li> <li>Python environment setup (recommend using <code>virtualenv</code>).</li> <li>Installed <code>fastapi</code>, <code>uvicorn</code>, <code>python-dotenv</code>, and <code>kafka-python</code> packages.</li> </ol> <p>Step 1: Setup Environment</p> <p>Create a directory.</p> <pre><code>mkdir msds-demo2b\ncd msds-demo2b\n</code></pre> <p>Begin by setting up a Python virtual environment to keep our dependencies isolated: <pre><code>$ python3 -m venv .venv\n$ source .venv/bin/activate\n</code></pre></p> <p>To set up the environment, install the necessary packages. If you have a requirements.txt file, you can use it. Otherwise, you can manually install the packages using the following command: <pre><code>$ pip install fastapi uvicorn python-dotenv kafka-python\n</code></pre></p> <p>Step 2: Configuration Management with .env</p> <p>Our demo uses environment variables to manage Kafka configurations. Create a <code>.env</code> file in your project directory. Referring to <code>config.ini</code> from the previous demo could be helpful. In this demo, we introduce <code>python-dotenv</code> as an alternative solution to store secrets instead of <code>config.ini</code>.</p> <p>Note: When using Git, ensure that both <code>.env</code> and <code>.venv</code> folders are added to your <code>.gitignore</code> file. Especially the <code>.env</code> file, as it contains sensitive configuration data, should never be committed to source control.</p> <p><pre><code>BOOTSTRAP_SERVERS=your_kafka_bootstrap_servers\nSASL_MECHANISM=your_sasl_mechanism\nSASL_USERNAME=your_sasl_username\nSASL_PASSWORD=your_sasl_password\nTOPICS_NAME=your_topic_name\nTOPICS_PARTITIONS=your_partition_count\nTOPICS_REPLICAS=your_replica_count\n</code></pre> Replace placeholders with actual Kafka configurations.</p> <p>Note: Both the <code>.env</code> and <code>.venv</code> folder can be gitignored and should not be committed to source control.</p> <p>Step 3: Code Overview</p> <p>We have three main Kafka functionalities within FastAPI. If you need help, please refer to the Appendix with the Scripts used in the demo. </p> <ol> <li>Create a Kafka topic at the app starting up. </li> <li>Delete a Kafka topic.</li> <li>Get the status of a Kafka topic.</li> </ol> <p>For all operations, we make use of the <code>KafkaAdminClient</code> provided by <code>kafka-python</code>.</p> <p>The <code>main.py</code> structure looks like below.  <pre><code>\"\"\"\nKafka Topic Management API (v2)\nJeremy Gu (Scaffolded Version)\nThis script provides a FastAPI based microservice to interact with Kafka for topic management. \nYour task:\n- Complete the missing parts of the code and test the endpoints.\n\"\"\"\n# Required imports\nimport logging\nimport os\nfrom fastapi import FastAPI, HTTPException\nfrom dotenv import load_dotenv\nfrom kafka import KafkaAdminClient\nfrom kafka.admin import NewTopic\nfrom kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n# Loading environment variables from .env file\nload_dotenv(verbose=True)\n# Initializing FastAPI application\napp = FastAPI()\n# Set up logging (no changes needed here)\nlogger = logging.getLogger()\nlogging.basicConfig(level=logging.INFO)\ndef get_kafka_client():\n\"\"\"\n    Create and return a KafkaAdminClient instance using environment variables.\n    TODO:\n    - Create a KafkaAdminClient with the required configurations.\n    \"\"\"\n# Fill in the required parameters from the environment variables\nreturn KafkaAdminClient(\nbootstrap_servers=None,  # TODO: Get from environment variable\nsecurity_protocol=None,  # TODO\nsasl_mechanism=None,    # TODO\nsasl_plain_username=None,  # TODO\nsasl_plain_password=None   # TODO\n)\n@app.post(\"/topics/create\")\nasync def create_topic():\n\"\"\"\n    Create a Kafka topic based on configurations from the .env file.\n    TODO:\n    - Extract topic configurations from the environment.\n    - Use KafkaAdminClient to create the topic.\n    \"\"\"\nclient = get_kafka_client()\n# TODO: Get topic configurations (name, partitions, replicas) from environment variables\ntopic = NewTopic(\nname=None,           # TODO\nnum_partitions=None,  # TODO\nreplication_factor=None   # TODO\n)\n# Try creating the topic\ntry:\n# TODO: Use client to create the topic\npass\nexcept TopicAlreadyExistsError:\nraise HTTPException(status_code=400, detail=\"Topic already exists\")\nfinally:\nclient.close()\n@app.post(\"/topics/delete\")\nasync def delete_topic():\n\"\"\"\n    Delete a Kafka topic based on configurations from the .env file.\n    TODO:\n    - Check if the topic exists.\n    - Delete the topic if it exists.\n    \"\"\"\nclient = get_kafka_client()\n# TODO: Get topic name from environment variables\n# Check if the topic exists\nexisting_topics = client.list_topics()\nif None:  # TODO: Replace None with the condition to check if topic exists\n# TODO: Use client to delete the topic\npass\nelse:\nraise HTTPException(status_code=400, detail=\"Topic not found\")\nfinally:\nclient.close()\n@app.get(\"/topics/status\")\nasync def topic_status():\n\"\"\"\n    Get the status and metadata of a Kafka topic based on configurations from the .env file.\n    TODO:\n    - Fetch the topic's metadata.\n    - Extract relevant information for the response.\n    \"\"\"\nclient = get_kafka_client()\n# TODO: Get topic name from environment variables\ntry:\n# TODO: Fetch the topic's metadata using the client\n# TODO: Construct the result with relevant topic metadata\nresult = {\n\"name\": None,  # TODO\n\"partitions\": None,  # TODO\n\"replicas\": None,  # TODO\n\"leaders\": None,  # TODO\n\"configs\": None   # TODO\n}\nreturn result\nexcept KeyError:\nraise HTTPException(status_code=404, detail=\"Topic not found\")\nfinally:\nclient.close()\n# NOTE: Don't forget to handle exceptions properly where necessary!\n</code></pre></p> <p>Step 4: Run the FastAPI Application To run our FastAPI application, we need to use <code>uvicorn</code>. In the terminal, navigate to the directory containing your FastAPI code (from the provided code in the previous sections) and run: <pre><code>$ uvicorn filename:app --reload\n</code></pre></p> <p>Or, run <code>uvicorn filename:app --reload --port 8001</code> if 8000 is already occupied.</p> <p>Replace <code>filename</code> with the name of your Python file (without the <code>.py</code> extension).</p> <p>Step 5: Using the FastAPI Endpoints</p> <p>Navigate to http://127.0.0.1:8000/docs to access the FastAPI documentation interface. </p> <ol> <li> <p>Creating a Kafka Topic:    Use a tool like <code>curl</code> or any API testing tool to hit the endpoint:    <pre><code>$ curl -X 'POST' 'http://127.0.0.1:8000/topics/create' -H 'accept: application/json'\n</code></pre>    If successful, you'll receive a response indicating the topic was created. The topic name is defined in the <code>.env</code> file.</p> </li> <li> <p>Deleting a Kafka Topic:    Similarly, to delete a topic:    <pre><code>$ curl -X 'POST' 'http://127.0.0.1:8000/topics/delete' -H 'accept: application/json'\n</code></pre></p> </li> <li> <p>Checking Topic Status:    To retrieve the status of a Kafka topic:    <pre><code>$ curl -X 'GET' 'http://127.0.0.1:8000/topics/status' -H 'accept: application/json'\n</code></pre></p> </li> </ol>"},{"location":"lec2/demo_2b/#future-steps","title":"Future Steps","text":"<p>In this tutorial, we've walked through the process of setting up a FastAPI application to manage Kafka topics using a Python client. This approach offers a programmatic way to manage Kafka topics and can be integrated into larger systems or web interfaces. </p> <p>This is a very simple touch on FastAPI and Web Application in the course. Mastering FastAPI and Kafka offers a competitive edge for data engineers and app developers. FastAPI allows swift API development, and Kafka handles real-time data streaming. Learning these tools enhances your skill set and positions you well in the evolving tech landscape. While there's a learning curve, the practical benefits of integrating FastAPI and Kafka are substantial and worth the effort.</p>"},{"location":"lec2/demo_2b/#appendix-scripts-used-in-the-demo","title":"Appendix. Scripts used in the demo","text":"<ul> <li> <p>.env</p> </li> <li> <p>config.ini: We won't need this file as we're using <code>.env</code>.</p> </li> <li> <p>main.py</p> </li> <li> <p>requirements.txt</p> </li> </ul>"}]}