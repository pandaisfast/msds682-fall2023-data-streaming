{"config":{"lang":["en","de","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MSDS 682 Data Streaming Processing Overview","text":""},{"location":"#course-description","title":"Course Description","text":"<p>This class are to equips students with the skills necessary to process continuous data streams at scale and in real-time.</p> <p>Students will gain hands-on experience with Apache Kafka and other modern data engineering tools. We focus on blending foundational knowledge with hands-on skills, often using real-world examples to anchor theoretical concepts. The primary programming language for the course is Python. </p> <ul> <li>Semester: Fall 2023</li> <li>Graduate Course in Data Engineering Track</li> <li>Course Notes: https://pandaisfast.github.io/msds682-fall2023-data-streaming/</li> <li>Canvas: https://usfca.instructure.com/courses/1617043</li> <li>Additional Materials: From time to time, I'll be publishing supplementary resources and materials on Canvas. Please check periodically to ensure you have all the required content.</li> <li>Homework Submission: Please submit all your homework assignments through Canvas. This ensures there's a central location for submission, and it's easier to keep track.</li> <li>Piazza: Q/A &amp; discussing lectures and assignments. https://piazza.com/usfca/fall2023/msds682</li> </ul>"},{"location":"#instructor-inforomation","title":"Instructor Inforomation","text":"<ul> <li>Jeremy W. Gu</li> <li>wgu9@usfca.edu</li> </ul>"},{"location":"#office-hours","title":"Office Hours","text":"<ul> <li>8:45-9:30am on Wed</li> <li>Office Location: Virtual Zoom</li> </ul>"},{"location":"#prerequisite-details","title":"Prerequisite Details","text":"<p>To ensure your success in this course, the following knowledge prerequisites are essential:</p> <ul> <li>Statistics: Grasp of concepts like mean, variance, data visualization, tabular data, and pandas. Relevant courses include MSDS 504 - Review Probability and Stats and MSDS 593 - EDA and Visualization.</li> <li>Python: Familiarity with Python Classes and Objects. While proficiency with the Mac command line is beneficial, there will be no focus on Java or Scala in the JVM ecosystem.</li> <li>SQL and Data Pipelines: Knowledge derived from courses such as MSDS 681 - Data Lakehouse is ideal.</li> <li>Machine Learning: Familiarity with scikit-learn, supervised and unsupervised learning, evaluation of model performance, and feature selection. Courses like MSDS 621 - Intro to Machine Learning, MSDS 630 - Advanced Machine Learning, and MSDS 680 - Machine Learning Operations are recommended, though not mandatory.</li> <li>Business Setting: This course requires project presentations and reports. Effective and professional communication in both written and spoken English is crucial. Business writing should be concise, straightforward, and to the point. </li> </ul>"},{"location":"#course-learning-outcomes","title":"Course Learning Outcomes","text":"<p>This applied, hands-on course provides students with the essential skills for processing and analyzing real-time data streams using modern data engineering tools and technologies. The goal is to equip students with foundational knowledge paired with practical skills in core data streaming tools and technologies like Kafka to prepare them for real-world data engineering and data science roles.</p> <p>The course focuses on core concepts like the Kafka ecosystem while providing practical skills development through hands-on projects. Students will gain experience with real-time data ingestion using Kafka, conducting streaming analytics, and extracting insights. Optional materials will expose students to additional technologies like Faust and Spark Structured Streaming for building and evaluating streaming applications.</p>"},{"location":"#fundamentals","title":"Fundamentals","text":"<ul> <li> <p>Kafka Ecosystem: Grasp the components of data streaming systems and delve into the Kafka ecosystem, recognizing the challenges each solution addresses. Kafka remains the primary focus.</p> </li> <li> <p>Confluent Kafka and Confluent Cloud: Use the Confluent Kafka Python library for tasks like topic management, production, and consumption. Demonstrations will be held using Confluent Cloud.</p> </li> <li> <p>Real-time Data Streaming with Apache Kafka: Students will undertake a course project for hands-on experience, ingesting real-time data using Apache Kafka, conducting live analytics, and extracting insights from streaming console reports.</p> </li> </ul>"},{"location":"#additional-skills","title":"Additional Skills","text":"<ul> <li> <p>Faust Stream Processing: Familiarize with the Faust Stream Processing Python library to craft real-time stream-based applications.</p> </li> <li> <p>Additional Tools: Time permitting, other tools will be introduced. For instance, students will explore the integration of Apache Spark Structured Streaming with Apache Kafka and understand the reports generated by the Structured Streaming console.</p> </li> </ul>"},{"location":"#textbooks-recommended-readings","title":"Textbooks &amp; Recommended Readings","text":"<p> Kafka in Action</p> <ul> <li>ISBN: 9781617295232</li> <li>Authors: Dylan Scott, Viktor Gamov, Dave Klein</li> <li>Publisher: Simon and Schuster</li> <li>Publication Date: 2022-02-15</li> <li>Required or recommended?: Recommended reading as a reference book.</li> </ul> <p></p> <p>Kafka: The Definitive Guide</p> <ul> <li>ISBN: 9781492043034</li> <li>Authors: Gwen Shapira, Todd Palino, Rajini Sivaram, Krit Petty</li> <li>Publisher: \"O'Reilly Media, Inc.\"</li> <li>Publication Date: 2021-11-05</li> <li>Edition: 2nd Edition</li> <li>Required or recommended?: Recommended. Not Required.</li> </ul>"},{"location":"#assignments","title":"Assignments","text":"<p>There will be three Assignments and one Final Project. It's crucial to adhere to deadlines. No submissions will be accepted past the due date.</p> <p>Policy on Collaboration and Academic Integrity</p> <ul> <li>Group Discussions &amp; Collaborations: We encourage effective group discussions and collaborations. Discussing concepts, techniques, and general approaches to problems with classmates can be beneficial. However, the work you submit must be your own.</li> <li>Originality of Work: All submitted assignments, code, and project reports must be your own original work. Copying and pasting code or text from classmates or any other source is strictly prohibited.</li> <li>Using AI Services (e.g., ChatGPT): Utilizing ChatGPT or other LLM models to aid in understanding course materials is permissible. However, always write in your own words and code independently. If you incorporate insights or specific code from any AI Services, clearly indicate which part(s) were influenced or sourced from them. </li> <li>Online Sources &amp; References: If you utilize or are inspired by online sources, always provide proper citation and reference. Respect the intellectual property of others.</li> <li>Cheating and plagiarism: We have a zero-tolerance stance on cheating and plagiarism. Engaging in any form of academic dishonesty will result in severe penalties, which may include failure in the course.</li> </ul>"},{"location":"#grading-breakdown-and-grading-policies","title":"Grading Breakdown and Grading Policies","text":""},{"location":"#grading-breakdown","title":"Grading Breakdown","text":"<ul> <li>Attendance and Professionalism: 5%</li> <li>Regular attendance and active participation are expected. </li> <li>Individual Assignment: 30%</li> <li>Assignments (10% each) will assess individual understanding and application of course material.</li> <li>Final Project: 45%</li> <li>Project Proposal 5%</li> <li>Written Report 20%</li> <li>Final Presentation 20%</li> <li>Midterm Exam: 20%</li> <li>No Final Exam</li> </ul> <p>This class is a standard, graded course with letter grades A - F. Each grade reflects the quality and understanding demonstrated by the student, as follows:</p> <p>A (90-100): Exceptional understanding and application. Demonstrates depth of knowledge and skill and indicates readiness to apply concepts in a professional setting.</p> <ul> <li> <p>A+: 96-100</p> </li> <li> <p>A: 93-95</p> </li> <li> <p>A-: 90-92</p> </li> </ul> <p>B (80-89): Competent understanding and application of course material, representing the expected level of competence in a business setting.</p> <ul> <li> <p>B+: 87-89</p> </li> <li> <p>B: 83-86</p> </li> <li> <p>B-: 80-82</p> </li> </ul> <p>C (70-79): Basic understanding, with room for improvement in application and depth, indicating achievements lower than the expected competence in the subject.</p> <ul> <li> <p>C+: 77-79</p> </li> <li> <p>C: 73-76</p> </li> <li> <p>C-: 70-72</p> </li> </ul> <p>F (Below 70): Limited understanding and application of course material, representing an unacceptably low level of knowledge and understanding of the subject matter.</p> <p>The expected class average is 85+ (Letter grade of B or above), with a normal distribution around this mean.</p>"},{"location":"#beyond-grades","title":"Beyond Grades","text":"<p>While the grading system in this course is a measure of academic performance, it can also shed light on the challenges one might face in the professional realm and the manner in which these are addressed. For example, when faced with tight deadlines, how does one ensure the quality of work? How does collaboration occur within a team, especially when compromises are necessary to meet delivery dates amidst differing opinions? How well do you explain data science concepts to people with no data background, and how effective is communication with peers and superiors? How are colleagues persuaded when introducing innovative ideas? </p> <p>In reality, certain missteps or attitude problems can have severe consequences, while standout performances can bring about many opportunities in your career. These are aspects indirectly reflected in your grades. If you earn an 'A+', there\u2019s a high probability that you will be a superstar in the workplace. Earning a 'B' is also commendable, as in a professional setting, it typically signifies \"Meeting Expectations\", meaning that you are performing your job. Conversely, if your grades are low, you might face significant risks of criticism from both managers and peers during performance evaluations. I hope every student can treat this course as a practical experience of the working world.</p>"},{"location":"#course-schedule","title":"Course Schedule","text":"Warning <p>This version of Course Schedule is tentative and may be delayed in updates. For the most accurate and up-to-date information, please use our course website on USF Canvas for the latest syllabus.</p> <p>Tentative Schedule (Each lecture: 5:30 - 7:20pm PST)</p> <ul> <li>Lecture #1: Oct 20, 2023 (F) - San Francisco-101 Howard 529</li> <li>Lecture #2: Oct 24, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #3: Oct 27, 2023 (F) - San Francisco-101 Howard 529</li> <li>Assignment #1: Due by 11:59pm on 10/28/2023</li> <li>Lecture #4: Oct 31, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #5: Nov 03, 2023 (F) - San Francisco-101 Howard 529</li> <li>Assignment #2: Due by 11:59pm on 11/4/2023</li> <li>Lecture #6: Nov 07, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #7: Nov 10, 2023 (F) - San Francisco-101 Howard 529 | 60-min Midterm Exam</li> <li>Lecture #8: Nov 14, 2023 (T) - San Francisco-101 Howard 529</li> <li>Final Project Proposal: Due by 11:59pm on 11/13/2023</li> <li>Lecture #9: Nov 17, 2023 (F) - San Francisco-101 Howard 527</li> <li>Lecture #10: Nov 21, 2023 (T) - San Francisco-101 Howard 529</li> <li>Assignment #3: Due by 11:59pm on 11/22/2023</li> <li>Lecture #11: Nov 24, 2023 (F) - San Francisco-101 Howard 529 | Thanksgiving: No Class</li> <li>Lecture #12: Nov 28, 2023 (T) - San Francisco-101 Howard 529</li> <li>Lecture #13: Dec 01, 2023 (F) - San Francisco-101 Howard 529</li> <li>Final Project Written Report and Code: Due by 11:59pm on 12/3/2023</li> <li>Final Project Presentation Deck: Must be submitted before the final lecture 5:30pm on 12/5/2023</li> <li>Lecture #14: Dec 05, 2023 (T) - San Francisco-101 Howard 157<ul> <li>Note: This lecture is allocated for the Final Project Presentation. Each student has a about 10-minute presentation slot.</li> </ul> </li> </ul>"},{"location":"#attendance-policy","title":"Attendance Policy","text":"<ul> <li>Mandatory attendance for every lecture. </li> <li>Use of Laptops: Please keep your laptops closed unless instructed otherwise, specifically during demo or exercise sessions. This is to ensure focus and participation during lectures.</li> <li>Absence Due to Illness: If you are unable to attend a lecture due to sickness or any other unavoidable circumstance, please notify me in advance. </li> <li>No Distractions: Mobile phones and other electronic devices should be kept silent and should not be used during class time. </li> </ul>"},{"location":"lec1/","title":"Lecture 1. Streaming Processing","text":"<p>Author: Jeremy Gu</p>"},{"location":"lec1/#basics","title":"Basics","text":"<p>Stream processing involves continuous calculations on constantly evolving data streams in real-time. Key aspects:</p> <ul> <li> <p>Data streams are unbounded sequences (potentially endless) of data that can change in shape and size over time. </p> </li> <li> <p>Immutable data - once in a stream, data is fixed and append-only. Existing data cannot be modified. Updates create new events.</p> </li> <li> <p>Varying data rates - streams can have bursts and lulls at uneven frequencies.</p> </li> <li> <p>Low latency - results are produced with minimal delays to enable real-time actions.</p> </li> <li> <p>Fault tolerance - streaming systems must handle failures gracefully.</p> </li> </ul>"},{"location":"lec1/#streaming-data-characteristics","title":"Streaming Data Characteristics","text":"<ul> <li> <p>Small data sizes - events are typically less than 1 MB.</p> </li> <li> <p>High throughput - streams sustain high input data velocities. </p> </li> <li> <p>Irregular arrival patterns - events arrive at inconsistent, uneven frequencies.</p> </li> </ul>"},{"location":"lec1/#events-in-streaming-data","title":"Events in Streaming Data","text":"<p>Events capture immutable facts about something that happened in the system. </p> <ul> <li>Example: GPS pings, Ads Clicks, purchases, sensor readings, etc.</li> </ul> <p>Event data is dynamic, short-lived and not intended to be stored for a long duration, compared to traditional databases that overwrite older data.</p> <p>Event producers emit facts without targeting specific consumers, unlike messaging queues which often have designated receivers.</p> <p>Note: On updating existing record. To 'update' an event in an immutable system, one doesn't modify the existing event. Instead, a new event (or record) is appended to represent the change or the new state. Subsequent processing or reading systems can then consider the latest event as the 'current' state, effectively overriding the previous event.</p> <p>Info</p> <p>We will cover more on message queues and invented systems later on. Please refer to Additional Topics for details. </p>"},{"location":"lec1/#stream-processing-and-batch-processing","title":"Stream Processing and Batch Processing","text":"<p>In the realm of data engineering, batch and stream processing serve as two prominent paradigms. </p> <pre><code>          +-----------------------------+\n          |                             |\n    +-----&gt;     Batch Processing        |\n    |     |                             |\n    |     +-----------------------------+\n    |     |   Dataset 1   |   Dataset 2   | ... |   Dataset N   |\n    |     +-----------------------------+---------------------+\n    |      \n    |      Time -------------------------&gt;\n\n    |      +-----------------------------+\n    |      |                             |\n    +------&gt;   Streaming Processing      |\n           |                             |\n           +-----------------------------+\n           | Event 1 | Event 2 | Event 3 | ... | Event N |\n           +-----------------------------+---------------------+\n\n           Time -------------------------&gt;\n</code></pre>"},{"location":"lec1/#comparison-of-batch-and-stream-processing","title":"Comparison of Batch and Stream Processing","text":"Batch Processing Stream Processing Definition and Usage Periodic analysis of unrelated groups of data. Historically common for data engineers. Continuous analysis as new events are generated. Operation Frequency Runs on a scheduled basis. Runs at whatever frequency events (that may be uncertain) are generated. Duration &amp; Storage May run for a longer period of time and write results to a SQL-like store. Typically runs quickly, updating in-memory aggregates. Stream Processing applications may simply emit events themselves, rather than write to an event store. Data Analyzed May analyze all historical data at once. Typically analyzes trends over a limited period of time due to data volume. Some batch jobs might only process a subset of the available data. Data Mutability Typically works with mutable data and data stores. Typically analyzes immutable data and data stores. Examples (1) Aggregates the last hour of data every hour. This can help in capturing hourly user engagement metrics. (2) Results usually written to a SQL-like store, like aggregating monthly sales data for financial reporting. (1) Real-time fraud detection based on latest transactions. (2) Social media trends based on real-time posts and interactions. Pros (1) Data consumers receive updates periodically. (2) Can leverage all available data in the system. (1) Outputs are up-to-date with the latest event. (2) Typically deals with immutable data, ensuring data consistency. Cons (1) Best resolution of data is based on the batch interval. (2) Data might be mutable, meaning records can change between batches. This might lead to inconsistencies. (1) Might lack full historical context. For instance, while it can capture sudden spikes in website traffic, it might not have the context of the overall daily or monthly trend. (2) Best suited for short-term trends."},{"location":"lec1/#key-distinctions-between-batch-and-stream-processing","title":"Key Distinctions Between Batch and Stream Processing","text":"Key Characteristics Batch Processing Stream Processing Nature of Data Processed Operates on finite stored datasets. Handles near-infinite streams. Job Frequency Runs jobs at discrete intervals. Continuous processing. Result Availability Results are available later. Results are available with low latency. Focus &amp; Accuracy Emphasizes on completeness. Emphasizes on low latency but can offer accurate results with \"exactly-once\" semantics. <p>Note: \"Exactly-once semantics\" means the system has mechanisms in place to ensure every piece of data is processed one time only.</p> <pre><code>Batch Processing:\n\n+---------+    +----------+    +----------+    +---------+   +---------+\n| Raw     | -&gt; | Extract  | -&gt; | Transform| -&gt; | Load    | -&gt;| Database|\n| Data    |    | &amp; Clean  |    | &amp; Model  |    | Process |   |         |\n+---------+    +----------+    +----------+    +---------+   +---------+\n              [Periodic intervals: e.g., daily, weekly]\n\nStreaming Processing:\n\n+---------+    +----------+     +--------+\n| Event   | -&gt; | Real-time| -&gt;  |Output  |\n| Stream  |    | Analysis |     |or DB   |\n+---------+    +----------+     +--------+\n              [Continuous: events processed as they arrive]\n</code></pre>"},{"location":"lec1/#general-notes","title":"General Notes","text":"<ul> <li>Distinctions &amp; Exceptions: While we often categorize \"batch processing\" as periodic and \"stream processing\" as real-time, these are broad generalizations. In practice:<ul> <li>Some batch jobs may operate close to real-time.</li> <li>Some stream processes might not be entirely real-time.</li> </ul> </li> <li>Hybrid Systems: Systems that integrate both batch and stream processing can harness the advantages of each approach, yielding more versatile and robust solutions.</li> <li>Interplay Between Batch and Stream: Batch systems often produce events that are subsequently processed in real-time by stream systems. This synergy bridges the divide between historical data processing and real-time analytics.<ul> <li>It's essential to recognize the unique strengths of each approach. Neither makes the other redundant. In the realm of data engineering, batch and stream processing often complement each other, rather than serving as alternatives.</li> </ul> </li> </ul>"},{"location":"lec1/#streaming-processing-application","title":"Streaming Processing Application","text":"<p>Two key components: Streaming data store and Streaming calculations.</p>"},{"location":"lec1/#e-commerce-example","title":"E-commerce Example","text":"<pre><code>+-------------------+          +------------------+          +---------------------+          +---------------------+\n| Customer places   |   ---&gt;   | System generates |   ---&gt;   | Processes the       |   ---&gt;   | Inform Warehouse &amp;  |\n| an order          |          | an order ID      |          | payment             |          | Send Delivery Info  |\n+-------------------+          +------------------+          +---------------------+          +---------------------+\n                                                                                        |\n                                                                                        v\n                                                                                +---------------+\n                                                                                | Notify User   |\n                                                                                +---------------+\n\nOutcome: Seamless integration of Kafka and stream processing applications for real-time e-commerce analytics.\n</code></pre> <p>When a customer places an order, the system of the e-commerce website will begin the tasks below:</p> <ul> <li> <p>The system creates an order ID</p> </li> <li> <p>Process payment </p> </li> <li> <p>Inform the warehouse for packaging</p> </li> <li> <p>Send delivery information to USPS or FedEx for shipping</p> </li> </ul> <p>The system creates an order, and the user receives a response that the order is being processed. </p> <p>1. Streaming data store: The e-commerce platform can use Kafka as a streaming data store to store event data such as users browsing, adding items to cart, and placing orders. Kafka stores data in time order and ensures data immutability.</p> <p>2. Stream processing calculation: Perform real-time calculation of users' browsing volume in the past 1 hour and generate reports. Consume real-time event data from Kafka, perform counting, and generate browsing volume report events output to the database. This allows observing some issues, such as some goods being too popular leading to low inventory, or finding issues in certain steps (like payment) that requires engineer maintenance. Other use cases of stream processing calculation:</p> <ul> <li> <p>Real-time tracking of most popular products (most clicked items)</p> </li> <li> <p>Real-time analysis of cart conversion rate (how many add to cart but do not purchase) </p> </li> <li> <p>Real-time generation of purchase suggestions e.g. \"Frequently bought together\"</p> </li> </ul> <p>Kafka stores the input data, stream processing applications perform real-time processing and analysis on data consumed from Kafka, and output results. The combination enables real-time data processing for the e-commerce platform.</p>"},{"location":"lec1/#summary-and-typical-data-processing-workflow","title":"Summary and Typical Data Processing Workflow","text":"<p>In summary, Streaming Data prioritizes immediacy. Stored Data prioritizes query-ability and depth. In many contexts, especially when distinguishing between \"Stored\" Data and Streaming data, it's generally implied that the data is stored in traditional, structured, and query-friendly systems, such as SQL-like databases.</p> Parameter/Feature Streaming Data (Real-time Data) Stored Data (SQL-like databases) Purpose Primarily used for real-time analytics and responses. Used for long-term storage and in-depth analysis. Typical Use Cases - Analyzing user behaviors such as browsing, clicking, and purchasing in real-time. - Making instantaneous decisions, e.g., adjusting product recommendations. - Product Details: Long-term storage of static info like product names, descriptions, categories, images, etc. - Historical Pricing: Time series data storing price fluctuations for trend analysis. - User Reviews: Long-term storage of feedback from customers. - Order Information: Permanent record of each transaction's details. Characteristics - Emphasizes immediacy and real-time actions. - Data might be transient or archived after being processed. - Emphasizes query-ability and historical context. - Data is persistently stored for future retrieval and analysis. <p>Typical Data Processing Workflow:</p> <ol> <li>Real-time Stream Processing: Streaming data undergoes real-time computations, providing insights like trending products.</li> <li>Data Storage: Results from real-time computations are written to storage systems.</li> <li>In-depth Analysis: Business intelligence systems query the stored data for comprehensive reports.</li> <li>Feedback Loop: Analytical results are integrated back into the product or business strategies, completing a full cycle.</li> </ol>"},{"location":"lec1/#course-coverage","title":"Course Coverage","text":"<pre><code>    +------------------+\n    |    Kafka (MQ)    |\n    +------------------+\n               |\n               v\n    +--------------------------+\n    | Stream Processing Apps   |\n    +--------------------------+\n               |\n               v\n    +------------------------+\n    | Confluent KSQL &amp; Faust  |\n    +------------------------+\n</code></pre> <p>Streaming Data Store : We will focus on using Kafka in the course. Kafka is a message queue system mainly used for processing and transporting real-time data streams. It is designed as a publish-subscribe system to ensure data can be consumed in real-time by multiple consumers.</p> <p>SQL stores like Cassandra will not be covered in our course. Cassandra is a database mainly used for long term data storage and querying. Although it supports stream data, its primary function is as a data storage system.</p> <p>Stream Processing Framework - A comprehensive set of tools and utilities, often bundled together as a library or a platform, which facilitates the creation and management of Stream Processing Applications. This framework offers components that handle various aspects of stream processing, such as data ingestion, real-time analytics, state management, and data output, allowing developers to focus on the specific business logic of their application. Here is a list of Common Stream Processing Application Frameworks, as follows:</p> <ul> <li> <p>Confluent KSQL - will be focused on in the course</p> </li> <li> <p>Faust Python Library - will be focused on in the course</p> </li> <li> <p>Apache Flink - will be mentioned in the course</p> </li> <li> <p>Apache Spark Structure Streaming - self-study needed</p> </li> <li> <p>Kafka Streams - self-study needed, especially for students proficient in Java</p> </li> <li> <p>Apache Samza - will not be covered in the course</p> </li> </ul>"},{"location":"lec1/#examples-of-using-data-streaming-services","title":"Examples of Using Data Streaming Services","text":"<p>Shareride Requests</p> <ul> <li> <p>Input data: Continuous stream of location data from riders' smartphones using GPS, details about driver locations, and availability.</p> </li> <li> <p>Output: Match rider requests with the most suitable driver in real time.</p> </li> <li> <p>Problem statement: To enable efficient and swift ride dispatching, process incoming location streams, and match the closest available driver to a requesting rider.</p> </li> <li> <p>Models/Algorithms: Could use the Haversine formula for calculating distances between geo-coordinates; a greedy assignment algorithm to match riders with drivers based on proximity and availability. (Optional reading: Approximate Nearest Neighbor (ANN) algorithms could be more efficient when dealing with a vast number of drivers and riders in close proximity.)</p> </li> <li> <p>Streaming services: Kafka for ingesting streams of rider requests and driver location updates; Spark Streaming for processing and assignment.</p> </li> </ul> <p>Fraud Detection in Payments</p> <ul> <li> <p>Input data: Continuous stream of financial transaction data, which includes purchases, withdrawals, and deposits.</p> </li> <li> <p>Output: Real-time alerts on suspicious or anomalous transactions.</p> </li> <li> <p>Problem statement: From a continuous stream of financial transactions, quickly identify and flag those that display patterns indicative of fraud.</p> </li> <li> <p>Models/Algorithms: Utilize unsupervised outlier detection models; clustering algorithms to identify and spotlight unusual patterns. Supervised learning techniques, such as Random Forests or Gradient Boosted Machines, could be employed when we have labeled data for fraudulent and non-fraudulent transactions. They can offer higher precision in anomaly detection compared to unsupervised methods. Also, mostly important process is to incorporate human feedback loop. </p> </li> <li> <p>Streaming services: Kafka for ingesting transactional data streams; Flink for real-time anomaly and pattern detection.</p> </li> </ul> <p>Package Delivery Tracking</p> <ul> <li> <p>Input data: Continuous GPS pings from delivery drivers' smartphones; shipment status updates.</p> </li> <li> <p>Output: Real-time updates on package delivery status and estimated time of arrival.</p> </li> <li> <p>Problem statement: Using a continuous stream of location data, frequently update the delivery status, and apply real-time optimizations to delivery routes.</p> </li> <li> <p>Models/Algorithms: Use rules-based algorithms to infer delivery status based on GPS data; real-time algorithms for optimizing delivery routes based on traffic and other constraints. Optimizing routes based on certain metrics (e.g. minimizing costs, maximizing speed, or other metrics) when there are multiple drops in bundles in a single trip.</p> </li> <li> <p>Streaming services: Kafka to ingest streams of GPS data and delivery updates; Spark Streaming for route analysis and optimization.</p> </li> </ul> <p>Other Considerations:</p> <ul> <li> <p>Trade-offs: Every streaming solution has to deal with the balance between latency (how fast data is processed) and throughput (how much data can be processed in a time frame). There's also accuracy, especially in ML where a faster prediction may be less accurate.</p> </li> <li> <p>Stream Imperfections: Data in the real world is messy. Streams can have missing data, or data can arrive out of sequence. Handling these imperfections requires strategies like watermarking or windowing.</p> </li> <li> <p>Joining Streaming with Historical Data: Often, the value from streaming data comes when it's combined with larger, historical datasets. Doing this efficiently is a challenge in stream processing.</p> </li> <li> <p>Testing and Monitoring: Streaming systems are complex and require sophisticated monitoring solutions. There should be mechanisms to ensure data integrity, monitor system health, and handle failures gracefully.</p> </li> </ul>"},{"location":"lec1/#quiz","title":"Quiz","text":"<p>Question 1. What is a key characteristic of data streams?</p> <ul> <li>A) Bounded sequences </li> <li>B) Immutable data</li> <li>C) Regular data arrival patterns</li> <li>D) Potentially unbounded sequences</li> </ul> <p>Question 2. What is a difference between streaming and batch data processing?</p> <ul> <li>A) Streaming focuses on completeness while batch emphasizes speed </li> <li>B) Streaming uses finite stored data while batch uses infinite streams</li> <li>C) Batch has higher latency than streaming</li> <li>D) Batch runs at scheduled intervals while streaming is continuous </li> </ul> <p>Question 3. Which statement describes event data accurately?</p> <ul> <li>A) Event data overwrites older data </li> <li>B) Events directly target specific downstream consumers</li> <li>C) Events capture immutable facts about a system</li> <li>D) Events are typically larger than 1 MB in size</li> </ul> <p>Question 4. What does stream processing involve?</p> <ul> <li>A) Discrete jobs running at regular intervals</li> <li>B) One-time calculations on finite data</li> <li>C) Continuous calculations on evolving data streams</li> <li>D) Loading data batches from databases</li> </ul> <p>Question 5. Which is NOT a streaming data characteristic?</p> <ul> <li>A) Potentially high throughput </li> <li>B) Strictly ordered arrival patterns</li> <li>C) Low latency results</li> <li>D) Small data sizes for individual events</li> </ul> <p>Question 6. What are benefits of using Streaming Processing Applications?</p> <p>(Open-ended question. Think about the themes like \"use cases\", \"speed\", \"scalability\", \"storage\", \"architecture\", etc.)</p>"},{"location":"lec1/#answers","title":"Answers","text":"<p>Question 1.</p> <p>D) Potentially unbounded sequences</p> <p>Explanation: Data streams are potentially infinite and unbounded in size, rather than having a fixed length like bounded sequences. This endless nature is a core characteristic.</p> <p>Question 2.</p> <p>D) Batch runs at scheduled intervals while streaming is continuous  </p> <p>Explanation: Batch processing runs at discrete scheduled intervals to process fixed datasets, while stream processing is continuous and operates on constantly evolving data streams.</p> <p>Question 3.</p> <p>C) Events capture immutable facts about a system</p> <p>Explanation: Events are immutable facts about occurrences in a system, rather than overwriting older data like in databases. Events also emit facts indirectly rather than targeting specific downstream systems.</p> <p>Question 4.</p> <p>C) Continuous calculations on evolving data streams</p> <p>Explanation: Stream processing continually performs calculations on live, updating data streams rather than finite stored data or intermittent batch jobs.</p> <p>Question 5.</p> <p>B) Strictly ordered arrival patterns</p> <p>Explanation: Irregular and uneven arrival patterns are a characteristic of streaming data. Strictly ordered patterns are not typical as streams can have bursts and lulls.</p>"},{"location":"lec1/1.1/","title":"Lec 1: Additional Topics","text":"<p>Author: Jeremy Gu</p> <p>Note</p> <p>In each lecture, we will have a section called \"Additional Topics\" covering concepts that, due to our fast-paced curriculum, we may not have time to cover in class. However, I believe understanding these is important for fully mastering data streaming. I may also include recommended readings and coding exercises to reinforce your learning. Note that additional topics will not be on the midterm exam. In your future work, I encourage revisiting these to strengthen your data streaming knowledge. </p>"},{"location":"lec1/1.1/#message-queues","title":"Message Queues","text":"<p>We introduce an example to demonstrate the concepts of Message Queues and Invented Systems in the context of an e-commerce website.</p> <p>Let's revisit the e-commerce website example to illustrate these concepts:</p>"},{"location":"lec1/1.1/#e-commerce-website-example","title":"E-commerce Website Example","text":"<p>Scenario: A customer places an order on an e-commerce website. Upon this action, the system needs to:</p> <ul> <li>Create an order ID.</li> <li>Process payment.</li> <li>Inform the warehouse for packaging.</li> <li>Send delivery information to USPS or FedEx for shipping.</li> </ul> <p>Case 1: Synchronous Processing</p> <ul> <li>User clicks \u201cPlace Order.\u201d</li> <li>The system begins all tasks one by one.<ul> <li>First, it creates an order ID.</li> <li>Next, it processes the payment.</li> <li>After that, it informs the warehouse.</li> <li>Finally, it sends the delivery information to USPS or FedEx.</li> </ul> </li> <li>Synchronous: Only after all these tasks are completed does the user receive a response.</li> <li>This Synchronous Processing doesn't use message queue. The user might have to wait a significant amount of time before getting feedback. </li> </ul> <p>Case 2: Asynchronous Processing with Message Queue</p> <ul> <li>User clicks \u201cPlace Order.\u201d</li> <li>The system quickly creates an order ID and sends the user a response that their order is being processed.</li> <li>The subsequent tasks (payment processing, notifying the warehouse, sending delivery info) are queued up in a message queue and are handled separately without making the user wait.</li> <li>The message queue acts as an intermediary, helping different services work together asynchronously, enhancing the system's responsiveness and decoupling.</li> </ul> <p>Decoupling - In the context of our example, decoupling means that different parts of the system (e.g., payment processing, warehouse notifications, and shipping notifications) operate independently. If one part fails or is slow, it doesn't directly impact the other parts. The use of a message queue helps in achieving this by allowing these parts to communicate without being directly connected.</p> <p>Asynchronous - Tasks or operations do not wait for the previous one to complete. They can start, run, and complete in overlapping time periods. In our example, after placing the order, the user doesn't have to wait for all tasks to complete. They immediately get feedback, while the system processes other tasks in the background.</p> <p>Synchronous - Tasks or operations happen in a specific order. Each task waits for the previous one to finish before starting. In our e-commerce example, it would mean the user waits for every single task to complete before getting any feedback.</p>"},{"location":"lec1/1.1/#append-only","title":"Append-only","text":"<p>Append-only logs serve as a fundamental mechanism both in stream processing and traditional SQL databases, but with different primary focuses. </p> <p>In streaming, they're vital for ensuring the real-time recording of events with guaranteed ordering, even at high rates. On the other hand, in conventional SQL databases, these logs primarily record and synchronize data changes between database nodes, ensuring data consistency.</p> <p>It's essential to recognize the diverse applications of this mechanism across various technological domains, highlighting its versatility and fundamental role in data management. This dual application elucidates the need for precision when adopting append-only logs, depending on the specific requirements and constraints of the given context.</p> Feature/Characteristic Stream Processing Traditional SQL Databases Primary Use High-throughput stream processing Data synchronization and backup Core Concept Append-only logs ensure data ordering and immutability Append-only or \"write-ahead logs\" record and synchronize all changes Application Real-time event recording, saved in the order they are received Data synchronization between primary and secondary database nodes Benefits Ensures events are correctly ordered even at high throughputs Ensures data consistency and synchronization between nodes Limitations \u2014\u2014 Append-only logs typically not suited for stream processing but are mainly for replication and backup"},{"location":"lec1/1.1/#optional-log-structured-storage-systems","title":"(Optional) Log-structured Storage Systems","text":"<p>1. Background</p> <ul> <li>Early Storage Challenges: Traditional storage inefficiencies with random write operations due to disk seek times.</li> <li>Solution: Emergence of log-structured storage to prioritize sequential writes, diminishing disk seek overhead.</li> <li>Transition: Continued relevance of log-structured storage with SSDs due to inherent benefits beyond disk optimization.</li> </ul> <p>2. Benefits of Log-structured Systems</p> <ul> <li>Efficiency: Continuous writes to the end of the log, optimizing disk throughput with minimized disk head movement.</li> <li>Reliability: Non-overwriting nature provides a natural version history and lowered data corruption risk.</li> <li>Concurrency: Enables multiple operations to write data simultaneously, ideal for multi-core processors and distributed architectures.</li> </ul> <p>3. Common Characteristics</p> <ul> <li>Immutable Data &amp; Append-Only Nature: Fundamental to the design, once data is written, it remains unaltered. All new data is added to the end, ensuring the append-only approach.</li> <li>Merge and Compaction: With log growth, older or less relevant data can be pruned, and logs can be merged for better efficiency.</li> <li>Log Files Handling: Logs can be processed based on:</li> <li>Time: Old logs might be archived or deleted after a pre-defined duration.</li> <li>Size: A new log is initiated once the current one hits a certain size.</li> </ul> <p>4. Challenges and Solutions</p> <ul> <li>Data Retrieval: Efficiently finding older data in sequential storage.</li> <li>Solution: Leveraging indexing and SSTables for speedy data segment access.</li> <li>Garbage Collection: Identifying and eliminating obsolete data over time.</li> <li>Obsolete Data Example: In a user activity log, a record showing a user liked a post followed by an entry indicating they unliked it can render the former redundant.</li> </ul> <p>5. Real-World Applications</p> <ul> <li>Apache Kafka: Adopted by corporations like LinkedIn for real-time activity tracking and analysis. Application scenario: Streamlining real-time analytics for website interactions.</li> <li>Apache Cassandra: Chosen by businesses like Instagram for large data volumes with high availability demands. Application scenario: Managing user profiles and activity feeds with low latency.</li> <li>Apache HBase: Used by Facebook for storing vast quantities of messages and posts, given its capacity to handle large, sparse datasets. Application scenario: Real-time analytics on user interactions and content delivery.</li> </ul>"},{"location":"lec1/1.1/#lambda-architecture-vs-kappa-architecture","title":"Lambda Architecture vs Kappa architecture","text":"<p>Lambda Architecture is a data processing architecture that uses both batch and stream-processing methods. Stream-processing is fast and caters to real-time requirements, while batch processing is thorough and handles large datasets. By combining the two, we can handle real-time data analytics without losing the depth that comes with batch processing. The beauty of Lambda Architecture is that it offers the best of both worlds: you get a snapshot of the present moment and also a detailed picture of the past. Here is Lambda Architecture from Databricks that covers more details.</p> <p>Example: Think of a social media platform where users post content and interact with one another. The platform wants to understand what's trending right now, but also wishes to make in-depth analyses periodically.</p> <ul> <li> <p>Speed Layer (Real-time): As soon as users post or interact, this data gets processed instantly, giving a real-time view of trends or popular posts. This is like looking at what's hot in the last hour or day.</p> </li> <li> <p>Batch Layer (Deep Analysis): Every day or week, the platform gathers all its data and performs a deep analysis, offering a comprehensive view. This could show things like the most influential users over a month or which topics consistently trended.</p> </li> <li> <p>Serving Layer (Combination): This is where the magic happens. It combines the real-time and the in-depth data, so that apps or analysts can get insights from both instant and historical data.</p> </li> </ul> <p>However, a Twitter architecture article in 2021 describes how Twitter migrated from a Lambda architecture to a Kappa-like architecture for large-scale real-time data processing. For more information on kappa architecture, please refer to What is the Kappa Architecture.</p> <p>Original Lambda Architecture:</p> <ul> <li>Batch layer - Process data in Hadoop using Scalding </li> <li>Speed layer - Real-time processing using Heron</li> <li>Serving layer - TSAR to query batch and real-time results</li> </ul> <p>Lambda Architecture had issues like data loss and high latency. Batch processing was also costly.</p> <p>(New) Kappa-like Architecture: </p> <ul> <li>Pre-processing - Data preprocessing on Kafka</li> <li>Real-time processing - Aggregation on Google Cloud Dataflow</li> <li>Serving layer - Query results from BigQuery and BigTable</li> </ul> <p>Twitter migrated from a Lambda architecture to a Kappa-like architecture by removing the batch path and relying solely on real-time processing. This improved performance and costs while maintaining accuracy. Key highlights below:</p> <ul> <li>Removed the batch processing path, only real-time path remains</li> <li>Reduced latency, improved throughput and accuracy</li> <li>Simplified architecture and reduced costs</li> <li>Validated accuracy by comparing to batch results</li> </ul>"},{"location":"lec1/1.1/#optional-reading-real-world-examples","title":"(Optional Reading) Real World Examples","text":"<p>The paper Shopper intent prediction from clickstream e-commerce data with minimal browsing information<sup>1</sup> addresses the problem of predicting whether a user will make a purchase during their session on an e-commerce website, based on their clickstream data.</p> <p>Here is a summary of the clickstream prediction problem formulation for Alice's example session, used in the paper. In this case, the clickstream data from users browsing the ecommerce site is a data stream - a continuous sequence of user actions happening in real time. Each user action like a page view, add to cart, purchase etc is considered an event. The rich clickstream sessions are distilled into symbolic trajectories retaining only event types. The prediction task is to take Alice's symbolic trajectory as input and predict whether she will make a purchase in that session.</p> <ul> <li> <p>A-&gt;B: Getting Formatted Data From Streams Alice's actual browsing session contains rich metadata like time, product info, etc (Layer A in Figure 1). </p> </li> <li> <p>B-&gt;C: Labeling Certain Events This is simplified into a minimal symbolic trajectory by retaining only the event types (page view, add to cart, etc.) as integers (Layer C in Figure 1).</p> </li> </ul> <p>The window of each session: It is segmented into sessions using a 30 min threshold. Events in a session are symbolized into a sequence retaining only the event type. For Alice's session, this results in a symbolic trajectory of event types. Her actual session is simplified to a symbolic sequence of events. This symbolic sequence is used to make the purchase prediction. The authors target only these minimal symbolic trajectories (Layer C) in experiments, providing a benchmark for methods that leverage the richer session metadata (Layer A). The prediction task is to take Alice's symbolic trajectory as input and predict if she will purchase in that session. </p> <p></p> <p>Data Processing:</p> <ul> <li>The raw clickstream data is sessionized and symbolized in pre-processing.</li> <li>Experiments are done on both balanced and imbalanced datasets.</li> <li>Analysis provides insights into how different amounts of clickstream data affects prediction performance.</li> </ul> <p>Output:</p> <p>The response is a binary prediction of whether the user will purchase something or not in that session.</p> <ul> <li>Page view event is mapped to symbol '1'</li> <li>Detail event (user views product page) is mapped to '2'</li> <li>Add event (add to cart) is mapped to '3'</li> <li>Remove event (remove from cart) is mapped to '4'</li> <li>Purchase event (buy product) is mapped to '5'</li> <li>Click event (click result after search) is mapped to '6'</li> </ul> <p>Machine Learning Approach:</p> <ul> <li>The paper compares two approaches:</li> <li>Hand-crafted feature engineering with classical ML classifiers</li> <li>Deep Learning based classification using LSTM architectures</li> <li>For feature engineering, they use k-gram statistics and visibility graph motifs as features.</li> <li>For deep learning, they benchmark and improve on previous LSTM models for sequential data.</li> </ul> <p></p> <ol> <li> <p>Requena, B., Cassani, G., Tagliabue, J. et al. Shopper intent prediction from clickstream e-commerce data with minimal browsing information. Sci Rep 10, 16983 (2020). https://doi.org/10.1038/s41598-020-73622-y\u00a0\u21a9</p> </li> </ol>"},{"location":"lec1/assignment/","title":"Assignment 1","text":""},{"location":"lec1/assignment/#assignment-1","title":"Assignment #1","text":""},{"location":"lec1/demo_or_exercise/","title":"Demo #1 Setting Up Environment","text":""},{"location":"lec1/demo_or_exercise/#principles-in-our-demos","title":"Principles in our demos","text":"<ul> <li>You should focus more on learning the core concepts and applications than setting up environment. Environment setup and cluster management are usually not the responsibility of new hires, especially in large companies with dedicated ops teams. </li> <li>Don't spend too much time on complex cluster configuration and environment setup as a beginner. This can detract you from learning the fundamentals. Focus more on the higher level abstractions and use cases.</li> <li>Leverage existing documentation and sample code. Don\u2019t reinvent the wheel.</li> <li>Take an iterative approach to learning. Get the basics working first, and later dive into refinement, optimizations and customizations. </li> </ul> <p>We have three options of setting up Kafka clusters (Official Confluent Platform reference). The first one with Confluent Cloud + Confluent CLI (python) will be used in our classroom. The Confluent Cloud path provides a fully managed service so you don't have to provision your own Kafka infrastructure. The local install and Docker approaches (Method #2 and #3) allow you to run Confluent Platform locally for development and testing. </p> <ul> <li> <p>Method #1: Managed Confluent Cloud + CLI</p> <p>We have two options of python client: <code>confluent-kafka-python</code> and <code>kafka-python</code>. In Demo #2, we will cover both of them.</p> </li> <li> <p>Method #2: Local Docker install (not covered in the course)</p> </li> <li>Method #3: Local native install (not covered in the course)</li> </ul>"},{"location":"lec1/demo_or_exercise/#method-1-confluent-cloud-confluent-cli","title":"Method #1: Confluent Cloud + Confluent CLI","text":""},{"location":"lec1/demo_or_exercise/#overview","title":"Overview","text":"<p>Confluent Cloud is a managed Kafka service provided by Confluent, the company behind some of the popular Kafka toolsets. The significant advantage of using Confluent Cloud is that you don't have to worry about the infrastructure or configuration of a Kafka cluster. Everything is managed, and you can focus entirely on your application and client development. This method is used in the classroom.</p>"},{"location":"lec1/demo_or_exercise/#benefits","title":"Benefits","text":"<ol> <li>Simplicity: No need to set up or maintain the Kafka cluster.</li> <li>Scalability: Managed services often offer easy ways to scale your usage as needed.</li> <li>Reliability: Managed by experts, ensuring high availability, backups, and other best practices.</li> </ol>"},{"location":"lec1/demo_or_exercise/#method-2-local-cluster-self-managed","title":"Method #2: Local Cluster (self-managed)","text":""},{"location":"lec1/demo_or_exercise/#overview_1","title":"Overview","text":"<p>This is about setting up a Kafka cluster on your local machine, often for development or testing purposes. There are resources available in the official documentation to guide through this setup. Although Method #2 is not our default setup for demos, I highly recommend self-study so you may find it useful in your course project with other services to be developed on local machine. </p>"},{"location":"lec1/demo_or_exercise/#docker-with-confluent-platform","title":"Docker with Confluent Platform","text":"<p>Docker provides a way to run applications securely isolated in a container, packaged with all its dependencies and libraries. The links you provided are guides on setting up the Confluent Platform (which includes Kafka and other tools) using Docker containers.</p> <p>Note: The native Kafka installation is one option, though Docker provides isolation benefits.</p>"},{"location":"lec1/demo_or_exercise/#benefits-of-docker-based-local-setup","title":"Benefits of Docker-based Local Setup","text":"<ol> <li>Isolation: Ensures Kafka doesn't interfere with other software on your machine.</li> <li>Reproducibility: A consistent setup across different machines.</li> <li>Ease of Setup &amp; Cleanup: With a few commands, you can start and stop a Kafka cluster.</li> </ol>"},{"location":"lec1/demo_or_exercise/#instructions-method-1","title":"Instructions (Method #1)","text":"<p>Step 1.Confluent Cloud Account: Sign up and be aware of the $400 free credit. Link.</p> <p></p> <p>Step 2. Creating &amp; Running Confluent Cluster: Set up a Kafka cluster on Confluent Cloud.</p> <p>Create Cluster</p> <p></p> <p></p> <p></p> <p>Create Topic Let's create topic called \"demo1_free_text\". Produce 8 messages with same <code>key = 1</code> and different <code>sport</code> values. </p> <pre><code>value: {\n      \"sport\": \"400 metres\"\n}\n\nkey: 1\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Do NOT shutdown the cluster. Let's go to the next section to set up CLI. We will come back to this Confluent UI later in the end of the instructions.</p> <p>Step 3. Python 3: Make sure it's version <code>3.10.12</code> for consistency. If you already have python 3.10.x set up, then you can skip Step 3.</p> <p>3.1 Environment</p> <ul> <li>Use a Macbook with Ventura 13.5.x. Avoid upgrading to MacOS 14.0 Sonoma.</li> <li>Ensure <code>pip</code> (Python's package manager) is up-to-date.</li> <li> <p>Install <code>brew</code> if you haven't. It's a handy tool for Mac users. Please run <code>brew install brew</code> to update it to latest version.</p> </li> <li> <p>Consider using a Python virtual environment. It helps keep things tidy!</p> </li> <li>Before the demo, check your setups. For instance, verify the Python version with <code>python --version</code>.</li> <li>If you face setup issues, don't worry! We'll ensure everyone can follow along.</li> </ul> <p>3.2 Installing ASDF</p> <p>To manage python packages, ASDF can be installed on macOS using Homebrew:</p> <pre><code>brew install asdf\n</code></pre> <p>After installation, ensure ASDF is added to your shell.</p> <p>For zsh:</p> <pre><code>echo -e \"\\n. $(brew --prefix asdf)/libexec/asdf.sh\" &gt;&gt; ${ZDOTDIR:-~}/.zshrc\n</code></pre> <p>For bash:</p> <pre><code>echo -e \"\\n. $(brew --prefix asdf)/libexec/asdf.sh\" &gt;&gt; ~/.bash_profile\n</code></pre> <p>3.3 Using ASDF</p> <p>Here are some commonly used ASDF commands:</p> <p>List all available plugins:</p> <p>bash</p> <pre><code>asdf plugin list all | grep -i &lt;plugin&gt;\n</code></pre> <p>List installed plugins:</p> <p>bash</p> <pre><code>asdf plugin list\n</code></pre> <p>Add plugins:</p> <p>bash</p> <pre><code>asdf plugin add python asdf plugin add poetry\n</code></pre> <p>Install specific versions:</p> <p>bash</p> <pre><code>asdf install python 3.10.12\n# asdf uninstall python 3.10.8\nasdf list-all python\n</code></pre> <p>3.4 Error <pre><code>ModuleNotFoundError: No module named '_lzma'\nWARNING: The Python lzma extension was not compiled. Missing the lzma lib?\n</code></pre> -&gt; simply install it via <code>brew install xz</code></p> <p>Great, you have successfully installed <code>Python 3.10.12</code> with asdf. Here's what you might want to do next:</p> <p>3.5 Set the Python version</p> <p>Now that you have Python 3.10.12 installed, you might want to set it as your default version. You can do this with asdf's global command:</p> <p><pre><code>asdf global python 3.10.12\n</code></pre> This will make Python 3.10.12 your default Python version in all directories.</p> <p>Alternatively, if you want to use this Python version only in your current directory (for a specific project), you can use asdf's local command:</p> <pre><code>asdf local python 3.10.12\n</code></pre> <p>Step 4. Installing Confluent CLI</p> <p>Install the Confluent CLI and follow this official guide. </p> <p>You can update it to latest CLI version by running <code>brew upgrade confluentinc/tap/cli</code> (assuming you installed CLI via <code>brew</code>).</p> <p>My version of Confluent CLI: <code>3.38.0</code>.</p> <p>Step 5. Setting up Confluent CLI</p> <p>Again, I recommend using official guide for most up-to-date information. In our demo today, here are the key steps below.</p> <p>Environment: In Confluent Cloud, an environment is a logical workspace where you can organize your resources. You can think of it like a project folder in many other systems. One user can have multiple environments.</p> <p>Cluster: Within an environment, you can have one or more Kafka clusters. A cluster is essentially a running instance of Kafka, where you can produce and consume messages. </p> <ol> <li> <p>Logging in to Confluent Cloud: </p> <p>This logs you into Confluent Cloud via the CLI and saves your credentials locally, so you don't need to enter them repeatedly. <pre><code>confluent login --save\n</code></pre></p> <p>If you're using SSO (google login for example), then run this code below.</p> <pre><code>confluent login --no-browser\n</code></pre> </li> <li> <p>Listing Environments:</p> <p><pre><code>confluent environment list\n</code></pre> This lists all the environments you have access to. For a new account, this will typically be just one environment. Make note of the ID of the environment you want to use.</p> </li> <li> <p>Setting the Active Environment:     <pre><code>confluent environment use {ID}\n</code></pre>     With this command, you're telling the CLI: \"Hey, I want to work within this specific environment.\"</p> </li> <li> <p>Listing Clusters:     <pre><code>confluent kafka cluster list\n</code></pre>     This lists all the Kafka clusters within the currently active environment. Make note of the cluster ID you want to interact with.</p> </li> <li> <p>Setting the Active Cluster:     <pre><code>confluent kafka cluster use {ID}\n</code></pre>     This command sets the Kafka cluster you want to work with. All subsequent commands will interact with this cluster unless you change it.</p> </li> <li> <p>Creating an API Key:     <pre><code>confluent api-key create --resource {ID}\n</code></pre>     An API key and secret are needed to authenticate and interact with your Kafka cluster programmatically. The <code>--resource {ID}</code> is the cluster ID you've previously noted. This command will provide you an API key and secret. Keep them safe; you'll need them to authenticate your requests.</p> </li> <li> <p>Using the API Key:     <pre><code>confluent api-key use {API Key} --resource {ID}\n</code></pre>     This tells the CLI to use the provided API key for authentication when interacting with the specified resource (Kafka cluster).</p> </li> </ol> <p>Step 6. Managing topics with Confluent CLI</p> <p>In this step, we'll learn how to manage Kafka topics using the Confluent CLI. We'll produce some messages to a topic and consume them to see how Kafka handles message streams.</p> <p>6.1 Listing Existing Topics: Before diving into producing and consuming messages, let's see which topics are already available in your Kafka cluster:</p> <pre><code>confluent kafka topic list\n</code></pre> <p>6.2 Producing and Consuming Messages: To truly understand Kafka, it's beneficial to visualize the interaction between a producer (which sends messages) and a consumer (which reads messages). For this, we'll open two terminal windows: one for the producer and one for the consumer.</p> <ul> <li> <p>Terminal 1: Producer's Perspective:</p> <p>To send messages to a topic, use the following command:</p> <pre><code>confluent kafka topic produce {topic_name} --parse-key\n</code></pre> <p>Now, input the following messages:</p> <pre><code>1:\"200 metres\"\n1:\"100 metres\"\n1:\"archery\"\n</code></pre> </li> <li> <p>Terminal 2: Consumer's Perspective:</p> <p>To read messages from a topic, use the following command:</p> <pre><code>confluent kafka topic consume --from-beginning {topic_name}\n</code></pre> <p>Observe the messages appearing in real-time as they\u2019re produced in Terminal 1.</p> </li> </ul> <p>6.3 Verifying in Confluent Cloud UI:</p> <p>After producing and consuming messages using the CLI, it's a good practice to check the Confluent Cloud UI to see the data visually:</p> <ul> <li>Go to Confluent Cloud.</li> <li>Navigate to the topic overview page for the topic you chose.</li> <li>Click on the Messages tab.</li> <li>In the Jump to offset field, enter \"0\".</li> <li>Select different partitions to observe where the new messages have been stored.</li> </ul>"},{"location":"lec1/demo_or_exercise/#summary","title":"Summary","text":"<p>There are many concepts so far. I hope to give a clear picture of the relationship between the different components of Confluent Cloud and how they are accessed via the CLI. Let's break down the hierarchy and relationship of the Confluent Cloud UI and how it interacts with the CLI.</p>"},{"location":"lec1/demo_or_exercise/#confluent-cloud-hierarchy","title":"Confluent Cloud Hierarchy:","text":"<ol> <li> <p>Environment:</p> <ul> <li>An environment is the highest level of organization in Confluent Cloud. It's a logical grouping mechanism.</li> <li>You might have different environments for different purposes, such as Development, Testing, and Production.</li> <li>Each environment has a unique Environment ID.</li> </ul> </li> <li> <p>Cluster:</p> <ul> <li>Within an environment, you can have one or more Kafka clusters.</li> <li>Each cluster has its resources, such as brokers, topics, etc.</li> <li>Each cluster has a unique Cluster ID within the environment.</li> </ul> </li> </ol>"},{"location":"lec1/demo_or_exercise/#accessing-via-cli","title":"Accessing via CLI:","text":"<p>To access and manage resources within Confluent Cloud using the CLI, you often need to specify both the environment and cluster you want to interact with. This is especially true if you have multiple environments or clusters, as many organizations do.</p> <p>Typically, the process involves:</p> <ol> <li> <p>Authentication:</p> <ul> <li>You'll first authenticate your CLI with Confluent Cloud using your API key and secret.</li> </ul> </li> <li> <p>Setting the Environment:</p> <ul> <li>Use the Environment ID to specify which environment you want to work within.</li> </ul> </li> <li> <p>Accessing/Managing a Cluster:</p> <ul> <li>Once inside an environment, use the Cluster ID to specify which cluster you want to interact with.</li> </ul> </li> </ol>"},{"location":"lec1/demo_or_exercise/#example","title":"Example:","text":"<p>Imagine you have two environments: <code>Development</code> and <code>Production</code>. In the <code>Development</code> environment, you have a Kafka cluster named <code>DevCluster</code>.</p> <p>To manage a resource in <code>DevCluster</code> using the CLI:</p> <ol> <li>Authenticate your CLI with Confluent Cloud.</li> <li>Set your working environment to <code>Development</code> using its Environment ID.</li> <li>Access or manage <code>DevCluster</code> using its Cluster ID.</li> </ol>"},{"location":"lec1/demo_or_exercise/#conceptual-visualization","title":"Conceptual Visualization:","text":"<pre><code>Confluent Cloud\n|\n|-- Environment 1 (Development)\n|   |\n|   |-- Cluster A (DevCluster)\n|   |   |\n|   |   |-- Topic 1\n|   |   |-- Topic 2\n|   |\n|   |-- Cluster B\n|\n|-- Environment 2 (Production)\n    |\n    |-- Cluster C\n    |\n    |-- Cluster D\n</code></pre> <p>When working with the Confluent CLI, you'd authenticate, select Environment 1 (using its ID), then select Cluster A (using its ID) to interact with topics or other resources within that cluster.</p>"}]}