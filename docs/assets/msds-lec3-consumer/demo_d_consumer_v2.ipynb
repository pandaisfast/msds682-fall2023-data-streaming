{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook - Consumer\n",
    "\n",
    "## **Kafka Producer-Consumer Demo Overview**\n",
    "\n",
    "In this demo, we'll be exploring a fundamental pattern in event-driven architectures: producing and consuming messages with Kafka. We will simulate a simple scenario where users (riders) request cars, producing this request as a message to a Kafka topic. Concurrently, we will have a consumer reading these requests and processing them.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Producer:** Simulates user requests by producing random `rider-name` and `location` data to a Kafka topic.\n",
    "2. **Consumer:** Listens to the Kafka topic and processes the incoming user requests by printing them.\n",
    "3. **Configuration Loader:** Loads the Kafka configurations required for authentication and communication.\n",
    "4. **Callbacks:** Handle specific events, such as a successful message delivery or topic partition assignment.\n",
    "\n",
    "**Technical Highlights:**\n",
    "\n",
    "- Asynchronous programming with `asyncio` to handle concurrent operations.\n",
    "- Use of the Confluent Kafka Python library for Kafka operations.\n",
    "- Integration with `.env` files for environment variable management using `dotenv`.\n",
    "\n",
    "## List of Used files\n",
    "\n",
    "Please run this python file in your directory. [Standalone python file](demo_d_consumer_v2.py).\n",
    "\n",
    "The `.env` file is [here](.env) as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kafka Consumer-Producer Demo Setup and Execution**\n",
    "\n",
    "### **1. Virtual Environment Setup**\n",
    "\n",
    "Virtual environments allow for isolated spaces to manage dependencies. It's beneficial to use a virtual environment to avoid potential conflicts between package versions.\n",
    "\n",
    "#### **1.1. Create & Activate a Virtual Environment**\n",
    "\n",
    "- **Create**: Navigate to your project directory and execute:\n",
    "  \n",
    "  ```bash\n",
    "  python -m venv venv\n",
    "  ```\n",
    "\n",
    "- **Activate**:\n",
    "\n",
    "  - For **Linux & Mac**:\n",
    "    ```bash\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "  \n",
    "  - For **Windows (PowerShell)**:\n",
    "    ```bash\n",
    "    .\\venv\\Scripts\\Activate.ps1\n",
    "    ```\n",
    "\n",
    "#### **1.2. Install Dependencies**\n",
    "\n",
    "With the virtual environment activated, install the necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install confluent_kafka python-dotenv asyncio\n",
    "```\n",
    "\n",
    "### **2. Kafka Cluster Setup and API Key Creation**\n",
    "\n",
    "To connect with the Kafka cluster, follow these steps:\n",
    "\n",
    "#### **2.1. Get Cluster Description**\n",
    "\n",
    "Retrieve details of the Kafka cluster:\n",
    "\n",
    "```bash\n",
    "confluent kafka cluster describe\n",
    "```\n",
    "\n",
    "#### **2.2. Create an API Key**\n",
    "\n",
    "Generate an API key for your Kafka cluster:\n",
    "\n",
    "```bash\n",
    "confluent api-key create --resource {id}\n",
    "```\n",
    "\n",
    "Replace `{id}` with your cluster ID from the previous step.\n",
    "\n",
    "#### **2.3. Set up Environment Variables**\n",
    "\n",
    "Use the provided details to populate an `.env` file:\n",
    "\n",
    "```\n",
    "BOOTSTRAP_SERVERS={Endpoint}\n",
    "SECURITY_PROTOCOL=SASL_SSL\n",
    "SASL_MECHANISMS=PLAIN\n",
    "SASL_USERNAME={API Key}\n",
    "SASL_PASSWORD={API Secret}\n",
    "CONSUMER_GROUP_ID=my_consumer_group\n",
    "```\n",
    "\n",
    "### **3. Running the Demo**\n",
    "\n",
    "Once the setup is complete, run the demo script:\n",
    "\n",
    "```bash\n",
    "python3 demo_d_consumer_v2.py consumer_example_v2\n",
    "```\n",
    "\n",
    "### **4. (Optional) Jupyter Notebook Setup**\n",
    "\n",
    "If using Jupyter Notebook or Jupyter Lab, it's crucial to set the appropriate Python kernel.\n",
    "\n",
    "#### **4.1. Install `ipykernel`**\n",
    "\n",
    "```bash\n",
    "pip install ipykernel\n",
    "```\n",
    "\n",
    "#### **4.2. Set the Jupyter Kernel**\n",
    "\n",
    "- Start Jupyter Notebook or Jupyter Lab.\n",
    "- Open the desired notebook.\n",
    "- Select `Kernel` -> `Change kernel` -> `venv`.\n",
    "\n",
    "**Note**: Asynchronous tasks in Jupyter might behave unexpectedly. It's advised to run the Kafka code as a standalone script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Topics\n",
    "\n",
    "This part is the same as demos in Lec 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "    \n",
    "## Recommended way of loading secrets from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "def load_config():\n",
    "    \"\"\"Load Kafka configuration.\"\"\"\n",
    "    return {\n",
    "        'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n",
    "        'security.protocol': os.getenv('SECURITY_PROTOCOL'),\n",
    "        'sasl.mechanisms': os.getenv('SASL_MECHANISMS'),\n",
    "        'sasl.username': os.getenv('SASL_USERNAME'),\n",
    "        'sasl.password': os.getenv('SASL_PASSWORD')\n",
    "    }\n",
    "\n",
    "## \n",
    "def topic_exists(admin_client, topic_name):\n",
    "    \"\"\"Check topic existence.\"\"\"\n",
    "    return topic_name in set(admin_client.list_topics(timeout=5).topics.keys())\n",
    "\n",
    "def create_topic(admin_client, topic_name, partitions=1, replication_factor=1, config={}):\n",
    "    \"\"\"Create topic if not existing.\"\"\"\n",
    "    if not topic_exists(admin_client, topic_name):\n",
    "        new_topic = [\n",
    "            NewTopic(\n",
    "                topic_name, \n",
    "                num_partitions=partitions, \n",
    "                replication_factor=replication_factor, \n",
    "                config=config)]\n",
    "        created_topic = admin_client.create_topics(new_topic)\n",
    "        for topic, f in created_topic.items():\n",
    "            try:\n",
    "                f.result()\n",
    "                print(f\"Topic {topic} created\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create topic {topic}: {e}\")\n",
    "    else:\n",
    "        print(f\"Topic {topic_name} already exists\")\n",
    "\n",
    "# Main execution\n",
    "config = load_config()\n",
    "admin_client = AdminClient(config)\n",
    "topic_name = \"consumer_example_v2\"\n",
    "\n",
    "# Create topic\n",
    "topic_config = {'cleanup.policy': 'compact'} \n",
    "create_topic(admin_client, topic_name, partitions=3, replication_factor=3, config=topic_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all topics \n",
    "# https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html\n",
    "\n",
    "topic_metadata = admin_client.list_topics(timeout=5)\n",
    "list(topic_metadata.topics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Running Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, os\n",
    "import random\n",
    "from confluent_kafka import Consumer, Producer, OFFSET_BEGINNING# Consumer Group ID for ensuring unique offset tracking\n",
    "\n",
    "CONSUMER_GROUP_ID = os.getenv('CONSUMER_GROUP_ID', 'default-group-id')\n",
    "\n",
    "async def consume(topic_name):\n",
    "    \"\"\"Asynchronously consume data from the specified Kafka Topic.\"\"\"\n",
    "    \n",
    "    # Short delay before initiating the consumer\n",
    "    await asyncio.sleep(2.5)\n",
    "\n",
    "    # Configure consumer with Kafka settings and subscribe to the topic\n",
    "    c = Consumer({\n",
    "        **config,\n",
    "        \"group.id\": CONSUMER_GROUP_ID,\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "    })\n",
    "    c.subscribe([topic_name], on_assign=on_assign)\n",
    "\n",
    "    # Continuously poll for new messages in the topic\n",
    "    while True:\n",
    "        message = c.poll(1.0)\n",
    "        if message is None:\n",
    "            print(\"no message received by consumer\")\n",
    "        elif message.error() is not None:\n",
    "            print(f\"error from consumer {message.error()}\")\n",
    "        else:\n",
    "            print(f\"consumed message {message.key()}: {message.value()}\")\n",
    "        await asyncio.sleep(0.1)  # Brief pause to reduce CPU load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_assign(consumer, partitions):\n",
    "    \"\"\"Callback executed when partitions are assigned. Sets partition offset to beginning.\"\"\"\n",
    "    for partition in partitions:\n",
    "        partition.offset = OFFSET_BEGINNING\n",
    "    consumer.assign(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    "    \"\"\"Callback function to report the result of a produce operation.\"\"\"\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")\n",
    "\n",
    "async def produce(topic_name, config = config):\n",
    "    \"\"\"Asynchronously produce random person-location data into the specified Kafka Topic.\"\"\"\n",
    "    \n",
    "    p = Producer(config)\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "    \n",
    "    # Continuously produce messages to the topic\n",
    "    while True:\n",
    "        name = random.choice(names)\n",
    "        lat = random.uniform(-90, 90)\n",
    "        long = random.uniform(-180, 180)\n",
    "        message_key = f\"rider-{name}\".encode(\"utf-8\")\n",
    "        message_value = f\"rider {name} requests a car at ({lat:.2f}, {long:.2f})\".encode(\"utf-8\")\n",
    "        \n",
    "        p.produce(topic_name, key=message_key, value=message_value, callback=delivery_report)\n",
    "        p.poll(0.1)  # Poll to allow callbacks to be executed\n",
    "        await asyncio.sleep(0.1)  # Brief pause to reduce CPU load\n",
    "\n",
    "async def produce_consume(topic_name):\n",
    "    \"\"\"Concurrently run producer and consumer tasks using asyncio.\"\"\"\n",
    "    \n",
    "    t1 = asyncio.create_task(produce(topic_name))  # Task for producing messages\n",
    "    t2 = asyncio.create_task(consume(topic_name))  # Task for consuming messages\n",
    "    await t1  # Wait for producer task to complete (infinite loop in this case)\n",
    "    await t2  # Wait for consumer task to complete (infinite loop in this case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no message received by consumer\n",
      "no message received by consumer\n",
      "no message received by consumer\n",
      "no message received by consumer\n"
     ]
    }
   ],
   "source": [
    "## main\n",
    "topic_name = 'consumer_example_v2'\n",
    "await produce_consume(topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Stopping the execution of a cell in Jupyter (such as by pressing the \"Stop\" button) will interrupt the kernel, which should ideally stop the execution of the code within the cell. However, there could be a few reasons why you're still seeing outputs:\n",
    "\n",
    "1. **Asynchronous Nature**: The nature of asynchronous tasks means that while the main task (in this case, the Jupyter cell execution) might be stopped, some tasks might still be in the queue to be executed.\n",
    "\n",
    "2. **Kafka Consumer Lag**: Depending on how you've configured Kafka and your consumer, there might be a lag between when messages are produced and when they are consumed. If your producer produced many messages quickly, the consumer might still be processing those even after the producer has stopped.\n",
    "\n",
    "3. **Jupyter Kernel State**: In some situations, the Jupyter kernel might not effectively stop the execution of certain tasks or threads. This can especially be the case with certain libraries or tools that manage their own internal threads or processes.\n",
    "\n",
    "To ensure that everything is stopped:\n",
    "\n",
    "1. **Manual Stop**: After you've pressed the \"Stop\" button in Jupyter, you can manually close the producer and consumer connections to Kafka if they were opened. This will ensure that no new messages are produced or consumed.\n",
    "\n",
    "2. **Restart the Kernel**: In the Jupyter Notebook toolbar, there's an option to restart the kernel. This will completely reset the Python process running your notebook, ensuring all tasks, threads, and processes related to it are stopped. It's a bit of a \"nuclear option\", but it's a surefire way to stop everything.\n",
    "\n",
    "3. **Improve Cleanup in Code**: Consider adding cleanup code that will be executed when stopping the tasks. For instance, closing the Kafka connections gracefully, ensuring all asynchronous tasks are canceled, etc. \n",
    "\n",
    "In the long term, if you're finding that running these tasks in Jupyter is causing issues, you might want to consider other environments for long-running or complex asynchronous tasks, like a standalone Python script, especially when dealing with systems like Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The key points in the Kafka producer-consumer demo:\n",
    "\n",
    "- The demo uses `asyncio` to handle concurrency between the producer and consumer tasks. This allows them to run asynchronously.\n",
    "- The producer randomly generates `rider name` and `location data`, and publishes it to a Kafka topic.\n",
    "- The consumer listens to the same topic and prints any messages it receives.\n",
    "- They use the `confluent_kafka` Python library to interact with Kafka. This handles the connections, serialization, etc.\n",
    "- `Configuration` like bootstrap servers and authentication is loaded from a `.env` file using `python-dotenv`. This keeps secrets out of code.\n",
    "- `Callbacks` are defined to handle events like successful delivery or partition assignment.\n",
    "- A `consumer group` is configured to allow scaling consumers while ensuring message ordering.\n",
    "- Topics are created programmatically if they don't already exist.\n",
    "- The demo could be adapted to publish real data and have the consumer process it in some application-specific way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of an indefinite loop, using `close()` directly within the loop itself wouldn't make sense, because the loop is designed to run forever. However, it's still good practice to handle cases where the program might be interrupted (e.g., by a keyboard interrupt, SIGTERM, or other termination signals). \n",
    "\n",
    "The key idea is to gracefully handle those interruptions so that any buffered messages in the producer are sent to the Kafka broker (`flush()`) and the consumer's connection and any pending offsets are properly closed (`close()`). \n",
    "\n",
    "Here's how you can achieve that:\n",
    "\n",
    "1. **Signal Handling**: You can use Python's `signal` library to handle termination signals and gracefully shut down the producer and consumer.\n",
    "\n",
    "2. **Global references**: You might have to make the producer and consumer references global or pass them around to ensure you can close them from the signal handling function.\n",
    "\n",
    "Here's a simple example focusing on signal handling:\n",
    "\n",
    "```python\n",
    "import signal\n",
    "\n",
    "producer = None\n",
    "consumer = None\n",
    "\n",
    "def shutdown(signalnum, frame):\n",
    "    \"\"\"Gracefully shut down on SIGINT or SIGTERM.\"\"\"\n",
    "    global producer, consumer\n",
    "    if producer:\n",
    "        producer.flush()\n",
    "        print(\"Flushed producer\")\n",
    "    if consumer:\n",
    "        consumer.close()\n",
    "        print(\"Closed consumer\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# Attach the signal handlers:\n",
    "signal.signal(signal.SIGINT, shutdown)\n",
    "signal.signal(signal.SIGTERM, shutdown)\n",
    "\n",
    "# Your existing asyncio code\n",
    "# ...\n",
    "\n",
    "async def produce(topic_name,config = config):\n",
    "    global producer\n",
    "    producer = Producer(config)\n",
    "    # ... rest of the produce code ...\n",
    "\n",
    "async def consume(topic_name):\n",
    "    global consumer\n",
    "    consumer = Consumer({\n",
    "        **config,\n",
    "        \"group.id\": CONSUMER_GROUP_ID,\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "    })\n",
    "    consumer.subscribe([topic_name], on_assign=on_assign)\n",
    "    # ... rest of the consume code ...\n",
    "```\n",
    "\n",
    "With the above implementation, if the program receives a SIGINT (Ctrl+C) or a SIGTERM, it will invoke the `shutdown` function, which will flush the producer and close the consumer.\n",
    "\n",
    "This method ensures that even with an indefinite loop, there's a mechanism to handle unexpected interruptions gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
